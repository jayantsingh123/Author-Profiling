{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu7gPMlr_NAD"
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVAVWxIYA8YT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOt9QuliH6Jd"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/My Drive/Colab Notebooks/NLP/Data (1)/Data.zip' 'Data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gb3rasjbIEG3"
   },
   "outputs": [],
   "source": [
    "!unzip -qq Data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsdQTyVLA8Yb"
   },
   "outputs": [],
   "source": [
    "path_train = '/content/Data/train/en'\n",
    "path_test = '/content/Data/test/en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFnkhURaA8Yi"
   },
   "outputs": [],
   "source": [
    "files_train = [f for f in listdir(path_train) if isfile(join(path_train, f))]\n",
    "files_test = [f for f in listdir(path_test) if isfile(join(path_test, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "F_Avr659A8Yp",
    "outputId": "3282d239-49f9-4de0-8bda-adae27bd8f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Lin_Manuel We called it Butts Up in Alberta Canada.\n",
      "@UnionSportsGuy @joerogan @rickyred09 They're shooting near the tigers to scare them off. Almost looks like sandbags at one point.\n",
      "@_McDaniel16 Best 4th Quarter Ever.\n",
      "@MaxStrunk @EASPORTSFIFA I don't think it's just you. I can't connect to Battlefield on PS4.\n",
      "@BoLeviMitchell Yeeeeesssss. Scrappy white receiver? Perfect for the Pats. But will Bill take a WR in the 1st?\n",
      "@ScottMitchellPM You should throw your name into the hat. It can't get worse can it?\n",
      "@tomsegura On your tour dates you have Calgary spelt as 'Calgray'. Leads me to belive @ChristinaP is the personality champ.\n",
      "@ScottMitchellPM Jesus, this is bad.\n",
      "@nottjmiller @cashinginwithtj @CashLevy Shepard's Pie. Delicious Englishman's dish? Or a delicious dish of Englishman? #maskers\n",
      "@nottjmiller @cashinginwithtj @CashLevy Shepard's Pie. Delicious Englishman's dish? Or a delicious dish of Englishman?\n",
      "@dabnorfish @AesopRockWins That's like them blatantly trying to screw their customers.\n",
      "@BoLeviMitchell How do you feel about another player named Mitchell wearing 19?\n",
      "@BoLeviMitchell Freelance photographer's are this generations actors. There's a dime a dozen because everyone thinks they can do it.\n",
      "@StampMachine You can google maps Area 51. https://t.co/Dn0v9EWUAW\n",
      "@IanLaralive Awesome show tonight. How long are you in town for?\n",
      "@OriginalFunko Any plans for Planet of the Apes from the newer films? Gotta have Caesar and Maurice.\n",
      "@bigjayoakerson Hearing you talk about anxiety on @joerogan has really made me feel better about my own and that I'm not alone. Thank you.\n",
      "@OriginalFunko Any plans for Caesar and Maurice from the new Planet of the Apes franchise?\n",
      "@MikeReiss How many post season yards did Wes have with the Pats?\n",
      "@DannyAustin_9 You're a Calgarian now and that will not be tolerated.\n",
      "I hate my bloody car.\n",
      "@DannyAustin_9 Your life sounds like an episode of a sitcom.\n",
      "@JordanETID The Nice Guys?\n",
      "I sent a package to New Zealand and it arrived 5 days later. Very impressed @canadapostcorp\n",
      "@AesopRockWins Do you know what time you fine lads will be hitting the stage?\n",
      "Thank you @TofinoBrewCo for bringing your lager back to the mainland. Best Christmas present I could've asked for.\n",
      "@DannyAustin_9 @ScottFisherPM He's retiring? All the best Scott. Does that mean you're taking over the Stamps beat full time?\n",
      "@Lemarvelous23 This is actually pretty low considering I remember paying 1.40-1.50 a year ago.\n",
      "@AesopRockWins Wanna watch mine? It's only 5 minutes.\n",
      "The Goldbergs is a fun show with the worst theme song ever.\n",
      "@AesopRockWins @jennruhl My bad I got carried away. Apologies to you both.\n",
      "@jennruhl @AesopRockWins Why don't you chill the fuck out? It's not his fault, stop blaming him.\n",
      "@DannyAustin_9 I've been at a Christmas party. In 140 letters or less why did it turn out so well?\n",
      "@StampMachine The irony is thick especially with what he's wearing.\n",
      "@nikel18 Get a chance to watch it?\n",
      "@nikel18 https://t.co/B7EJVV9iku Password is Victoria\n",
      "@nikel18 Hey Nik, would you be interested in watching a short film I made. Just trying to see what the public thinks.\n",
      "@DannyAustin_9 @SheaSerrano But he went platinum without any singles!\n",
      "@OnePerfectShot Simply put, did you enjoy it?\n",
      "@rmaver6 When the depression from the loss ends.\n",
      "@6BONECRUSHER3 it's either an extension which makes my heart sing or you're going to the NFL which makes my heart break.\n",
      "@_McDaniel16 are you saying you don't want to come back?\n",
      "@DannyAustin_9 Not at all, you were awesome. I there's a way I can let the guys upstairs to keep you on this beat let me know. #Danny2017\n",
      "@DannyAustin_9 It was a fun ride, albeit frustrating. You'll be with us next year I hope?\n",
      "@alexsingleton43 Should've got Most Valuable Canadian regardless.\n",
      "@_McDaniel16 Now we know how the Seahawks felt. Should've given it to Messam.\n",
      "@DannyAustin_9 The Stamps have so much to be ashamed of. No excuses just results.\n",
      "Should've given it to @JMessam this has been a travesty @calstampeders\n",
      "@EricDionRogers Shoulder, but I didn't see what happened and no one is saying anything.\n",
      "@DannyAustin_9 No one can. This is heartbreaking. They said they weren't going to over confident but they don't look prepared at all.\n",
      "@DannyAustin_9 What even happened?\n",
      "I predict that #TeamShaw #BoLeviMitchell will hoist the #GreyCup. What about you? https://t.co/gwJVIrMhXn\n",
      "@BoLeviMitchell You said you were going to watch movies, could you check out my shot? Password: Victoria https://t.co/B7EJVV9iku\n",
      "@tfcnation07 @MinCanadaFA So you have something in common with him then.\n",
      "@DannyAustin_9 I thought the Grey Cup was tomorrow? üòè\n",
      "@Lin_Manuel @kelsblackerby @VAMNit Surely you mean uncanine-y.\n",
      "@ScottMitchellPM I almost stupidly asked who's rings those were. Now I'm just stupidly pointing it out.\n",
      "@JMessam winning just crashed the @CFL Feed!\n",
      "CONGRATS @6BONECRUSHER3!!! Absolutely deserved. And damn you clean up nicely.\n",
      "@calstampeders @SincerelyToot Damn Toot that suit is lit.\n",
      "@LeDoctor I may or may not have used this shot as influence for one of my own. https://t.co/I2u2vfFHt7\n",
      "@Lin_Manuel What's your team Lin? I'm guessing Giants or Jets?\n",
      "@Lin_Manuel I'm surprised Canada's so late to the party https://t.co/1t0tmX5oYn\n",
      "@protodenNIS @6BONECRUSHER3 Juwan Simpson broke it when we won in 2014. It's not the most sturdy trophy.\n",
      "#GoodLuckStamps Go be the Champions I know you are.\n",
      "@DannyAustin_9 It's only that big when they build the bleachers for the Grey Cup.\n",
      "@J_Wall30 wearing 31 scores a pick six to bring the game to 31 points. #ThanksMylan @calstampeders\n",
      "@DannyAustin_9 News on Finch? I'm just wondering why Durant is returning.\n",
      "@bigjayvegas @RealGDT @LACMA There's a del Toro doppleganger that was there as well it looks like.\n",
      "@DannyAustin_9 of this whole empire?\n",
      "Tonight, I wrote two pages of dialogue about whether or not a hot dog was a sandwich. This is art folks.\n",
      "@BCLions I really want to go to the game tomorrow but can't afford a ticket. Help a @CFL fan out?\n",
      "@DannyAustin_9 Were you one of these questions? https://t.co/yjkCv8iOsM\n",
      "@CFL @Shaw_CFL We got a lot of DDs on the team but that isn't one of them. I think you mean Derek Dennis @6BONECRUSHER3 Future MOL\n",
      "@SteveMorley62 Yet our dollar dropped a whole cent in 4 hours because of it.\n",
      "The first thing Trump did was tank the Canadian dollar a cent in 4 hours. Thanks a lot, the future's going to be swell.\n",
      "This season of South Park has gotten out of hand.\n",
      "@joshdrozda The people chose those candidates unfortunately.\n",
      "@nikel18 Calgary or Montreal?\n",
      "@BoLeviMitchell How long till you can apply for dual citizenship?\n",
      "@6BONECRUSHER3 How about a new ring?\n",
      "@randychevrier The only place in Canada that thinks our country is going in the wrong direction is the prairies. This doesn't compare.\n",
      "@_McDaniel16 Wikipedia was sadly bare for such a great receiver so I updated it. But can someone with more knowledge upload a picture @CFL?\n",
      "Hey @calstampeders Our leading receiver needs a picture for his wikipedia page. @_McDaniel16\n",
      "@BoLeviMitchell @Greg_Wilson15 You know it's the bye week when Bo starts trolling his receivers. #IsItNov20Yet?\n",
      "@DannyAustin_9 What they did with the original was combine it into 2 hour Documentary and included new footage. They may do it again.\n",
      "@nikel18 gets stripped by @Mr_Raymond25. Gotta love the two former @calstampeders ballin' hard!\n",
      "@df_sacks @calstampeders @packers @NHLFlames It's still a little jarring to see green in all that red ;)\n",
      "@EricDionRogers @_McDaniel16 Just adding fuel to the fire that is your basketball rivalry. https://t.co/bZd7a4WqxQ Brutal airball.\n",
      "@SportsOn770 @DebStrickland65 I agree with Jock. @humbled_roy has been incredible and given our ST unit a completely different look.\n",
      "@nikel18 Still look good in red and white.\n",
      "@greecebaII @ScottMitchellPM Jags would die in Calgary. The cold would destroy them\n",
      "@ScottMitchellPM Not going to be anywhere near as historic as yours!\n",
      "The older I get the more disgusting I think ketchup is.\n",
      "@ScottMitchellPM Please tweet more about the CFL going into the playoffs. We miss you.\n",
      "@calstampeders @StampMachine @BoLeviMitchell @TD_Canada Where's Bo's beard?\n",
      "@nikel18 At least with Cardinals they had Wagner coming in hot, would've rattled the best guy. Seahawks? No excuse. Just incompetence.\n",
      "@DannyAustin_9 If it still isn't working the stamps app is updating.\n",
      "@DannyAustin_9 This is true. I won't be surprised if the oline gels better for the rest of the game with Lavertu out. Wilson seemed better\n",
      "@DannyAustin_9 he's only missed two passes. Granted those could've been caught.\n"
     ]
    }
   ],
   "source": [
    "for r in ET.parse(join(path_train,files_train[6])).getroot()[0]:\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gE5nvBmFA8Yw"
   },
   "outputs": [],
   "source": [
    "#ET.parse converts the file into the tree\n",
    "def convert_texts(path_train, files_train):\n",
    "    \n",
    "    doc = []\n",
    "    for i in range(len(files_train)):\n",
    "        #Append the tweets to the corresponding document\n",
    "        try:\n",
    "            doc1 =[r.text for r in ET.parse(join(path_train,files_train[i])).getroot()[0]]\n",
    "            doc.append(' '.join(t for t in doc1))\n",
    "        except:\n",
    "            print(files_train[i])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "y_MC0VZlA8Y2",
    "outputId": "2fa6faf9-79ec-4d20-bab7-c40721c52965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth.txt\n",
      "truth.txt\n"
     ]
    }
   ],
   "source": [
    "# list of train and test tweets\n",
    "t_train = convert_texts(path_train, files_train)\n",
    "t_test  = convert_texts(path_test, files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XeZuavIuMEqH",
    "outputId": "09aff042-af06-44fe-c1dc-557eb6c042a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "6j_mAoWKL67d",
    "outputId": "c082a1bc-6f2d-4170-9a86-1cd76869d146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@secrettourpro @OmegaDDC he had 9 previous wins in Torrey Pines but we saw what happened there!! @MolloyJoe one for the next GW podcast üò≥üò≥ https://t.co/uHtO2fZCD5 @NoLayingUp @IndoSport looking forward to next Sunday already! @arielhelwani no reebok attire from Ronda either (Apart from weigh in gear) Surely Reebok won't be happy? Dominic Cruz is the man üòÇüëè #UFC207  https://t.co/OxBw1jRqgD @arielhelwani check this out üòÇüôå #legend  https://t.co/ESuNGQkOKq The world we live in these days  https://t.co/7pUraXm1ZK .@McIlroyRory my mate @AzorAhai88  thinks I'm wasting my time! Prove him wrong üòé #CantSpellForHisLife #BOSE https://t.co/VMlYFDadvc @CarlMullan all a bit coincidental don't you think? I reckon Perez or Hulkenberg could be in with a shout there! Alonso also üòé @Sutzer94 I was close üòÇ https://t.co/IQq8gjTGPH @todayfm Smooth Criminal #TrumpTune @ShannonPurcell might aswell highlight what everyone already's thinking! @secrettourpro everyone writing off the rookies. Bet it'll be one of them that shines and wins the trophy for either side! @VirginMediaIE hey, is there anyone I can talk to about reviewing our monthly bill? Bin offered cheaper elsewhere and wondering wud u match? @secrettourpro really? Always thought a Green jacket would be every tour pros dream! https://t.co/1XanEesDTQ @secrettourpro @5footergolf Rickie Fowler #TakingSTPsTip @secrettourpro what would go thru the afternoon groups heads after this? Must be hard not to feel hard done by having to play in that? @VirginMediaIE my tv and Internet is still down here. Over a day now, can you provide me with a number to ring someone please? @meepops @VirginMediaIE @amy82burke same issue in Templeogue... Second time it's happened here recently. What's the story? #MissedTheMatch @CarlMullan their battle between each other just keeps getting better and better! They're gonna box the heads of each other some day! üòÇ @Sutzer94 they're about 4 yrs behind the trend! #ShaveOrDye üòéüòÇ https://t.co/WUzOGcpC2n @Sutzer94 @AskPaddyPower worth a score üëÄüôåüî• @Thedirtypilot @mendieGK @secrettourpro was thinking surely it can't be that overrated to have hosted 4 Ryder Cups but that explains it all! @secrettourpro what's happened The Belfry? Know it's not in London but surely it'd be appealing? Danny Willett is the Jamie Vardy of golf! #Masters2016 #Masters @nikegolf @BrianWindsor91 @nikestore any more going on sale? #Please @secrettourpro how come Langer still has the long putter?? #Masters2016 @arielhelwani what's Cormiers injury? Haven't heard what's wrong which is strange no? @secrettourpro @JustinRose99 is in the Bahamas too üëÄüëÄ @TheNotoriousMMA going to have to attend press conferences with Diaz, Edgar and Aldo... Amazing! #UFC200 @JordanSpieth @JustinThomas34 where's the trash talk? #WGCDellMatchplay #WhoWins #BaldBets @paddypower love an outside e/w on this fella #CheltMental https://t.co/SCKpXOlVs0 @secrettourpro imagine having HIM as your president! @secrettourpro who's the biggest trash talker on each tour? @secrettourpro should be 2 top amateurs and 2 top pros qualify for each team. Keep some of the amateur tradition alive! That'd be great imo @talktoBOI yeah just logged in there. Dunno why the app wouldn't let me? @talktoBOI still showing the below? My details entered are correct though. https://t.co/kRVmPuFouv @talktoBOI sorry was on work Twitter there oops. Eh mobile app! @GavMcS @mcd_productions @faithless @LeGalaxie lets get in touch with our mate ha @McIlroyRory think you should make a call to Jose and offer him tickets to the Irish Open! #NeedHimQuick #Connections @FM104 playing the full explicit version of Stan.. #SomeBalls #SomebodyThinkOfTheChildren @EnergiaHome Your phone line says Office Hours are 9-5... Been trying to ring for the last 15 Minutes. Everyone dodging off early?? @secrettourpro when a TP signs with a brand is it in their contract that they have to wear a hat during a round? Or is it just preference? @secrettourpro #ThereCanOnlyBeOne Actually can't stop laughing and smiling.... @TheNotoriousMMA ur a gas man! #Effortless @secrettourpro @TheSergioGarcia I reckon u join myself and @AzorAhai88 for a round of golf and a round of Guinness before the Irish open 16üëç @VirginMediaIE Hi, not yet no. I saw that there was a problem in our area (Osprey, D6W) any update? Thanks @VirginMediaIE @itsliamo did you get sorted Liam? I'm in Osprey and the below is popping up! Link above is no use https://t.co/s1ufVncEq4 @VirginMediaIE both tv and wifi knocked themselves off... Everything is inserted perfectly into Box &amp; router! #help https://t.co/SPP9GHgxK9 @Graeme_McDowell the grind and hard work always pays off! Well done on the win brother! #ReturnOfTheGMac #LoveTheGrind @ActorTrivia @JCavanagh92 That's bollox, he's a cartoon reenactment of the mighty @DelaneyYardel and no one else!! #I‚ù§Ô∏èFarquad Well @MrNiallMcGarry let's hear your opinion... Should Mr. O'Shea have gotten the All Star? #BeHonestNow @SportsJOEdotie pullin strings üòÇ @thewhitmore whopper costume absolutely nailed it! #HarleyQuinn #Skwad Natalie Sawyer flashing a bit of cleveage is the ideal remedy for my hangover today! #LoveYou #AnyChance @secrettourpro @TitleistEurope Defo this, from one masked man to another!! https://t.co/fRe3GTuAy9 @McIlroyRory myself and @AzorAhai88 missed you in Carton yday! We're playing Irish amateur Matchplay finals next week... #AnyTips @JoshHadden @secrettourpro @7NY @Hawkguy30 yet today he comes out and says he won't go back. More money in top 20 finishes on PGA. #NotRight @7NY @secrettourpro @Hawkguy30 Charles Howell is one of the highest grossing players on tour... But who's going to remember Charles Howell?? @thebattler2000 @secrettourpro @Hawkguy30 @MattWabe but those endorsements can have obligations to play! Eg. Omega &amp; Bose sponsored events. @secrettourpro Dyu think that wud be enough to entice the Casey's and Rory's back over though? Wud it be enough for you? #BigNamesBigNumbers @secrettourpro What changes wud u suggest to make Euro Tour compete with PGA? Apart from bigger prize money of course... Rebrand it?? I was just wondering, will there ever be a boy born that could swim faster than a shark?! #TheOffice #Priceless The awkward moment when you can't even walk into ur own sittin room cos ur bro is a bit  hormanal! #pubertyprobz üôå @secrettourpro some sort of loyalty discount for returning members especially mid 20s/30s would be a good incentive also! 6 fuckin 20!!!! #EP2015 Whoever predicted this \"Indian summer\" for #EP2015 is a moron! #Freezin @BeechParkGolf  #ChampionsOfSouthDublin #CartonHouseBound #Beecher http://t.co/pF6B6oesAv @Ryanair that's great thanks! And the passbook for boarding passes? Will it be ok to use that in Murcia airport? @Ryanair If I have a carry on bag am I allowed bring a schoolbag aswell? And also do boarding passes with passbook work in all airports? Anyone have any links for #TheOpen2015 ? #TeamDunne @AzorAhai88 #NothinButNet http://t.co/AFiG1Bkcn3 Winter is here. @richierichballs @AzorAhai88 #consistency @SecondCaptains #TeamLowry Gwan Shane!!! Cersei, Walder Frey, The Mountain, Meryn Trant. @TheRealKirstyG tough job for you this week! #VeryJealous @zachbraff I presume my E-Vite is on the way?? I have my E-Maybe response ready to go here! #HappiestOfBirthdays @walsh_eoin @gerryhussey both of them and the Tuesday in work after a long weekend!! Fresh sheetsüòç You're a fraud and you know it\n",
      "But it's too good to throw it all away\n",
      "Anyone would do the same\n",
      "#suits #quote #veryapt @miss_ashcurtis stick with me and ul go places!! #LetsTalkTactics @miss_ashcurtis monetaire or Buywise in the next #ComeOnBaby @rte real sound lads............... #Disaster DJ on fire... Defo worth a punt for the masters! #Doral #augusta #Masters2015 @Sutzer94 cheers for Leavin the blazer and shoes out for me for tnite! #BlazeUpForTheDarts? I cringe for all the girloz that jump on the #EP2015 bandwagon! #YouveGotNoFans About to go into the dentist... If I don't make it, I am batman and I put the screw in the tuna!! #RIPKev Poor aul Toure hasn't ran this much in years... #GoOnYaya @Sutzer94 @RoyalGolfCo ship to Ireland?? #BranchingOut @RonanOGara10 glorious shirt! #lookinsharp @johnmahon5646 @TXFMDublin OMC #HowBizarre @HayleyMcQueen @JimWhite @SkySportsNewsHQ is the Carmex his secret weapon? @paddypower you're on fire tonight... Making up for the lack of transfers! #KeepUpTheGoodWork Played in my first ever golf competition yesterday and came second... Delighted with life! #watchoutrory #kevscominforya #TheBeecherMassive @halfadams ever watch \"the scene\" from a few good men to get you in the lawyer zone? #toogood #AskSuits 2 seasons of #PeakyBlinders dusted off in a day... What a show!! ‚ÄúFinishing all the gift-wrapping\" http://t.co/7L7TVnxANA‚Äù @effy_mcl image of us last nite ha ‚ÄúBest gigs of 2014 according to listeners:\n",
      "\n",
      "1. @ArcticMonkeys Marlay Pk\n",
      "2. @WeAreAugustines at Whelan's \n",
      "3. @KasabianHQ at 3Arena\"\n",
      "\n",
      "#2outta3 Actually love #MalcolmInTheMiddle @seanstledger12 hope you serenaded Mick Mc Carthy with it! #GreatChoice\n"
     ]
    }
   ],
   "source": [
    "print(t_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuUrKgRHA8ZA"
   },
   "source": [
    "# Get Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZI5XIsCSA8ZC"
   },
   "outputs": [],
   "source": [
    "def get_labels(path, files):\n",
    "    dic={}\n",
    "    task=[[],[]]\n",
    "    for e in open(join(path,'truth.txt')).read().split('\\n'):\n",
    "        d=e.split(':::')\n",
    "        if(len(d)==3):\n",
    "            dic.update({d[0]: d[1:]})\n",
    "            task[0].append(d[1])\n",
    "            task[1].append(d[2])\n",
    "    task=[sorted(list(set(t))) for t in task]\n",
    "    \n",
    "    d_task = [{item:i for i, item in enumerate(t)} for t in task]\n",
    "    labels=[[],[]]\n",
    "\n",
    "    for e in files:\n",
    "        try:\n",
    "            ID=e[:-4]\n",
    "            labels[0].append(d_task[0][dic[ID][0]])\n",
    "            labels[1].append(d_task[1][dic[ID][1]])\n",
    "        except:\n",
    "            pass\n",
    "    print(d_task)\n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "IZTZb4TWA8ZI",
    "outputId": "3e2e0df5-7793-4fd1-e566-ae0f175cfa57",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'female': 0, 'male': 1}, {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}]\n",
      "[{'female': 0, 'male': 1}, {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3600)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train = get_labels(path_train, files_train)\n",
    "labels_test  = get_labels(path_test , files_test)\n",
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gxWt4RDqA8ZN",
    "outputId": "f6a128fd-7c8e-466d-f60f-8bb8aed6f058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJBdWdGcA8ZV"
   },
   "source": [
    "# Check distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "2W9DlThdA8ZX",
    "outputId": "24453354-dba6-41e8-a8ce-a694ac0584f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAEvCAYAAACQQh9CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "gender_num= Counter(labels_train[0])\n",
    "\n",
    "nation_num = Counter(labels_train[1])\n",
    "\n",
    "ax[0].bar(gender_num.keys(),gender_num.values())\n",
    "\n",
    "ax[1].bar(nation_num.keys(), nation_num.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53vL-zQAA8Zd"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zl7m9n8uA8Zf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "    #match url (i.e: https://t.co/5tF5G9VKtq)\n",
    "    (r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '<url>'),\n",
    "\n",
    "    #match user (i.e: @cerpintor )\n",
    "    (r'@\\w+', '<user>'),\n",
    "\n",
    "    #match hashtag (i.e: #WomensMarchOnWashington)\n",
    "    (r'#\\w+', '<hashtag>'),\n",
    "\n",
    "    #Replace \"&...\" with ''\n",
    "    (r'&\\w+', '')\n",
    "]\n",
    "\n",
    "class RegexReplacer(object):\n",
    "    def __init__(self, patterns = replacement_patterns):\n",
    "        self.patterns = [(re.compile(regrex),repl) for (regrex, repl) in\n",
    "                        patterns]\n",
    "    \n",
    "    #Replace the words that match the patterns with replacement words\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5MJCNPHhA8Zk",
    "outputId": "3271fd84-f74c-4898-c7db-7dee37ba9758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tknz = TweetTokenizer()\n",
    "replacer = RegexReplacer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punc = string.punctuation\n",
    "\n",
    "def normalize(doc):\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        \n",
    "        #Tokenize with replacement\n",
    "        doc[i] = tknz.tokenize(replacer.replace(doc[i]))\n",
    "        \n",
    "        #Filter stopwords, punctuations, and lowercase\n",
    "        doc[i] = [w.lower() for w in doc[i] if w not in punc and w not in stopwords]\n",
    "    \n",
    "        #Stem words\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        doc[i] = [lemmatizer.lemmatize(w, pos='v') for w in doc[i]]\n",
    "        \n",
    "        \n",
    "        #concat\n",
    "        doc[i] = ' '.join(w for w in doc[i])\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mlN-a4blA8Zp",
    "outputId": "9669cfeb-df42-483e-cafe-c0a98c9667bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "t_train = normalize(t_train)\n",
    "t_test =normalize(t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VIDfFlhYA8Zv",
    "outputId": "083acf80-1b9a-42f1-e386-852aaccc2f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "print(len(t_train))\n",
    "print(len(t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZcPKTaBOA8ch"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJ0mFCByBQSm"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JeE4d69A8Z0"
   },
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9-ANpmoA8Z3"
   },
   "outputs": [],
   "source": [
    "#Find Frequency of Words\n",
    "\n",
    "corpus_tweet = []\n",
    "\n",
    "for doc in t_train:\n",
    "    corpus_tweet += doc.split()\n",
    "\n",
    "fdist = nltk.FreqDist(corpus_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6f-minz-A8Z8",
    "outputId": "3bfd22b9-c8ef-425b-da75-db1fd6b6380c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104400"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iB2yIYdRA8aG"
   },
   "outputs": [],
   "source": [
    "#Sort the dist by decreasing order\n",
    "def sortFreqDist(freqdict):\n",
    "    \n",
    "    freq = [(freqdict[key], key) for key in freqdict]\n",
    "    return sorted(freq, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HJXeE4WA8aL"
   },
   "outputs": [],
   "source": [
    "#Pick 5000 most frequent words\n",
    "freq_tweets = sortFreqDist(fdist)\n",
    "freq_tweets = freq_tweets[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUtRtnp7A8aR"
   },
   "outputs": [],
   "source": [
    "#index of frequent tweets\n",
    "dict_indices = dict()\n",
    "index = 0\n",
    "for i, word in enumerate(freq_tweets):\n",
    "    dict_indices[word[1]] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeNRifs7A8ah"
   },
   "outputs": [],
   "source": [
    "#Split the frequency and the frequent words\n",
    "freq, freq_words = zip(*freq_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ue-79vcEA8al"
   },
   "outputs": [],
   "source": [
    "#Build a bag of words\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_bow_tr(t_train, freq_words, dict_indices):\n",
    "    \n",
    "    BOW = np.zeros((len(t_train), len(freq_tweets)), dtype=int)\n",
    "    \n",
    "    #Loop through documents\n",
    "    for index, tr in enumerate(t_train):\n",
    "        fdist_doc = nltk.FreqDist(tr.split())\n",
    "        \n",
    "        #Loop through the words in each document\n",
    "        for word in fdist_doc:\n",
    "            #If word in document is also in most frequenty words,\n",
    "            #append the frequency of that word in BOW\n",
    "            if word in freq_words:\n",
    "                BOW[index, dict_indices[word]] = fdist_doc[word]\n",
    "        \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZIOBREUA8as"
   },
   "outputs": [],
   "source": [
    "#BOW for training set\n",
    "BOW_tr = build_bow_tr(t_train, freq_words, dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hey-IhZeA8ax",
    "outputId": "874a0e32-b567-4fba-fd0c-870d3d6fc02f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 5000)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3600 doc with 5000 words\n",
    "BOW_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awk-hcZqA8a2"
   },
   "outputs": [],
   "source": [
    "#BOW for test set\n",
    "BOW_val = build_bow_tr(t_test, freq_words, dict_indices )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t6XQaSc7A8a9",
    "outputId": "a54b312d-d8fd-4102-eb49-f7d9a23ccb5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 5000)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM9j5V7nA8ck"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "qfvjCvTaA8cp",
    "outputId": "6821cdae-e1c0-4933-a959-00e33ddf34cc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[945 255]\n",
      " [315 885]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77      1200\n",
      "           1       0.78      0.74      0.76      1200\n",
      "\n",
      "    accuracy                           0.76      2400\n",
      "   macro avg       0.76      0.76      0.76      2400\n",
      "weighted avg       0.76      0.76      0.76      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "\n",
    "grid.fit(BOW_tr,labels_train[0])\n",
    "\n",
    "labels_pred = grid.predict(BOW_val)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(labels_test[0], labels_pred, average='macro', pos_label=None)\n",
    "\n",
    "print(confusion_matrix(labels_test[0], labels_pred))\n",
    "print(metrics.classification_report(labels_test[0], labels_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AB3CR0hXA8cs",
    "outputId": "c413be53-e73f-4999-e6c9-af91ee1271bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7625"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(labels_test[0], labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "L-tdj0Y6A8cw",
    "outputId": "9049b07c-3cd7-4984-a0d7-5186b8a76b27",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[275  32  21  12  36  24]\n",
      " [ 19 301  12  15  16  37]\n",
      " [ 12  17 302  39  14  16]\n",
      " [ 16  12  30 313  13  16]\n",
      " [ 23   6  13   8 338  12]\n",
      " [ 15  46  10   9   8 312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.72       400\n",
      "           1       0.73      0.75      0.74       400\n",
      "           2       0.78      0.76      0.77       400\n",
      "           3       0.79      0.78      0.79       400\n",
      "           4       0.80      0.84      0.82       400\n",
      "           5       0.75      0.78      0.76       400\n",
      "\n",
      "    accuracy                           0.77      2400\n",
      "   macro avg       0.77      0.77      0.77      2400\n",
      "weighted avg       0.77      0.77      0.77      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "grid.fit(BOW_tr,labels_train[1])\n",
    "\n",
    "labels_pred = grid.predict(BOW_val)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(labels_test[1], labels_pred, average='macro', pos_label=None)\n",
    "\n",
    "print(confusion_matrix(labels_test[1], labels_pred))\n",
    "print(metrics.classification_report(labels_test[1], labels_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oPHfznBVA8c0",
    "outputId": "245f2280-5d00-43c9-88e8-b4819a908c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7670833333333333"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(labels_test[1], labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1--fXz9iA8bj"
   },
   "source": [
    "## Tf-Idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6Xlie6sA8bl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "#tf-idf\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "\n",
    "#just send in all your docs here\n",
    "tfidf_vectorizer_train=tfidf_vectorizer.fit_transform(t_train)\n",
    "\n",
    "tfidf_vectorizer_test=tfidf_vectorizer.transform(t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l2ZJpKvsA8bq",
    "outputId": "511ad441-cc7a-44e0-8e8d-72e0992ce0d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 87879)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HiRasU0IA8bv",
    "outputId": "84d2e452-cc34-41d5-a811-3c1d3e3193a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 87879)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tddcjNgcA8c7",
    "outputId": "5986593e-6fee-41c9-b2df-3b76f3a9f659",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[986 214]\n",
      " [268 932]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1200\n",
      "           1       0.81      0.78      0.79      1200\n",
      "\n",
      "    accuracy                           0.80      2400\n",
      "   macro avg       0.80      0.80      0.80      2400\n",
      "weighted avg       0.80      0.80      0.80      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "grid.fit(tfidf_vectorizer_train,labels_train[0])\n",
    "\n",
    "labels_pred = grid.predict(tfidf_vectorizer_test)\n",
    "\n",
    "print(confusion_matrix(labels_test[0], labels_pred))\n",
    "print(metrics.classification_report(labels_test[0], labels_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DArLCSzSgJBF",
    "outputId": "86e84ce7-2426-458e-f93f-1bc883f4c892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7945439045183291\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(labels_test[0], labels_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "wSy1LAzeA8c9",
    "outputId": "810904f7-3f1f-4689-e741-1ee18044dbb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334  21  14  11  14   6]\n",
      " [ 11 342   9   5   6  27]\n",
      " [ 15  13 323  26   7  16]\n",
      " [  9   8  21 339   7  16]\n",
      " [ 13  12   7   1 359   8]\n",
      " [ 13  44   5   6   3 329]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       400\n",
      "           1       0.78      0.85      0.81       400\n",
      "           2       0.85      0.81      0.83       400\n",
      "           3       0.87      0.85      0.86       400\n",
      "           4       0.91      0.90      0.90       400\n",
      "           5       0.82      0.82      0.82       400\n",
      "\n",
      "    accuracy                           0.84      2400\n",
      "   macro avg       0.85      0.84      0.84      2400\n",
      "weighted avg       0.85      0.84      0.84      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "grid.fit(tfidf_vectorizer_train,labels_train[1])\n",
    "\n",
    "labels_pred = grid.predict(tfidf_vectorizer_test)\n",
    "\n",
    "print(confusion_matrix(labels_test[1], labels_pred))\n",
    "print(metrics.classification_report(labels_test[1], labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "meP4M1LLgLTz",
    "outputId": "e49d12d1-f312-41b7-f144-9097e3382476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8444450997871643\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(labels_test[1], labels_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7IGjcKWg8Mo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EbvQQxhTxuV"
   },
   "source": [
    "### Observe the top predictors for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1cyO4T6zA8dB",
    "outputId": "dd86e1f8-d43e-4309-8d87-721df8a6604a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2}\n"
     ]
    }
   ],
   "source": [
    "words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "param = grid.best_params_\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Ex-yyaUuA8dE",
    "outputId": "3995292f-12df-4cca-d00a-06e99e4afff0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.LinearSVC(C=0.5)\n",
    "model.fit(tfidf_vectorizer_train,labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xYvpsp1JA8dH",
    "outputId": "e7786330-8070-4817-9f35-d5bdcbd6e5ca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44586367  0.46637921  0.01676967 ... -0.02122574 -0.02122574\n",
      " -0.02122574]\n",
      "87879\n"
     ]
    }
   ],
   "source": [
    "words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "coef = model.coef_[0]\n",
    "print(coef)\n",
    "\n",
    "print(len(coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oX4gBB6A8dL"
   },
   "outputs": [],
   "source": [
    "relevant_words = [(words[idx], c) for idx, c in enumerate(coef)]\n",
    "relevant_words.sort(key=lambda tup: abs(tup[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "Anq_7j9-A8dQ",
    "outputId": "ae6648d0-b34a-4525-ed53-79d5b42e2990",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('women', -2.512904852694919),\n",
       " ('the', 1.9638031282716166),\n",
       " ('yes', -1.7596613586266112),\n",
       " ('love', -1.7552786243824663),\n",
       " ('mate', 1.7011036708768705),\n",
       " ('game', 1.6458028987965705),\n",
       " ('woman', -1.5926499092368385),\n",
       " ('oh', -1.5591666074240185),\n",
       " ('xx', -1.5572704320302426),\n",
       " ('cute', -1.527151662159127),\n",
       " ('cry', -1.4699367613336254),\n",
       " ('excite', -1.4569822372280354),\n",
       " ('sleep', -1.2884669991219664),\n",
       " ('lovely', -1.28212283050248),\n",
       " ('play', 1.260305873192125),\n",
       " ('really', -1.2546474153388603),\n",
       " ('miss', -1.2508388937517938),\n",
       " ('hair', -1.2376598371683898),\n",
       " ('heart', -1.2290614282587402),\n",
       " ('fab', -1.2110874239149425),\n",
       " ('bro', 1.1999665788270135),\n",
       " ('years', 1.1952705416235558),\n",
       " ('thank', -1.1944720380728233),\n",
       " ('so', -1.1942902810296825),\n",
       " ('baby', -1.1915125601177075),\n",
       " ('husband', -1.1892965015022152),\n",
       " ('good', 1.1846253317686846),\n",
       " ('okay', -1.1535624887257665),\n",
       " ('more', -1.1098106388140823),\n",
       " ('if', 1.1078160025548347)]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCHcbtG3A8dT"
   },
   "outputs": [],
   "source": [
    "female = [w for w in relevant_words if w[1] < 0]\n",
    "male= [w for w in relevant_words if w[1] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "kMDZ2U9AA8dV",
    "outputId": "25758cc5-ccb0-4e83-8564-c1e118ca4e83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('women', -2.512904852694919),\n",
       " ('yes', -1.7596613586266112),\n",
       " ('love', -1.7552786243824663),\n",
       " ('woman', -1.5926499092368385),\n",
       " ('oh', -1.5591666074240185),\n",
       " ('xx', -1.5572704320302426),\n",
       " ('cute', -1.527151662159127),\n",
       " ('cry', -1.4699367613336254),\n",
       " ('excite', -1.4569822372280354),\n",
       " ('sleep', -1.2884669991219664),\n",
       " ('lovely', -1.28212283050248),\n",
       " ('really', -1.2546474153388603),\n",
       " ('miss', -1.2508388937517938),\n",
       " ('hair', -1.2376598371683898),\n",
       " ('heart', -1.2290614282587402),\n",
       " ('fab', -1.2110874239149425),\n",
       " ('thank', -1.1944720380728233),\n",
       " ('so', -1.1942902810296825),\n",
       " ('baby', -1.1915125601177075),\n",
       " ('husband', -1.1892965015022152)]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "ULWNwls1A8dZ",
    "outputId": "fbed161b-ea28-4d5d-e943-c6bc2a53bae8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1.9638031282716166),\n",
       " ('mate', 1.7011036708768705),\n",
       " ('game', 1.6458028987965705),\n",
       " ('play', 1.260305873192125),\n",
       " ('bro', 1.1999665788270135),\n",
       " ('years', 1.1952705416235558),\n",
       " ('good', 1.1846253317686846),\n",
       " ('if', 1.1078160025548347),\n",
       " ('man', 1.095239909006497),\n",
       " ('seem', 1.0780160622145363),\n",
       " ('fuck', 1.015029173029132),\n",
       " ('last', 0.9935142728212313),\n",
       " ('shit', 0.9840251878649682),\n",
       " ('gay', 0.9351377495079034),\n",
       " ('beer', 0.9145584938807096),\n",
       " ('team', 0.9091582072893521),\n",
       " ('apple', 0.8972397564901855),\n",
       " ('follow', 0.8875345656614726),\n",
       " ('gun', 0.8815135069994202),\n",
       " ('win', 0.8722037308425765),\n",
       " ('football', 0.8693533899534927),\n",
       " ('wife', 0.8526734479200061),\n",
       " ('album', 0.851458561135546),\n",
       " ('liberals', 0.8347453457052089),\n",
       " ('oppose', 0.8129879883371784),\n",
       " ('fine', 0.8090486845446687),\n",
       " ('beat', 0.7971616258515049),\n",
       " ('bet', 0.78403200033901),\n",
       " ('website', 0.7816891047315028),\n",
       " ('season', 0.774713278241528)]"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcuxY9ydA8dc"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2njlkf7YA8dd"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "uINfydgfA8df",
    "outputId": "8b1d7587-952f-4d8a-f0ca-e79db3251863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npath_glove = os.path.abspath('glove.twitter.27B/glove.twitter.27B.200d.txt')\\npath_w2v = os.path.abspath('glove.twitter.27B/glove.twitter.27B.200d_w2v.txt')\\n\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "'''\n",
    "path_glove = os.path.abspath('glove.twitter.27B/glove.twitter.27B.200d.txt')\n",
    "path_w2v = os.path.abspath('glove.twitter.27B/glove.twitter.27B.200d_w2v.txt')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rC05X_NA8di"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "glove_file = datapath(path_glove)\n",
    "tmp_file = get_tmpfile(path_w2v)\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "\"\"\"\n",
    "!cp '/content/drive/My Drive/Colab Notebooks/NLP/Data (1)/glove.twitter.27B.200d_w2v.txt' 'glove.twitter.27B.200d_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ptOJQfCVA8dk",
    "outputId": "f24280c8-c390-4b1f-ad0a-d7e965865d53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath('/content/glove.twitter.27B.200d_w2v.txt')\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "AOuKbhkjA8do",
    "outputId": "60b0d40c-f77a-4cea-dbdc-4d5bf586d01c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.8460860252380371),\n",
       " ('much', 0.7890045642852783),\n",
       " ('always', 0.7601683735847473),\n",
       " ('know', 0.7598055005073547),\n",
       " ('my', 0.7519949078559875),\n",
       " ('and', 0.7513089776039124),\n",
       " ('loves', 0.7512385845184326),\n",
       " ('life', 0.7443933486938477),\n",
       " ('it', 0.7426838874816895),\n",
       " (\"n't\", 0.7408115267753601)]"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "W8hMLPtwGwSL",
    "outputId": "e77ecb40-2cf7-409d-ac53-5470c5fcf488"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('boy', 0.8434211015701294),\n",
       " ('girls', 0.8288908004760742),\n",
       " ('she', 0.803076446056366),\n",
       " ('guy', 0.787306010723114),\n",
       " ('woman', 0.7817050218582153),\n",
       " ('chick', 0.7750228643417358),\n",
       " ('friend', 0.7702170014381409),\n",
       " ('bitch', 0.7611054182052612),\n",
       " ('that', 0.7493616938591003),\n",
       " ('pretty', 0.7465850710868835)]"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "etqpAHYpG0x8",
    "outputId": "6cb677c4-968e-48e9-a405-aa12655c2692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('girl', 0.8434211015701294),\n",
       " ('guy', 0.7662351131439209),\n",
       " ('kid', 0.7430557012557983),\n",
       " ('baby', 0.7402151823043823),\n",
       " ('dude', 0.7393355369567871),\n",
       " ('boys', 0.726765513420105),\n",
       " ('man', 0.7174534797668457),\n",
       " ('boi', 0.7088537216186523),\n",
       " ('nigga', 0.7040588855743408),\n",
       " ('brother', 0.7035762667655945)]"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['boy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "P2waeRosHELJ",
    "outputId": "068c0ee5-6dba-4a6c-dfd4-718f4abd6b1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dude', 0.7253456711769104),\n",
       " ('boy', 0.7174534797668457),\n",
       " ('guy', 0.6998012065887451),\n",
       " ('shit', 0.6852256655693054),\n",
       " ('was', 0.6779443621635437),\n",
       " (\"'s\", 0.6762615442276001),\n",
       " ('bad', 0.6734011173248291),\n",
       " ('men', 0.6714874505996704),\n",
       " ('that', 0.6709873080253601),\n",
       " ('lol', 0.6646673083305359)]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "DTjCRlNBRq39",
    "outputId": "7b37d8b4-009e-424b-ccfd-011f2c7c6fbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('girl', 0.7817050218582153),\n",
       " ('women', 0.7705847024917603),\n",
       " ('guy', 0.7154314517974854),\n",
       " ('she', 0.7104362845420837),\n",
       " ('person', 0.7034647464752197),\n",
       " ('wife', 0.7029582858085632),\n",
       " ('female', 0.7000529766082764),\n",
       " ('mother', 0.6994998455047607),\n",
       " ('lady', 0.6945761442184448),\n",
       " ('who', 0.6705518960952759)]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EscLwhZRrL_Y"
   },
   "outputs": [],
   "source": [
    "#Build vector by taking the mean of the W2V vector of each word\n",
    "def build_vector(t_train):\n",
    "    \n",
    "    w2v = []\n",
    "    \n",
    "    for doc in t_train:\n",
    "                   \n",
    "        mean = 0\n",
    "        count = 0\n",
    "        for word in doc.split():\n",
    "            try:\n",
    "                mean += model[word]\n",
    "                count +=1\n",
    "            except:\n",
    "                pass\n",
    "        w2v.append(mean/count)\n",
    "                   \n",
    "    return w2v\n",
    "        \n",
    "                \n",
    "train_w2v =build_vector(t_train)\n",
    "test_w2v = build_vector(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2UPA-ucfzNqe",
    "outputId": "82826897-2187-4835-da32-775d660eda44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[978 222]\n",
      " [263 937]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1200\n",
      "           1       0.81      0.78      0.79      1200\n",
      "\n",
      "    accuracy                           0.80      2400\n",
      "   macro avg       0.80      0.80      0.80      2400\n",
      "weighted avg       0.80      0.80      0.80      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "\n",
    "grid.fit(train_w2v,labels_train[0])\n",
    "\n",
    "labels_pred = grid.predict(test_w2v)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(labels_test[0], labels_pred, average='macro', pos_label=None)\n",
    "\n",
    "print(confusion_matrix(labels_test[0], labels_pred))\n",
    "print(metrics.classification_report(labels_test[0], labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "BrIrRJ3XzSUY",
    "outputId": "a4e026c1-894b-4981-cdf0-4457378d1254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235  33  12  32  67  21]\n",
      " [ 25 286   1  11  31  46]\n",
      " [ 25  15 247  64  34  15]\n",
      " [ 15  21  78 250  25  11]\n",
      " [ 40  25  22  19 281  13]\n",
      " [ 12  45   5   6  10 322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.59      0.62       400\n",
      "           1       0.67      0.71      0.69       400\n",
      "           2       0.68      0.62      0.65       400\n",
      "           3       0.65      0.62      0.64       400\n",
      "           4       0.63      0.70      0.66       400\n",
      "           5       0.75      0.81      0.78       400\n",
      "\n",
      "    accuracy                           0.68      2400\n",
      "   macro avg       0.68      0.68      0.67      2400\n",
      "weighted avg       0.68      0.68      0.67      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "\n",
    "grid.fit(train_w2v,labels_train[1])\n",
    "\n",
    "labels_pred = grid.predict(test_w2v)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(labels_test[1], labels_pred, average='macro', pos_label=None)\n",
    "\n",
    "print(confusion_matrix(labels_test[1], labels_pred))\n",
    "print(metrics.classification_report(labels_test[1], labels_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GjtSnP-A8d5"
   },
   "source": [
    "## Concatenate Word2Vec and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHR74kc3A8d6"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "#Select k-best\n",
    "\n",
    "#Concatenate tfidf and w2v vector\n",
    "train_concat=hstack((tfidf_vectorizer_train, np.array(train_w2v)))\n",
    "test_concat=hstack((tfidf_vectorizer_test, np.array(test_w2v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "It6QbTq6A8d9",
    "outputId": "08aa30aa-1676-41f0-8991-8399166c277f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 87879)\n",
      "(3600, 200)\n",
      "(3600, 88079)\n",
      "(3600,)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer_train.shape)\n",
    "print(np.array(train_w2v).shape)\n",
    "print(train_concat.shape)\n",
    "print(labels_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "LmcApyDiA8eD",
    "outputId": "637cb343-d868-46ef-de51-6211aabe281e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[979 221]\n",
      " [257 943]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1200\n",
      "           1       0.81      0.79      0.80      1200\n",
      "\n",
      "    accuracy                           0.80      2400\n",
      "   macro avg       0.80      0.80      0.80      2400\n",
      "weighted avg       0.80      0.80      0.80      2400\n",
      "\n",
      "0.7978003384094755\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=1, scoring=\"f1_macro\", cv=5)\n",
    "\n",
    "grid.fit(train_concat,labels_train[0])\n",
    "\n",
    "labels_pred = grid.predict(test_concat)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(labels_test[0], labels_pred, average='macro', pos_label=None)\n",
    "\n",
    "print(confusion_matrix(labels_test[0], labels_pred))\n",
    "print(metrics.classification_report(labels_test[0], labels_pred))\n",
    "print(metrics.f1_score(labels_test[0], labels_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80GkOJH6Qzkr"
   },
   "source": [
    "# Visualize with Dimension Reduction (TSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7KEEInYBQzkt"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def compute_dor(TR, ndocs_out=0, y=None):    \n",
    "    TR_subset_in = None\n",
    "    if y is None:\n",
    "        TR_subset_in, TR_subset_out = train_test_split(TR, test_size=ndocs_out, random_state=1)\n",
    "    else :\n",
    "        TR_subset_in, TR_subset_out = train_test_split(TR, stratify=y, test_size=ndocs_out, random_state=1)\n",
    "    \n",
    "    DTR = np.zeros((TR_subset_in.shape[1], TR_subset_in.shape[0]), dtype=np.float)\n",
    "    print(\"DOR: Shape of the input matrix (BoT):\", TR.shape)\n",
    "    print(\"DOR: Shape of the term-feat matrix:\", DTR.shape)\n",
    "\n",
    "    tam_V = TR_subset_in.shape[1]\n",
    "    for i, doc in enumerate(TR_subset_in):\n",
    "        nonzero_positions = np.nonzero(doc)[0] # returns a tuple of n-dimensions. Since we have 1D array docs, it returns a tuple with one element. thus, we get the 0 index.\n",
    "        tam_v = len(nonzero_positions) \n",
    "        for term in nonzero_positions:\n",
    "            DTR[term, i] = (1 + math.log10(doc[term])) * math.log10(tam_V/tam_v)\n",
    "    return DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8YzUm3bQzkw"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "DOR_base = compute_dor(tfidf_vectorizer_train.toarray(), 0.01)\n",
    "DOR_base = preprocessing.normalize(DOR_base, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sK7VkUw0Qzk0"
   },
   "outputs": [],
   "source": [
    "#Select best 1000 best features\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "feats = SelectKBest(chi2, k=1000)\n",
    "feats.fit(tfidf_vectorizer_train, labels_train[0])\n",
    "\n",
    "best=feats.get_support(indices=True)\n",
    "print('Reduce from {} features to {} features'.format(tfidf_vectorizer_train[1],best.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KC0Wgg1_DPGY"
   },
   "outputs": [],
   "source": [
    "dict_indices = {}\n",
    "for i, w in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "  dict_indices[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZsTqJBKQzk4"
   },
   "outputs": [],
   "source": [
    "#give the word provided the index\n",
    "dict_indice_invert = {}\n",
    "for w in dict_indices:\n",
    "    dict_indice_invert[dict_indices[w]] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qv6OZisfQzk6"
   },
   "outputs": [],
   "source": [
    "#Select the most important words\n",
    "target_words = [dict_indice_invert[index] for index in best]\n",
    "\n",
    "#Select important and most frequent words\n",
    "words = [word for word in target_words if word in dict_indices]\n",
    "\n",
    "#Find vector of these words\n",
    "target_matrix = np.array([DOR_base[dict_indices[word]] for word in words])\n",
    "target_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vRTV5YdQzk9"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/My Drive/Colab Notebooks/NLP/tsne_python/tsne.py' 'tsne.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVZc4nY5Qzk_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reduce the dimension\n",
    "\n",
    "from tsne import tsne\n",
    "\n",
    "reduced_matrix = tsne(target_matrix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gRvU39uQzlB"
   },
   "outputs": [],
   "source": [
    "#Set the limit of the axes\n",
    "max_x = np.amax(reduced_matrix, axis=0)[0]\n",
    "max_y = np.amax(reduced_matrix, axis=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqlIyFNNQzlD"
   },
   "outputs": [],
   "source": [
    "reduced_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEw3JC1GQzlJ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,40), dpi=100); plt.xlim((-max_x,max_x)); plt.ylim((-max_y,max_y))\n",
    "plt.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], 20);\n",
    "for i, word in enumerate(words):\n",
    "    x = reduced_matrix[i, 0]\n",
    "    y = reduced_matrix[i, 1]\n",
    "    plt.annotate(word, (x,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sJYl4Ey48Vx"
   },
   "source": [
    "# BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "id": "MBrQYY114_jC",
    "outputId": "60715a88-8eaa-42fc-efcb-4e40e6e04a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184kB 12.7MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 870kB 29.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.38.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.12.33)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.0MB 52.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.15.33)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->pytorch-transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->pytorch-transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=56a83e2203770a480b99ebc06bb942f809c8bee8338c1a7cf2acce76e8b192a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.38 sentencepiece-0.1.85\n",
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 13.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.33)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.33)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.33->boto3->pytorch-pretrained-bert) (1.12.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers\n",
    "!pip install pytorch-pretrained-bert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "WnAVqvIAX-i8",
    "outputId": "5b9ed0ed-2d72-4c85-a50f-1320e57b14b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 552kB 23.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.7MB 51.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.33)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.33)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (0.15.2)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.5.2 transformers-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EC2itlxlYebw"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "f681e7324d5f42798eea30a608a4ea66",
      "f4760dce9b064f15b36b72c132e9b020",
      "6a133aa3d96544cc8db36621d6aba780",
      "865122ced3b54394aef5e6c05ba7ab7a",
      "e7f2f514769e409b9d20470fda881160",
      "e74f99b5ba934ec9b4ef67e7be64d3aa",
      "12cfca9ab73e4734848ef284cf093e96",
      "8eed7a5cf47343f1b3995e1e57c90287",
      "3fab529cfa1b40d5acccd7ac79adb60b",
      "0a366769da444fb4bec573f68331d248",
      "52bcb9acc09c49ce8a1fd1c114d5e1ad",
      "440aeaf5fede4eb78085ba019d547787",
      "7dc898effe4646d1b090d0b1b1ec4b40",
      "d82b3051bf8e4190b075983ef4331189",
      "1ce20293a77d463bbec7fd22df885c1e",
      "1adba141b5564c6ea69bad5b5ca6397e",
      "c0556a3d98fa442981f556f5f924d1d8",
      "4c09464bf9544bf8ab6fe1c35f858060",
      "2e45a2fe644749839b4ee624482110fa",
      "3dc243aa3e7b45578406666f14e53741",
      "a1e2b980f97046498ff8123a95f82927",
      "c6544f71606443f092031ada9ed9ed2b",
      "be71f2fd7e574e6f832583159cb458f2",
      "2a6b799add46408682a0e6179180919d"
     ]
    },
    "colab_type": "code",
    "id": "9Pa9iiSj-vqq",
    "outputId": "a7420c8a-d726-4879-e573-a69ee9f2d0bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f681e7324d5f42798eea30a608a4ea66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid‚Ä¶"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fab529cfa1b40d5acccd7ac79adb60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=‚Ä¶"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0556a3d98fa442981f556f5f924d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xoWS93mp_ncu"
   },
   "outputs": [],
   "source": [
    "def get_bertinf(text):\n",
    "  ttokens=[]\n",
    "  for t in text:\n",
    "    it = tokenizer.encode(t, add_special_tokens=True, max_length=50, pad_to_max_length=True)\n",
    "\n",
    "    ttokens.append(it)\n",
    "\n",
    "  return torch.tensor(ttokens),(torch.tensor(ttokens)>0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mK--fc8rAP8R"
   },
   "outputs": [],
   "source": [
    "def BERT_eval(ttokens, mask):\n",
    "\n",
    "  return model(ttokens, attention_mask=mask)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kyy0OncMkgi"
   },
   "outputs": [],
   "source": [
    "def convert_texts_2(files_train, path_train):\n",
    "  doc = []\n",
    "  for i in range(len(files_train)):\n",
    "      #Append the tweets to the corresponding document\n",
    "      try:\n",
    "          doc1 =[r.text for r in ET.parse(join(path_train,files_train[i])).getroot()[0]]\n",
    "          doc.append(doc1)\n",
    "      except:\n",
    "          print(files_train[i])\n",
    "  return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pg2JeLWRO2cb",
    "outputId": "4a65fd32-ad22-437d-d953-21fbc71aa2e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth.txt\n",
      "truth.txt\n"
     ]
    }
   ],
   "source": [
    "text_train = convert_texts_2(files_train, path_train)\n",
    "text_test = convert_texts_2(files_test, path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTCg59KHARho"
   },
   "outputs": [],
   "source": [
    "MAX = 50\n",
    "\n",
    "def text_to_tensor(tweet_text):\n",
    "  ttokens=[]\n",
    "  masks=[]\n",
    "  for i in range(len(tweet_text)):\n",
    "    ttoken, mask = get_bertinf(tweet_text[i])\n",
    "    ttokens.append(ttoken)\n",
    "    masks.append(mask)\n",
    "\n",
    "  return ttokens, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rC9kvMzEbil"
   },
   "outputs": [],
   "source": [
    "labels_train= torch.tensor(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76lQcqlbyY98"
   },
   "outputs": [],
   "source": [
    "tokens_train, masks_train = text_to_tensor(text_train)\n",
    "tokens_test, masks_test = text_to_tensor(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjDrLlwSF5K6"
   },
   "outputs": [],
   "source": [
    "def tensor_to_vec(ttokens, masks):\n",
    "  Vec = []\n",
    "  for t, m in zip(ttokens, masks):\n",
    "    t, m = t.cuda(), m.cuda()\n",
    "\n",
    "    #1 is the presentation of the tweets\n",
    "    y_pred = BERT_eval(t,m)[1]\n",
    "    #Average of the tweets by features of one profile\n",
    "    Vec.append(y_pred.cpu().detach().mean(0).unsqueeze(0))\n",
    "\n",
    "  #Turn the list of arrays into an array\n",
    "  #0 is the first axis to concatenate \n",
    "  return torch.cat(Vec, 0)\n",
    "\n",
    "\n",
    "lb = labels_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyubDHOLphrW"
   },
   "outputs": [],
   "source": [
    "X_train = tensor_to_vec(tokens_train, masks_train)\n",
    "X_test = tensor_to_vec(tokens_test, masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LTLPkklowVLx",
    "outputId": "fbd45ccf-1b9b-4dbf-b173-a3178c962620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3600, 768])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EhstPoLBsrtN",
    "outputId": "51b95279-f4d1-4dcb-d48f-e68229cde19f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600,)"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06SaA6dXodWK"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "ltll0qM3GXc8",
    "outputId": "4b83ff60-89fb-4445-fe14-c90b0bdb3e34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, lb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inNjOHyGsnJI"
   },
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1oSNh0GDw8o2",
    "outputId": "5ba948ae-6136-4327-e965-178671cf2e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  \t Accuracy: 0.7817\t F1: 0.7904\n"
     ]
    }
   ],
   "source": [
    "acc=metrics.accuracy_score(labels_test[0], y_pred)\n",
    "f1=metrics.f1_score(labels_test[0], y_pred)\n",
    "\n",
    "print(\"\\n  \\t Accuracy: %.4f\\t F1: %.4f\"%( acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8xQy8CJ20HU"
   },
   "outputs": [],
   "source": [
    "acc=metrics.accuracy_score(labels_test[0], y_pred)\n",
    "f1=metrics.f1_score(labels_test[0], y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_WDx2pSc1Hx"
   },
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQ7xjpCh56m"
   },
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCu7bDiCcue5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJoI_xAGdE4O"
   },
   "outputs": [],
   "source": [
    "class Gender(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Gender, self).__init__()\n",
    "    self.den1=nn.Linear(768, 500)\n",
    "    self.den2=nn.Linear(500, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.den1(x))\n",
    "    \n",
    "    x = self.den2(x)\n",
    "\n",
    "    #The sum of the output of 1 for the loss function\n",
    "    return F.log_softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGbhTzjGfb6J"
   },
   "outputs": [],
   "source": [
    "gender = Gender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PC7o7Nw2fgci",
    "outputId": "79c04d32-57ca-4251-a0c4-37bfce5fa056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3600, 768])\n",
      "torch.Size([3600])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(labels_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iemmcUb0fnIg"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "X_train = X_train.view(-1, batch_size, 768)\n",
    "labels_train_gender = labels_train[0].view(-1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sDArEqWfiQL_",
    "outputId": "e4b15eab-658f-4025-8b5d-63576e031239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 50, 768])\n",
      "torch.Size([72, 50])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(labels_train_gender.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "g8KfK5ufiVeB",
    "outputId": "51abb849-ffb2-4f4e-fb71-7a9ec40fd766"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6451, -0.7436],\n",
       "        [-0.6384, -0.7511],\n",
       "        [-0.6441, -0.7447],\n",
       "        [-0.6392, -0.7502],\n",
       "        [-0.6446, -0.7441],\n",
       "        [-0.6442, -0.7447],\n",
       "        [-0.6345, -0.7555],\n",
       "        [-0.6363, -0.7535],\n",
       "        [-0.6462, -0.7424],\n",
       "        [-0.6376, -0.7520],\n",
       "        [-0.6481, -0.7403],\n",
       "        [-0.6490, -0.7394],\n",
       "        [-0.6424, -0.7466],\n",
       "        [-0.6364, -0.7533],\n",
       "        [-0.6491, -0.7392],\n",
       "        [-0.6454, -0.7433],\n",
       "        [-0.6369, -0.7527],\n",
       "        [-0.6515, -0.7366],\n",
       "        [-0.6410, -0.7482],\n",
       "        [-0.6390, -0.7504],\n",
       "        [-0.6408, -0.7484],\n",
       "        [-0.6373, -0.7523],\n",
       "        [-0.6391, -0.7503],\n",
       "        [-0.6405, -0.7488],\n",
       "        [-0.6365, -0.7532],\n",
       "        [-0.6506, -0.7375],\n",
       "        [-0.6440, -0.7449],\n",
       "        [-0.6295, -0.7611],\n",
       "        [-0.6443, -0.7445],\n",
       "        [-0.6413, -0.7478],\n",
       "        [-0.6407, -0.7485],\n",
       "        [-0.6451, -0.7436],\n",
       "        [-0.6408, -0.7484],\n",
       "        [-0.6512, -0.7369],\n",
       "        [-0.6391, -0.7503],\n",
       "        [-0.6428, -0.7462],\n",
       "        [-0.6383, -0.7511],\n",
       "        [-0.6428, -0.7461],\n",
       "        [-0.6379, -0.7516],\n",
       "        [-0.6379, -0.7516],\n",
       "        [-0.6379, -0.7516],\n",
       "        [-0.6474, -0.7411],\n",
       "        [-0.6433, -0.7456],\n",
       "        [-0.6381, -0.7514],\n",
       "        [-0.6423, -0.7467],\n",
       "        [-0.6394, -0.7499],\n",
       "        [-0.6383, -0.7512],\n",
       "        [-0.6330, -0.7571],\n",
       "        [-0.6404, -0.7489],\n",
       "        [-0.6436, -0.7453]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91q4Ed9UioyA"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gender.parameters(), lr=1e-5)\n",
    "criterio = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6GMXBXPTjtyP",
    "outputId": "0e12b167-4c1a-46bd-9271-c1d101812d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t Loss: 0.6884672642 \t\n",
      "Epoch: 0 \t Batch: 10 \t Loss: 0.6927222013 \t\n",
      "Epoch: 0 \t Batch: 20 \t Loss: 0.6914150715 \t\n",
      "Epoch: 0 \t Batch: 30 \t Loss: 0.6872707009 \t\n",
      "Epoch: 0 \t Batch: 40 \t Loss: 0.6880311966 \t\n",
      "Epoch: 0 \t Batch: 50 \t Loss: 0.7030938864 \t\n",
      "Epoch: 0 \t Batch: 60 \t Loss: 0.7147867084 \t\n",
      "Epoch: 0 \t Batch: 70 \t Loss: 0.6824566126 \t\n",
      "Epoch: 1 \t Batch: 0 \t Loss: 0.6872223616 \t\n",
      "Epoch: 1 \t Batch: 10 \t Loss: 0.7062281370 \t\n",
      "Epoch: 1 \t Batch: 20 \t Loss: 0.7490071654 \t\n",
      "Epoch: 1 \t Batch: 30 \t Loss: 0.7268819213 \t\n",
      "Epoch: 1 \t Batch: 40 \t Loss: 0.7216072679 \t\n",
      "Epoch: 1 \t Batch: 50 \t Loss: 0.6875697970 \t\n",
      "Epoch: 1 \t Batch: 60 \t Loss: 0.7681688666 \t\n",
      "Epoch: 1 \t Batch: 70 \t Loss: 0.7835636735 \t\n",
      "Epoch: 2 \t Batch: 0 \t Loss: 0.8378089666 \t\n",
      "Epoch: 2 \t Batch: 10 \t Loss: 0.8669942021 \t\n",
      "Epoch: 2 \t Batch: 20 \t Loss: 0.8228966594 \t\n",
      "Epoch: 2 \t Batch: 30 \t Loss: 0.7604531646 \t\n",
      "Epoch: 2 \t Batch: 40 \t Loss: 0.6873018742 \t\n",
      "Epoch: 2 \t Batch: 50 \t Loss: 0.6927984953 \t\n",
      "Epoch: 2 \t Batch: 60 \t Loss: 0.7020307183 \t\n",
      "Epoch: 2 \t Batch: 70 \t Loss: 0.7953366637 \t\n",
      "Epoch: 3 \t Batch: 0 \t Loss: 0.7695307732 \t\n",
      "Epoch: 3 \t Batch: 10 \t Loss: 0.8063966632 \t\n",
      "Epoch: 3 \t Batch: 20 \t Loss: 0.8376531601 \t\n",
      "Epoch: 3 \t Batch: 30 \t Loss: 0.8271179199 \t\n",
      "Epoch: 3 \t Batch: 40 \t Loss: 0.9031623006 \t\n",
      "Epoch: 3 \t Batch: 50 \t Loss: 0.8086093068 \t\n",
      "Epoch: 3 \t Batch: 60 \t Loss: 0.7446259260 \t\n",
      "Epoch: 3 \t Batch: 70 \t Loss: 0.7508708239 \t\n",
      "Epoch: 4 \t Batch: 0 \t Loss: 0.7148454785 \t\n",
      "Epoch: 4 \t Batch: 10 \t Loss: 0.6904315352 \t\n",
      "Epoch: 4 \t Batch: 20 \t Loss: 0.6810060143 \t\n",
      "Epoch: 4 \t Batch: 30 \t Loss: 0.6866762042 \t\n",
      "Epoch: 4 \t Batch: 40 \t Loss: 0.6846427917 \t\n",
      "Epoch: 4 \t Batch: 50 \t Loss: 0.7122886181 \t\n",
      "Epoch: 4 \t Batch: 60 \t Loss: 0.7513172030 \t\n",
      "Epoch: 4 \t Batch: 70 \t Loss: 0.7256219983 \t\n",
      "Epoch: 5 \t Batch: 0 \t Loss: 0.7582803965 \t\n",
      "Epoch: 5 \t Batch: 10 \t Loss: 0.7703898549 \t\n",
      "Epoch: 5 \t Batch: 20 \t Loss: 0.7709356546 \t\n",
      "Epoch: 5 \t Batch: 30 \t Loss: 0.7812968493 \t\n",
      "Epoch: 5 \t Batch: 40 \t Loss: 0.7240380645 \t\n",
      "Epoch: 5 \t Batch: 50 \t Loss: 0.7592641115 \t\n",
      "Epoch: 5 \t Batch: 60 \t Loss: 0.7817147970 \t\n",
      "Epoch: 5 \t Batch: 70 \t Loss: 0.7307163477 \t\n",
      "Epoch: 6 \t Batch: 0 \t Loss: 0.7625198364 \t\n",
      "Epoch: 6 \t Batch: 10 \t Loss: 0.7574947476 \t\n",
      "Epoch: 6 \t Batch: 20 \t Loss: 0.7449080348 \t\n",
      "Epoch: 6 \t Batch: 30 \t Loss: 0.7420554757 \t\n",
      "Epoch: 6 \t Batch: 40 \t Loss: 0.6958754659 \t\n",
      "Epoch: 6 \t Batch: 50 \t Loss: 0.7136622667 \t\n",
      "Epoch: 6 \t Batch: 60 \t Loss: 0.7202035189 \t\n",
      "Epoch: 6 \t Batch: 70 \t Loss: 0.6922230721 \t\n",
      "Epoch: 7 \t Batch: 0 \t Loss: 0.7068361044 \t\n",
      "Epoch: 7 \t Batch: 10 \t Loss: 0.7024263740 \t\n",
      "Epoch: 7 \t Batch: 20 \t Loss: 0.6972622275 \t\n",
      "Epoch: 7 \t Batch: 30 \t Loss: 0.6959279776 \t\n",
      "Epoch: 7 \t Batch: 40 \t Loss: 0.6873330474 \t\n",
      "Epoch: 7 \t Batch: 50 \t Loss: 0.6905745268 \t\n",
      "Epoch: 7 \t Batch: 60 \t Loss: 0.6893805861 \t\n",
      "Epoch: 7 \t Batch: 70 \t Loss: 0.6913866997 \t\n",
      "Epoch: 8 \t Batch: 0 \t Loss: 0.6894801855 \t\n",
      "Epoch: 8 \t Batch: 10 \t Loss: 0.6896033883 \t\n",
      "Epoch: 8 \t Batch: 20 \t Loss: 0.6919924617 \t\n",
      "Epoch: 8 \t Batch: 30 \t Loss: 0.6917157769 \t\n",
      "Epoch: 8 \t Batch: 40 \t Loss: 0.7075282931 \t\n",
      "Epoch: 8 \t Batch: 50 \t Loss: 0.6988660097 \t\n",
      "Epoch: 8 \t Batch: 60 \t Loss: 0.6917167902 \t\n",
      "Epoch: 8 \t Batch: 70 \t Loss: 0.7060047388 \t\n",
      "Epoch: 9 \t Batch: 0 \t Loss: 0.6956373453 \t\n",
      "Epoch: 9 \t Batch: 10 \t Loss: 0.6953192353 \t\n",
      "Epoch: 9 \t Batch: 20 \t Loss: 0.6976308227 \t\n",
      "Epoch: 9 \t Batch: 30 \t Loss: 0.6951563954 \t\n",
      "Epoch: 9 \t Batch: 40 \t Loss: 0.7148236632 \t\n",
      "Epoch: 9 \t Batch: 50 \t Loss: 0.7018839121 \t\n",
      "Epoch: 9 \t Batch: 60 \t Loss: 0.6928568482 \t\n",
      "Epoch: 9 \t Batch: 70 \t Loss: 0.7084404826 \t\n",
      "Epoch: 10 \t Batch: 0 \t Loss: 0.6966696382 \t\n",
      "Epoch: 10 \t Batch: 10 \t Loss: 0.6959010959 \t\n",
      "Epoch: 10 \t Batch: 20 \t Loss: 0.6978260875 \t\n",
      "Epoch: 10 \t Batch: 30 \t Loss: 0.6949280500 \t\n",
      "Epoch: 10 \t Batch: 40 \t Loss: 0.7141602635 \t\n",
      "Epoch: 10 \t Batch: 50 \t Loss: 0.7009965181 \t\n",
      "Epoch: 10 \t Batch: 60 \t Loss: 0.6922267675 \t\n",
      "Epoch: 10 \t Batch: 70 \t Loss: 0.7066943645 \t\n",
      "Epoch: 11 \t Batch: 0 \t Loss: 0.6956313848 \t\n",
      "Epoch: 11 \t Batch: 10 \t Loss: 0.6947730780 \t\n",
      "Epoch: 11 \t Batch: 20 \t Loss: 0.6964634657 \t\n",
      "Epoch: 11 \t Batch: 30 \t Loss: 0.6938014030 \t\n",
      "Epoch: 11 \t Batch: 40 \t Loss: 0.7111109495 \t\n",
      "Epoch: 11 \t Batch: 50 \t Loss: 0.6991414428 \t\n",
      "Epoch: 11 \t Batch: 60 \t Loss: 0.6913825274 \t\n",
      "Epoch: 11 \t Batch: 70 \t Loss: 0.7039175630 \t\n",
      "Epoch: 12 \t Batch: 0 \t Loss: 0.6942747235 \t\n",
      "Epoch: 12 \t Batch: 10 \t Loss: 0.6934309006 \t\n",
      "Epoch: 12 \t Batch: 20 \t Loss: 0.6948055625 \t\n",
      "Epoch: 12 \t Batch: 30 \t Loss: 0.6925425529 \t\n",
      "Epoch: 12 \t Batch: 40 \t Loss: 0.7070136070 \t\n",
      "Epoch: 12 \t Batch: 50 \t Loss: 0.6969443560 \t\n",
      "Epoch: 12 \t Batch: 60 \t Loss: 0.6906135678 \t\n",
      "Epoch: 12 \t Batch: 70 \t Loss: 0.7006626725 \t\n",
      "Epoch: 13 \t Batch: 0 \t Loss: 0.6929121017 \t\n",
      "Epoch: 13 \t Batch: 10 \t Loss: 0.6921898723 \t\n",
      "Epoch: 13 \t Batch: 20 \t Loss: 0.6932241917 \t\n",
      "Epoch: 13 \t Batch: 30 \t Loss: 0.6914891601 \t\n",
      "Epoch: 13 \t Batch: 40 \t Loss: 0.7024842501 \t\n",
      "Epoch: 13 \t Batch: 50 \t Loss: 0.6948164701 \t\n",
      "Epoch: 13 \t Batch: 60 \t Loss: 0.6902471185 \t\n",
      "Epoch: 13 \t Batch: 70 \t Loss: 0.6973483562 \t\n",
      "Epoch: 14 \t Batch: 0 \t Loss: 0.6918733716 \t\n",
      "Epoch: 14 \t Batch: 10 \t Loss: 0.6913455129 \t\n",
      "Epoch: 14 \t Batch: 20 \t Loss: 0.6920693517 \t\n",
      "Epoch: 14 \t Batch: 30 \t Loss: 0.6909552813 \t\n",
      "Epoch: 14 \t Batch: 40 \t Loss: 0.6982618570 \t\n",
      "Epoch: 14 \t Batch: 50 \t Loss: 0.6931533217 \t\n",
      "Epoch: 14 \t Batch: 60 \t Loss: 0.6905289292 \t\n",
      "Epoch: 14 \t Batch: 70 \t Loss: 0.6944864988 \t\n",
      "Epoch: 15 \t Batch: 0 \t Loss: 0.6914563179 \t\n",
      "Epoch: 15 \t Batch: 10 \t Loss: 0.6911717653 \t\n",
      "Epoch: 15 \t Batch: 20 \t Loss: 0.6915422678 \t\n",
      "Epoch: 15 \t Batch: 30 \t Loss: 0.6911143661 \t\n",
      "Epoch: 15 \t Batch: 40 \t Loss: 0.6943295002 \t\n",
      "Epoch: 15 \t Batch: 50 \t Loss: 0.6921969652 \t\n",
      "Epoch: 15 \t Batch: 60 \t Loss: 0.6915050149 \t\n",
      "Epoch: 15 \t Batch: 70 \t Loss: 0.6923606992 \t\n",
      "Epoch: 16 \t Batch: 0 \t Loss: 0.6917387247 \t\n",
      "Epoch: 16 \t Batch: 10 \t Loss: 0.6916976571 \t\n",
      "Epoch: 16 \t Batch: 20 \t Loss: 0.6917523146 \t\n",
      "Epoch: 16 \t Batch: 30 \t Loss: 0.6919479370 \t\n",
      "Epoch: 16 \t Batch: 40 \t Loss: 0.6914350390 \t\n",
      "Epoch: 16 \t Batch: 50 \t Loss: 0.6919416785 \t\n",
      "Epoch: 16 \t Batch: 60 \t Loss: 0.6930416822 \t\n",
      "Epoch: 16 \t Batch: 70 \t Loss: 0.6910010576 \t\n",
      "Epoch: 17 \t Batch: 0 \t Loss: 0.6926050186 \t\n",
      "Epoch: 17 \t Batch: 10 \t Loss: 0.6927771568 \t\n",
      "Epoch: 17 \t Batch: 20 \t Loss: 0.6925280690 \t\n",
      "Epoch: 17 \t Batch: 30 \t Loss: 0.6932613254 \t\n",
      "Epoch: 17 \t Batch: 40 \t Loss: 0.6893952489 \t\n",
      "Epoch: 17 \t Batch: 50 \t Loss: 0.6922305226 \t\n",
      "Epoch: 17 \t Batch: 60 \t Loss: 0.6948878169 \t\n",
      "Epoch: 17 \t Batch: 70 \t Loss: 0.6902948022 \t\n",
      "Epoch: 18 \t Batch: 0 \t Loss: 0.6938413382 \t\n",
      "Epoch: 18 \t Batch: 10 \t Loss: 0.6941972971 \t\n",
      "Epoch: 18 \t Batch: 20 \t Loss: 0.6936993003 \t\n",
      "Epoch: 18 \t Batch: 30 \t Loss: 0.6948658228 \t\n",
      "Epoch: 18 \t Batch: 40 \t Loss: 0.6882793307 \t\n",
      "Epoch: 18 \t Batch: 50 \t Loss: 0.6929808855 \t\n",
      "Epoch: 18 \t Batch: 60 \t Loss: 0.6967922449 \t\n",
      "Epoch: 18 \t Batch: 70 \t Loss: 0.6903969049 \t\n",
      "Epoch: 19 \t Batch: 0 \t Loss: 0.6953143477 \t\n",
      "Epoch: 19 \t Batch: 10 \t Loss: 0.6956692934 \t\n",
      "Epoch: 19 \t Batch: 20 \t Loss: 0.6949655414 \t\n",
      "Epoch: 19 \t Batch: 30 \t Loss: 0.6962639093 \t\n",
      "Epoch: 19 \t Batch: 40 \t Loss: 0.6879230738 \t\n",
      "Epoch: 19 \t Batch: 50 \t Loss: 0.6936274767 \t\n",
      "Epoch: 19 \t Batch: 60 \t Loss: 0.6979057193 \t\n",
      "Epoch: 19 \t Batch: 70 \t Loss: 0.6905636787 \t\n",
      "Epoch: 20 \t Batch: 0 \t Loss: 0.6961184144 \t\n",
      "Epoch: 20 \t Batch: 10 \t Loss: 0.6964965463 \t\n",
      "Epoch: 20 \t Batch: 20 \t Loss: 0.6956352592 \t\n",
      "Epoch: 20 \t Batch: 30 \t Loss: 0.6969246268 \t\n",
      "Epoch: 20 \t Batch: 40 \t Loss: 0.6877964139 \t\n",
      "Epoch: 20 \t Batch: 50 \t Loss: 0.6939132810 \t\n",
      "Epoch: 20 \t Batch: 60 \t Loss: 0.6982705593 \t\n",
      "Epoch: 20 \t Batch: 70 \t Loss: 0.6906383038 \t\n",
      "Epoch: 21 \t Batch: 0 \t Loss: 0.6963336468 \t\n",
      "Epoch: 21 \t Batch: 10 \t Loss: 0.6966508627 \t\n",
      "Epoch: 21 \t Batch: 20 \t Loss: 0.6957124472 \t\n",
      "Epoch: 21 \t Batch: 30 \t Loss: 0.6969264150 \t\n",
      "Epoch: 21 \t Batch: 40 \t Loss: 0.6878963709 \t\n",
      "Epoch: 21 \t Batch: 50 \t Loss: 0.6938661933 \t\n",
      "Epoch: 21 \t Batch: 60 \t Loss: 0.6979449987 \t\n",
      "Epoch: 21 \t Batch: 70 \t Loss: 0.6906557679 \t\n",
      "Epoch: 22 \t Batch: 0 \t Loss: 0.6960273981 \t\n",
      "Epoch: 22 \t Batch: 10 \t Loss: 0.6962668300 \t\n",
      "Epoch: 22 \t Batch: 20 \t Loss: 0.6953600645 \t\n",
      "Epoch: 22 \t Batch: 30 \t Loss: 0.6964252591 \t\n",
      "Epoch: 22 \t Batch: 40 \t Loss: 0.6882231832 \t\n",
      "Epoch: 22 \t Batch: 50 \t Loss: 0.6935938597 \t\n",
      "Epoch: 22 \t Batch: 60 \t Loss: 0.6971641779 \t\n",
      "Epoch: 22 \t Batch: 70 \t Loss: 0.6907042265 \t\n",
      "Epoch: 23 \t Batch: 0 \t Loss: 0.6954184175 \t\n",
      "Epoch: 23 \t Batch: 10 \t Loss: 0.6955953836 \t\n",
      "Epoch: 23 \t Batch: 20 \t Loss: 0.6947965026 \t\n",
      "Epoch: 23 \t Batch: 30 \t Loss: 0.6956857443 \t\n",
      "Epoch: 23 \t Batch: 40 \t Loss: 0.6887296438 \t\n",
      "Epoch: 23 \t Batch: 50 \t Loss: 0.6932578087 \t\n",
      "Epoch: 23 \t Batch: 60 \t Loss: 0.6962072849 \t\n",
      "Epoch: 23 \t Batch: 70 \t Loss: 0.6907988191 \t\n",
      "Epoch: 24 \t Batch: 0 \t Loss: 0.6947135925 \t\n",
      "Epoch: 24 \t Batch: 10 \t Loss: 0.6948570013 \t\n",
      "Epoch: 24 \t Batch: 20 \t Loss: 0.6941874027 \t\n",
      "Epoch: 24 \t Batch: 30 \t Loss: 0.6948972940 \t\n",
      "Epoch: 24 \t Batch: 40 \t Loss: 0.6893171668 \t\n",
      "Epoch: 24 \t Batch: 50 \t Loss: 0.6929539442 \t\n",
      "Epoch: 24 \t Batch: 60 \t Loss: 0.6952971220 \t\n",
      "Epoch: 24 \t Batch: 70 \t Loss: 0.6909540892 \t\n",
      "Epoch: 25 \t Batch: 0 \t Loss: 0.6940916181 \t\n",
      "Epoch: 25 \t Batch: 10 \t Loss: 0.6942186356 \t\n",
      "Epoch: 25 \t Batch: 20 \t Loss: 0.6936668158 \t\n",
      "Epoch: 25 \t Batch: 30 \t Loss: 0.6942623854 \t\n",
      "Epoch: 25 \t Batch: 40 \t Loss: 0.6899259686 \t\n",
      "Epoch: 25 \t Batch: 50 \t Loss: 0.6927725077 \t\n",
      "Epoch: 25 \t Batch: 60 \t Loss: 0.6945989728 \t\n",
      "Epoch: 25 \t Batch: 70 \t Loss: 0.6912155747 \t\n",
      "Epoch: 26 \t Batch: 0 \t Loss: 0.6936495900 \t\n",
      "Epoch: 26 \t Batch: 10 \t Loss: 0.6937489510 \t\n",
      "Epoch: 26 \t Batch: 20 \t Loss: 0.6933280826 \t\n",
      "Epoch: 26 \t Batch: 30 \t Loss: 0.6938055754 \t\n",
      "Epoch: 26 \t Batch: 40 \t Loss: 0.6905378699 \t\n",
      "Epoch: 26 \t Batch: 50 \t Loss: 0.6926981807 \t\n",
      "Epoch: 26 \t Batch: 60 \t Loss: 0.6940858960 \t\n",
      "Epoch: 26 \t Batch: 70 \t Loss: 0.6915267706 \t\n",
      "Epoch: 27 \t Batch: 0 \t Loss: 0.6933641434 \t\n",
      "Epoch: 27 \t Batch: 10 \t Loss: 0.6934379339 \t\n",
      "Epoch: 27 \t Batch: 20 \t Loss: 0.6931335926 \t\n",
      "Epoch: 27 \t Batch: 30 \t Loss: 0.6934992075 \t\n",
      "Epoch: 27 \t Batch: 40 \t Loss: 0.6910945177 \t\n",
      "Epoch: 27 \t Batch: 50 \t Loss: 0.6926918626 \t\n",
      "Epoch: 27 \t Batch: 60 \t Loss: 0.6937195659 \t\n",
      "Epoch: 27 \t Batch: 70 \t Loss: 0.6918261647 \t\n",
      "Epoch: 28 \t Batch: 0 \t Loss: 0.6931867003 \t\n",
      "Epoch: 28 \t Batch: 10 \t Loss: 0.6932417750 \t\n",
      "Epoch: 28 \t Batch: 20 \t Loss: 0.6930258274 \t\n",
      "Epoch: 28 \t Batch: 30 \t Loss: 0.6932969093 \t\n",
      "Epoch: 28 \t Batch: 40 \t Loss: 0.6915669441 \t\n",
      "Epoch: 28 \t Batch: 50 \t Loss: 0.6927272081 \t\n",
      "Epoch: 28 \t Batch: 60 \t Loss: 0.6934711337 \t\n",
      "Epoch: 28 \t Batch: 70 \t Loss: 0.6920939684 \t\n",
      "Epoch: 29 \t Batch: 0 \t Loss: 0.6930828691 \t\n",
      "Epoch: 29 \t Batch: 10 \t Loss: 0.6931237578 \t\n",
      "Epoch: 29 \t Batch: 20 \t Loss: 0.6929731369 \t\n",
      "Epoch: 29 \t Batch: 30 \t Loss: 0.6931768656 \t\n",
      "Epoch: 29 \t Batch: 40 \t Loss: 0.6919515729 \t\n",
      "Epoch: 29 \t Batch: 50 \t Loss: 0.6927828789 \t\n",
      "Epoch: 29 \t Batch: 60 \t Loss: 0.6933127642 \t\n",
      "Epoch: 29 \t Batch: 70 \t Loss: 0.6923222542 \t\n",
      "Epoch: 30 \t Batch: 0 \t Loss: 0.6930289865 \t\n",
      "Epoch: 30 \t Batch: 10 \t Loss: 0.6930640340 \t\n",
      "Epoch: 30 \t Batch: 20 \t Loss: 0.6929632425 \t\n",
      "Epoch: 30 \t Batch: 30 \t Loss: 0.6931120753 \t\n",
      "Epoch: 30 \t Batch: 40 \t Loss: 0.6922582984 \t\n",
      "Epoch: 30 \t Batch: 50 \t Loss: 0.6928468347 \t\n",
      "Epoch: 30 \t Batch: 60 \t Loss: 0.6932145953 \t\n",
      "Epoch: 30 \t Batch: 70 \t Loss: 0.6925150752 \t\n",
      "Epoch: 31 \t Batch: 0 \t Loss: 0.6930128336 \t\n",
      "Epoch: 31 \t Batch: 10 \t Loss: 0.6930462122 \t\n",
      "Epoch: 31 \t Batch: 20 \t Loss: 0.6929792166 \t\n",
      "Epoch: 31 \t Batch: 30 \t Loss: 0.6930830479 \t\n",
      "Epoch: 31 \t Batch: 40 \t Loss: 0.6924980879 \t\n",
      "Epoch: 31 \t Batch: 50 \t Loss: 0.6929041743 \t\n",
      "Epoch: 31 \t Batch: 60 \t Loss: 0.6931539774 \t\n",
      "Epoch: 31 \t Batch: 70 \t Loss: 0.6926711798 \t\n",
      "Epoch: 32 \t Batch: 0 \t Loss: 0.6930137277 \t\n",
      "Epoch: 32 \t Batch: 10 \t Loss: 0.6930387616 \t\n",
      "Epoch: 32 \t Batch: 20 \t Loss: 0.6929949522 \t\n",
      "Epoch: 32 \t Batch: 30 \t Loss: 0.6930648088 \t\n",
      "Epoch: 32 \t Batch: 40 \t Loss: 0.6926809549 \t\n",
      "Epoch: 32 \t Batch: 50 \t Loss: 0.6929429770 \t\n",
      "Epoch: 32 \t Batch: 60 \t Loss: 0.6931050420 \t\n",
      "Epoch: 32 \t Batch: 70 \t Loss: 0.6927817464 \t\n",
      "Epoch: 33 \t Batch: 0 \t Loss: 0.6930066943 \t\n",
      "Epoch: 33 \t Batch: 10 \t Loss: 0.6930118799 \t\n",
      "Epoch: 33 \t Batch: 20 \t Loss: 0.6929892302 \t\n",
      "Epoch: 33 \t Batch: 30 \t Loss: 0.6930254698 \t\n",
      "Epoch: 33 \t Batch: 40 \t Loss: 0.6928213835 \t\n",
      "Epoch: 33 \t Batch: 50 \t Loss: 0.6929536462 \t\n",
      "Epoch: 33 \t Batch: 60 \t Loss: 0.6930333376 \t\n",
      "Epoch: 33 \t Batch: 70 \t Loss: 0.6928486228 \t\n",
      "Epoch: 34 \t Batch: 0 \t Loss: 0.6929684877 \t\n",
      "Epoch: 34 \t Batch: 10 \t Loss: 0.6929633021 \t\n",
      "Epoch: 34 \t Batch: 20 \t Loss: 0.6929505467 \t\n",
      "Epoch: 34 \t Batch: 30 \t Loss: 0.6929685473 \t\n",
      "Epoch: 34 \t Batch: 40 \t Loss: 0.6929506063 \t\n",
      "Epoch: 34 \t Batch: 50 \t Loss: 0.6929572821 \t\n",
      "Epoch: 34 \t Batch: 60 \t Loss: 0.6929546595 \t\n",
      "Epoch: 34 \t Batch: 70 \t Loss: 0.6929188967 \t\n",
      "Epoch: 35 \t Batch: 0 \t Loss: 0.6929248571 \t\n",
      "Epoch: 35 \t Batch: 10 \t Loss: 0.6929069757 \t\n",
      "Epoch: 35 \t Batch: 20 \t Loss: 0.6929136515 \t\n",
      "Epoch: 35 \t Batch: 30 \t Loss: 0.6929059029 \t\n",
      "Epoch: 35 \t Batch: 40 \t Loss: 0.6930983663 \t\n",
      "Epoch: 35 \t Batch: 50 \t Loss: 0.6929622293 \t\n",
      "Epoch: 35 \t Batch: 60 \t Loss: 0.6928647757 \t\n",
      "Epoch: 35 \t Batch: 70 \t Loss: 0.6930004358 \t\n",
      "Epoch: 36 \t Batch: 0 \t Loss: 0.6928768158 \t\n",
      "Epoch: 36 \t Batch: 10 \t Loss: 0.6928418875 \t\n",
      "Epoch: 36 \t Batch: 20 \t Loss: 0.6928712130 \t\n",
      "Epoch: 36 \t Batch: 30 \t Loss: 0.6928317547 \t\n",
      "Epoch: 36 \t Batch: 40 \t Loss: 0.6932660937 \t\n",
      "Epoch: 36 \t Batch: 50 \t Loss: 0.6929680109 \t\n",
      "Epoch: 36 \t Batch: 60 \t Loss: 0.6927593946 \t\n",
      "Epoch: 36 \t Batch: 70 \t Loss: 0.6930897832 \t\n",
      "Epoch: 37 \t Batch: 0 \t Loss: 0.6928190589 \t\n",
      "Epoch: 37 \t Batch: 10 \t Loss: 0.6927629709 \t\n",
      "Epoch: 37 \t Batch: 20 \t Loss: 0.6928192377 \t\n",
      "Epoch: 37 \t Batch: 30 \t Loss: 0.6927442551 \t\n",
      "Epoch: 37 \t Batch: 40 \t Loss: 0.6934596300 \t\n",
      "Epoch: 37 \t Batch: 50 \t Loss: 0.6929711103 \t\n",
      "Epoch: 37 \t Batch: 60 \t Loss: 0.6926354170 \t\n",
      "Epoch: 37 \t Batch: 70 \t Loss: 0.6931903958 \t\n",
      "Epoch: 38 \t Batch: 0 \t Loss: 0.6927493215 \t\n",
      "Epoch: 38 \t Batch: 10 \t Loss: 0.6926693320 \t\n",
      "Epoch: 38 \t Batch: 20 \t Loss: 0.6927560568 \t\n",
      "Epoch: 38 \t Batch: 30 \t Loss: 0.6926405430 \t\n",
      "Epoch: 38 \t Batch: 40 \t Loss: 0.6936862469 \t\n",
      "Epoch: 38 \t Batch: 50 \t Loss: 0.6929735541 \t\n",
      "Epoch: 38 \t Batch: 60 \t Loss: 0.6924893856 \t\n",
      "Epoch: 38 \t Batch: 70 \t Loss: 0.6933067441 \t\n",
      "Epoch: 39 \t Batch: 0 \t Loss: 0.6926666498 \t\n",
      "Epoch: 39 \t Batch: 10 \t Loss: 0.6925588846 \t\n",
      "Epoch: 39 \t Batch: 20 \t Loss: 0.6926815510 \t\n",
      "Epoch: 39 \t Batch: 30 \t Loss: 0.6925191283 \t\n",
      "Epoch: 39 \t Batch: 40 \t Loss: 0.6939526200 \t\n",
      "Epoch: 39 \t Batch: 50 \t Loss: 0.6929775476 \t\n",
      "Epoch: 39 \t Batch: 60 \t Loss: 0.6923207045 \t\n",
      "Epoch: 39 \t Batch: 70 \t Loss: 0.6934430599 \t\n",
      "Epoch: 40 \t Batch: 0 \t Loss: 0.6925716400 \t\n",
      "Epoch: 40 \t Batch: 10 \t Loss: 0.6924321055 \t\n",
      "Epoch: 40 \t Batch: 20 \t Loss: 0.6925968528 \t\n",
      "Epoch: 40 \t Batch: 30 \t Loss: 0.6923809052 \t\n",
      "Epoch: 40 \t Batch: 40 \t Loss: 0.6942642927 \t\n",
      "Epoch: 40 \t Batch: 50 \t Loss: 0.6929855347 \t\n",
      "Epoch: 40 \t Batch: 60 \t Loss: 0.6921304464 \t\n",
      "Epoch: 40 \t Batch: 70 \t Loss: 0.6936036348 \t\n",
      "Epoch: 41 \t Batch: 0 \t Loss: 0.6924665570 \t\n",
      "Epoch: 41 \t Batch: 10 \t Loss: 0.6922911406 \t\n",
      "Epoch: 41 \t Batch: 20 \t Loss: 0.6925047636 \t\n",
      "Epoch: 41 \t Batch: 30 \t Loss: 0.6922283769 \t\n",
      "Epoch: 41 \t Batch: 40 \t Loss: 0.6946251988 \t\n",
      "Epoch: 41 \t Batch: 50 \t Loss: 0.6930010319 \t\n",
      "Epoch: 41 \t Batch: 60 \t Loss: 0.6919224858 \t\n",
      "Epoch: 41 \t Batch: 70 \t Loss: 0.6937913895 \t\n",
      "Epoch: 42 \t Batch: 0 \t Loss: 0.6923550963 \t\n",
      "Epoch: 42 \t Batch: 10 \t Loss: 0.6921402216 \t\n",
      "Epoch: 42 \t Batch: 20 \t Loss: 0.6924091578 \t\n",
      "Epoch: 42 \t Batch: 30 \t Loss: 0.6920660734 \t\n",
      "Epoch: 42 \t Batch: 40 \t Loss: 0.6950359344 \t\n",
      "Epoch: 42 \t Batch: 50 \t Loss: 0.6930271387 \t\n",
      "Epoch: 42 \t Batch: 60 \t Loss: 0.6917023659 \t\n",
      "Epoch: 42 \t Batch: 70 \t Loss: 0.6940079927 \t\n",
      "Epoch: 43 \t Batch: 0 \t Loss: 0.6922420263 \t\n",
      "Epoch: 43 \t Batch: 10 \t Loss: 0.6919847727 \t\n",
      "Epoch: 43 \t Batch: 20 \t Loss: 0.6923146844 \t\n",
      "Epoch: 43 \t Batch: 30 \t Loss: 0.6918998957 \t\n",
      "Epoch: 43 \t Batch: 40 \t Loss: 0.6954928041 \t\n",
      "Epoch: 43 \t Batch: 50 \t Loss: 0.6930667758 \t\n",
      "Epoch: 43 \t Batch: 60 \t Loss: 0.6914778948 \t\n",
      "Epoch: 43 \t Batch: 70 \t Loss: 0.6942521334 \t\n",
      "Epoch: 44 \t Batch: 0 \t Loss: 0.6921332479 \t\n",
      "Epoch: 44 \t Batch: 10 \t Loss: 0.6918309927 \t\n",
      "Epoch: 44 \t Batch: 20 \t Loss: 0.6922262311 \t\n",
      "Epoch: 44 \t Batch: 30 \t Loss: 0.6917363405 \t\n",
      "Epoch: 44 \t Batch: 40 \t Loss: 0.6959869862 \t\n",
      "Epoch: 44 \t Batch: 50 \t Loss: 0.6931209564 \t\n",
      "Epoch: 44 \t Batch: 60 \t Loss: 0.6912574172 \t\n",
      "Epoch: 44 \t Batch: 70 \t Loss: 0.6945189834 \t\n",
      "Epoch: 45 \t Batch: 0 \t Loss: 0.6920333505 \t\n",
      "Epoch: 45 \t Batch: 10 \t Loss: 0.6916855574 \t\n",
      "Epoch: 45 \t Batch: 20 \t Loss: 0.6921480894 \t\n",
      "Epoch: 45 \t Batch: 30 \t Loss: 0.6915815473 \t\n",
      "Epoch: 45 \t Batch: 40 \t Loss: 0.6965039968 \t\n",
      "Epoch: 45 \t Batch: 50 \t Loss: 0.6931888461 \t\n",
      "Epoch: 45 \t Batch: 60 \t Loss: 0.6910492182 \t\n",
      "Epoch: 45 \t Batch: 70 \t Loss: 0.6947998405 \t\n",
      "Epoch: 46 \t Batch: 0 \t Loss: 0.6919463873 \t\n",
      "Epoch: 46 \t Batch: 10 \t Loss: 0.6915527582 \t\n",
      "Epoch: 46 \t Batch: 20 \t Loss: 0.6920818090 \t\n",
      "Epoch: 46 \t Batch: 30 \t Loss: 0.6914404631 \t\n",
      "Epoch: 46 \t Batch: 40 \t Loss: 0.6970233321 \t\n",
      "Epoch: 46 \t Batch: 50 \t Loss: 0.6932660937 \t\n",
      "Epoch: 46 \t Batch: 60 \t Loss: 0.6908599734 \t\n",
      "Epoch: 46 \t Batch: 70 \t Loss: 0.6950813532 \t\n",
      "Epoch: 47 \t Batch: 0 \t Loss: 0.6918737888 \t\n",
      "Epoch: 47 \t Batch: 10 \t Loss: 0.6914359331 \t\n",
      "Epoch: 47 \t Batch: 20 \t Loss: 0.6920276284 \t\n",
      "Epoch: 47 \t Batch: 30 \t Loss: 0.6913158298 \t\n",
      "Epoch: 47 \t Batch: 40 \t Loss: 0.6975191832 \t\n",
      "Epoch: 47 \t Batch: 50 \t Loss: 0.6933457255 \t\n",
      "Epoch: 47 \t Batch: 60 \t Loss: 0.6906945109 \t\n",
      "Epoch: 47 \t Batch: 70 \t Loss: 0.6953456998 \t\n",
      "Epoch: 48 \t Batch: 0 \t Loss: 0.6918147206 \t\n",
      "Epoch: 48 \t Batch: 10 \t Loss: 0.6913352609 \t\n",
      "Epoch: 48 \t Batch: 20 \t Loss: 0.6919826269 \t\n",
      "Epoch: 48 \t Batch: 30 \t Loss: 0.6912081242 \t\n",
      "Epoch: 48 \t Batch: 40 \t Loss: 0.6979611516 \t\n",
      "Epoch: 48 \t Batch: 50 \t Loss: 0.6934175491 \t\n",
      "Epoch: 48 \t Batch: 60 \t Loss: 0.6905560493 \t\n",
      "Epoch: 48 \t Batch: 70 \t Loss: 0.6955721378 \t\n",
      "Epoch: 49 \t Batch: 0 \t Loss: 0.6917656660 \t\n",
      "Epoch: 49 \t Batch: 10 \t Loss: 0.6912493706 \t\n",
      "Epoch: 49 \t Batch: 20 \t Loss: 0.6919417381 \t\n",
      "Epoch: 49 \t Batch: 30 \t Loss: 0.6911164522 \t\n",
      "Epoch: 49 \t Batch: 40 \t Loss: 0.6983169317 \t\n",
      "Epoch: 49 \t Batch: 50 \t Loss: 0.6934698224 \t\n",
      "Epoch: 49 \t Batch: 60 \t Loss: 0.6904454827 \t\n",
      "Epoch: 49 \t Batch: 70 \t Loss: 0.6957390308 \t\n",
      "Epoch: 50 \t Batch: 0 \t Loss: 0.6917223334 \t\n",
      "Epoch: 50 \t Batch: 10 \t Loss: 0.6911751032 \t\n",
      "Epoch: 50 \t Batch: 20 \t Loss: 0.6918992400 \t\n",
      "Epoch: 50 \t Batch: 30 \t Loss: 0.6910380125 \t\n",
      "Epoch: 50 \t Batch: 40 \t Loss: 0.6985548735 \t\n",
      "Epoch: 50 \t Batch: 50 \t Loss: 0.6934909821 \t\n",
      "Epoch: 50 \t Batch: 60 \t Loss: 0.6903628707 \t\n",
      "Epoch: 50 \t Batch: 70 \t Loss: 0.6958261728 \t\n",
      "Epoch: 51 \t Batch: 0 \t Loss: 0.6916797161 \t\n",
      "Epoch: 51 \t Batch: 10 \t Loss: 0.6911098361 \t\n",
      "Epoch: 51 \t Batch: 20 \t Loss: 0.6918498874 \t\n",
      "Epoch: 51 \t Batch: 30 \t Loss: 0.6909712553 \t\n",
      "Epoch: 51 \t Batch: 40 \t Loss: 0.6986499429 \t\n",
      "Epoch: 51 \t Batch: 50 \t Loss: 0.6934720278 \t\n",
      "Epoch: 51 \t Batch: 60 \t Loss: 0.6903086305 \t\n",
      "Epoch: 51 \t Batch: 70 \t Loss: 0.6958190203 \t\n",
      "Epoch: 52 \t Batch: 0 \t Loss: 0.6916345358 \t\n",
      "Epoch: 52 \t Batch: 10 \t Loss: 0.6910520196 \t\n",
      "Epoch: 52 \t Batch: 20 \t Loss: 0.6917914152 \t\n",
      "Epoch: 52 \t Batch: 30 \t Loss: 0.6909158230 \t\n",
      "Epoch: 52 \t Batch: 40 \t Loss: 0.6985875964 \t\n",
      "Epoch: 52 \t Batch: 50 \t Loss: 0.6934092045 \t\n",
      "Epoch: 52 \t Batch: 60 \t Loss: 0.6902834177 \t\n",
      "Epoch: 52 \t Batch: 70 \t Loss: 0.6957124472 \t\n",
      "Epoch: 53 \t Batch: 0 \t Loss: 0.6915866733 \t\n",
      "Epoch: 53 \t Batch: 10 \t Loss: 0.6910027266 \t\n",
      "Epoch: 53 \t Batch: 20 \t Loss: 0.6917248964 \t\n",
      "Epoch: 53 \t Batch: 30 \t Loss: 0.6908739805 \t\n",
      "Epoch: 53 \t Batch: 40 \t Loss: 0.6983689666 \t\n",
      "Epoch: 53 \t Batch: 50 \t Loss: 0.6933055520 \t\n",
      "Epoch: 53 \t Batch: 60 \t Loss: 0.6902891397 \t\n",
      "Epoch: 53 \t Batch: 70 \t Loss: 0.6955124736 \t\n",
      "Epoch: 54 \t Batch: 0 \t Loss: 0.6915394664 \t\n",
      "Epoch: 54 \t Batch: 10 \t Loss: 0.6909657121 \t\n",
      "Epoch: 54 \t Batch: 20 \t Loss: 0.6916559339 \t\n",
      "Epoch: 54 \t Batch: 30 \t Loss: 0.6908496618 \t\n",
      "Epoch: 54 \t Batch: 40 \t Loss: 0.6980119944 \t\n",
      "Epoch: 54 \t Batch: 50 \t Loss: 0.6931704283 \t\n",
      "Epoch: 54 \t Batch: 60 \t Loss: 0.6903274655 \t\n",
      "Epoch: 54 \t Batch: 70 \t Loss: 0.6952368021 \t\n",
      "Epoch: 55 \t Batch: 0 \t Loss: 0.6914987564 \t\n",
      "Epoch: 55 \t Batch: 10 \t Loss: 0.6909459829 \t\n",
      "Epoch: 55 \t Batch: 20 \t Loss: 0.6915920377 \t\n",
      "Epoch: 55 \t Batch: 30 \t Loss: 0.6908473372 \t\n",
      "Epoch: 55 \t Batch: 40 \t Loss: 0.6975498199 \t\n",
      "Epoch: 55 \t Batch: 50 \t Loss: 0.6930180788 \t\n",
      "Epoch: 55 \t Batch: 60 \t Loss: 0.6903983355 \t\n",
      "Epoch: 55 \t Batch: 70 \t Loss: 0.6949104667 \t\n",
      "Epoch: 56 \t Batch: 0 \t Loss: 0.6914705038 \t\n",
      "Epoch: 56 \t Batch: 10 \t Loss: 0.6909471750 \t\n",
      "Epoch: 56 \t Batch: 20 \t Loss: 0.6915405989 \t\n",
      "Epoch: 56 \t Batch: 30 \t Loss: 0.6908694506 \t\n",
      "Epoch: 56 \t Batch: 40 \t Loss: 0.6970232129 \t\n",
      "Epoch: 56 \t Batch: 50 \t Loss: 0.6928628683 \t\n",
      "Epoch: 56 \t Batch: 60 \t Loss: 0.6904980540 \t\n",
      "Epoch: 56 \t Batch: 70 \t Loss: 0.6945608854 \t\n",
      "Epoch: 57 \t Batch: 0 \t Loss: 0.6914582849 \t\n",
      "Epoch: 57 \t Batch: 10 \t Loss: 0.6909694672 \t\n",
      "Epoch: 57 \t Batch: 20 \t Loss: 0.6915053129 \t\n",
      "Epoch: 57 \t Batch: 30 \t Loss: 0.6909135580 \t\n",
      "Epoch: 57 \t Batch: 40 \t Loss: 0.6964738965 \t\n",
      "Epoch: 57 \t Batch: 50 \t Loss: 0.6927158237 \t\n",
      "Epoch: 57 \t Batch: 60 \t Loss: 0.6906186938 \t\n",
      "Epoch: 57 \t Batch: 70 \t Loss: 0.6942118406 \t\n",
      "Epoch: 58 \t Batch: 0 \t Loss: 0.6914613247 \t\n",
      "Epoch: 58 \t Batch: 10 \t Loss: 0.6910084486 \t\n",
      "Epoch: 58 \t Batch: 20 \t Loss: 0.6914851069 \t\n",
      "Epoch: 58 \t Batch: 30 \t Loss: 0.6909731627 \t\n",
      "Epoch: 58 \t Batch: 40 \t Loss: 0.6959370375 \t\n",
      "Epoch: 58 \t Batch: 50 \t Loss: 0.6925827265 \t\n",
      "Epoch: 58 \t Batch: 60 \t Loss: 0.6907485723 \t\n",
      "Epoch: 58 \t Batch: 70 \t Loss: 0.6938800216 \t\n",
      "Epoch: 59 \t Batch: 0 \t Loss: 0.6914740801 \t\n",
      "Epoch: 59 \t Batch: 10 \t Loss: 0.6910545230 \t\n",
      "Epoch: 59 \t Batch: 20 \t Loss: 0.6914737821 \t\n",
      "Epoch: 59 \t Batch: 30 \t Loss: 0.6910370588 \t\n",
      "Epoch: 59 \t Batch: 40 \t Loss: 0.6954380274 \t\n",
      "Epoch: 59 \t Batch: 50 \t Loss: 0.6924630404 \t\n",
      "Epoch: 59 \t Batch: 60 \t Loss: 0.6908733845 \t\n",
      "Epoch: 59 \t Batch: 70 \t Loss: 0.6935737133 \t\n",
      "Epoch: 60 \t Batch: 0 \t Loss: 0.6914871335 \t\n",
      "Epoch: 60 \t Batch: 10 \t Loss: 0.6910954714 \t\n",
      "Epoch: 60 \t Batch: 20 \t Loss: 0.6914607882 \t\n",
      "Epoch: 60 \t Batch: 30 \t Loss: 0.6910915971 \t\n",
      "Epoch: 60 \t Batch: 40 \t Loss: 0.6949912906 \t\n",
      "Epoch: 60 \t Batch: 50 \t Loss: 0.6923518181 \t\n",
      "Epoch: 60 \t Batch: 60 \t Loss: 0.6909780502 \t\n",
      "Epoch: 60 \t Batch: 70 \t Loss: 0.6932936311 \t\n",
      "Epoch: 61 \t Batch: 0 \t Loss: 0.6914891005 \t\n",
      "Epoch: 61 \t Batch: 10 \t Loss: 0.6911164522 \t\n",
      "Epoch: 61 \t Batch: 20 \t Loss: 0.6914332509 \t\n",
      "Epoch: 61 \t Batch: 30 \t Loss: 0.6911219954 \t\n",
      "Epoch: 61 \t Batch: 40 \t Loss: 0.6946019530 \t\n",
      "Epoch: 61 \t Batch: 50 \t Loss: 0.6922400594 \t\n",
      "Epoch: 61 \t Batch: 60 \t Loss: 0.6910485625 \t\n",
      "Epoch: 61 \t Batch: 70 \t Loss: 0.6930336356 \t\n",
      "Epoch: 62 \t Batch: 0 \t Loss: 0.6914674044 \t\n",
      "Epoch: 62 \t Batch: 10 \t Loss: 0.6911029816 \t\n",
      "Epoch: 62 \t Batch: 20 \t Loss: 0.6913777590 \t\n",
      "Epoch: 62 \t Batch: 30 \t Loss: 0.6911143064 \t\n",
      "Epoch: 62 \t Batch: 40 \t Loss: 0.6942654848 \t\n",
      "Epoch: 62 \t Batch: 50 \t Loss: 0.6921172738 \t\n",
      "Epoch: 62 \t Batch: 60 \t Loss: 0.6910743117 \t\n",
      "Epoch: 62 \t Batch: 70 \t Loss: 0.6927832365 \t\n",
      "Epoch: 63 \t Batch: 0 \t Loss: 0.6914113760 \t\n",
      "Epoch: 63 \t Batch: 10 \t Loss: 0.6910454035 \t\n",
      "Epoch: 63 \t Batch: 20 \t Loss: 0.6912823915 \t\n",
      "Epoch: 63 \t Batch: 30 \t Loss: 0.6910600662 \t\n",
      "Epoch: 63 \t Batch: 40 \t Loss: 0.6939751506 \t\n",
      "Epoch: 63 \t Batch: 50 \t Loss: 0.6919690371 \t\n",
      "Epoch: 63 \t Batch: 60 \t Loss: 0.6910508275 \t\n",
      "Epoch: 63 \t Batch: 70 \t Loss: 0.6925278902 \t\n",
      "Epoch: 64 \t Batch: 0 \t Loss: 0.6913139224 \t\n",
      "Epoch: 64 \t Batch: 10 \t Loss: 0.6909393072 \t\n",
      "Epoch: 64 \t Batch: 20 \t Loss: 0.6911400557 \t\n",
      "Epoch: 64 \t Batch: 30 \t Loss: 0.6909598112 \t\n",
      "Epoch: 64 \t Batch: 40 \t Loss: 0.6937063932 \t\n",
      "Epoch: 64 \t Batch: 50 \t Loss: 0.6917885542 \t\n",
      "Epoch: 64 \t Batch: 60 \t Loss: 0.6909865737 \t\n",
      "Epoch: 64 \t Batch: 70 \t Loss: 0.6922527552 \t\n",
      "Epoch: 65 \t Batch: 0 \t Loss: 0.6911811233 \t\n",
      "Epoch: 65 \t Batch: 10 \t Loss: 0.6907852888 \t\n",
      "Epoch: 65 \t Batch: 20 \t Loss: 0.6909543872 \t\n",
      "Epoch: 65 \t Batch: 30 \t Loss: 0.6908192039 \t\n",
      "Epoch: 65 \t Batch: 40 \t Loss: 0.6934337616 \t\n",
      "Epoch: 65 \t Batch: 50 \t Loss: 0.6915801764 \t\n",
      "Epoch: 65 \t Batch: 60 \t Loss: 0.6908921003 \t\n",
      "Epoch: 65 \t Batch: 70 \t Loss: 0.6919443607 \t\n",
      "Epoch: 66 \t Batch: 0 \t Loss: 0.6910168529 \t\n",
      "Epoch: 66 \t Batch: 10 \t Loss: 0.6905920506 \t\n",
      "Epoch: 66 \t Batch: 20 \t Loss: 0.6907343864 \t\n",
      "Epoch: 66 \t Batch: 30 \t Loss: 0.6906458735 \t\n",
      "Epoch: 66 \t Batch: 40 \t Loss: 0.6931391954 \t\n",
      "Epoch: 66 \t Batch: 50 \t Loss: 0.6913510561 \t\n",
      "Epoch: 66 \t Batch: 60 \t Loss: 0.6907891035 \t\n",
      "Epoch: 66 \t Batch: 70 \t Loss: 0.6916049123 \t\n",
      "Epoch: 67 \t Batch: 0 \t Loss: 0.6908441782 \t\n",
      "Epoch: 67 \t Batch: 10 \t Loss: 0.6903910637 \t\n",
      "Epoch: 67 \t Batch: 20 \t Loss: 0.6904886365 \t\n",
      "Epoch: 67 \t Batch: 30 \t Loss: 0.6904658079 \t\n",
      "Epoch: 67 \t Batch: 40 \t Loss: 0.6928259730 \t\n",
      "Epoch: 67 \t Batch: 50 \t Loss: 0.6911125779 \t\n",
      "Epoch: 67 \t Batch: 60 \t Loss: 0.6906800866 \t\n",
      "Epoch: 67 \t Batch: 70 \t Loss: 0.6912301779 \t\n",
      "Epoch: 68 \t Batch: 0 \t Loss: 0.6906692982 \t\n",
      "Epoch: 68 \t Batch: 10 \t Loss: 0.6902212501 \t\n",
      "Epoch: 68 \t Batch: 20 \t Loss: 0.6902566552 \t\n",
      "Epoch: 68 \t Batch: 30 \t Loss: 0.6903027296 \t\n",
      "Epoch: 68 \t Batch: 40 \t Loss: 0.6925047040 \t\n",
      "Epoch: 68 \t Batch: 50 \t Loss: 0.6908827424 \t\n",
      "Epoch: 68 \t Batch: 60 \t Loss: 0.6905994415 \t\n",
      "Epoch: 68 \t Batch: 70 \t Loss: 0.6908704638 \t\n",
      "Epoch: 69 \t Batch: 0 \t Loss: 0.6905356646 \t\n",
      "Epoch: 69 \t Batch: 10 \t Loss: 0.6900718212 \t\n",
      "Epoch: 69 \t Batch: 20 \t Loss: 0.6901177168 \t\n",
      "Epoch: 69 \t Batch: 30 \t Loss: 0.6901670694 \t\n",
      "Epoch: 69 \t Batch: 40 \t Loss: 0.6922166944 \t\n",
      "Epoch: 69 \t Batch: 50 \t Loss: 0.6906841993 \t\n",
      "Epoch: 69 \t Batch: 60 \t Loss: 0.6905328631 \t\n",
      "Epoch: 69 \t Batch: 70 \t Loss: 0.6905673146 \t\n",
      "Epoch: 70 \t Batch: 0 \t Loss: 0.6904383302 \t\n",
      "Epoch: 70 \t Batch: 10 \t Loss: 0.6899131536 \t\n",
      "Epoch: 70 \t Batch: 20 \t Loss: 0.6900647879 \t\n",
      "Epoch: 70 \t Batch: 30 \t Loss: 0.6900314093 \t\n",
      "Epoch: 70 \t Batch: 40 \t Loss: 0.6920087934 \t\n",
      "Epoch: 70 \t Batch: 50 \t Loss: 0.6904974580 \t\n",
      "Epoch: 70 \t Batch: 60 \t Loss: 0.6904798746 \t\n",
      "Epoch: 70 \t Batch: 70 \t Loss: 0.6903613806 \t\n",
      "Epoch: 71 \t Batch: 0 \t Loss: 0.6903758049 \t\n",
      "Epoch: 71 \t Batch: 10 \t Loss: 0.6897306442 \t\n",
      "Epoch: 71 \t Batch: 20 \t Loss: 0.6900144815 \t\n",
      "Epoch: 71 \t Batch: 30 \t Loss: 0.6898921728 \t\n",
      "Epoch: 71 \t Batch: 40 \t Loss: 0.6918705106 \t\n",
      "Epoch: 71 \t Batch: 50 \t Loss: 0.6903264523 \t\n",
      "Epoch: 71 \t Batch: 60 \t Loss: 0.6903725863 \t\n",
      "Epoch: 71 \t Batch: 70 \t Loss: 0.6901875138 \t\n",
      "Epoch: 72 \t Batch: 0 \t Loss: 0.6902414560 \t\n",
      "Epoch: 72 \t Batch: 10 \t Loss: 0.6894835234 \t\n",
      "Epoch: 72 \t Batch: 20 \t Loss: 0.6898372769 \t\n",
      "Epoch: 72 \t Batch: 30 \t Loss: 0.6896515489 \t\n",
      "Epoch: 72 \t Batch: 40 \t Loss: 0.6918219328 \t\n",
      "Epoch: 72 \t Batch: 50 \t Loss: 0.6901053190 \t\n",
      "Epoch: 72 \t Batch: 60 \t Loss: 0.6900858283 \t\n",
      "Epoch: 72 \t Batch: 70 \t Loss: 0.6899629831 \t\n",
      "Epoch: 73 \t Batch: 0 \t Loss: 0.6899381876 \t\n",
      "Epoch: 73 \t Batch: 10 \t Loss: 0.6891127825 \t\n",
      "Epoch: 73 \t Batch: 20 \t Loss: 0.6894382238 \t\n",
      "Epoch: 73 \t Batch: 30 \t Loss: 0.6892417669 \t\n",
      "Epoch: 73 \t Batch: 40 \t Loss: 0.6918866038 \t\n",
      "Epoch: 73 \t Batch: 50 \t Loss: 0.6898235083 \t\n",
      "Epoch: 73 \t Batch: 60 \t Loss: 0.6895906925 \t\n",
      "Epoch: 73 \t Batch: 70 \t Loss: 0.6897024512 \t\n",
      "Epoch: 74 \t Batch: 0 \t Loss: 0.6894400120 \t\n",
      "Epoch: 74 \t Batch: 10 \t Loss: 0.6886316538 \t\n",
      "Epoch: 74 \t Batch: 20 \t Loss: 0.6888467669 \t\n",
      "Epoch: 74 \t Batch: 30 \t Loss: 0.6887173653 \t\n",
      "Epoch: 74 \t Batch: 40 \t Loss: 0.6920934319 \t\n",
      "Epoch: 74 \t Batch: 50 \t Loss: 0.6895499229 \t\n",
      "Epoch: 74 \t Batch: 60 \t Loss: 0.6889948845 \t\n",
      "Epoch: 74 \t Batch: 70 \t Loss: 0.6895775795 \t\n",
      "Epoch: 75 \t Batch: 0 \t Loss: 0.6889077425 \t\n",
      "Epoch: 75 \t Batch: 10 \t Loss: 0.6880689263 \t\n",
      "Epoch: 75 \t Batch: 20 \t Loss: 0.6881992221 \t\n",
      "Epoch: 75 \t Batch: 30 \t Loss: 0.6881483793 \t\n",
      "Epoch: 75 \t Batch: 40 \t Loss: 0.6925017834 \t\n",
      "Epoch: 75 \t Batch: 50 \t Loss: 0.6893022060 \t\n",
      "Epoch: 75 \t Batch: 60 \t Loss: 0.6883829832 \t\n",
      "Epoch: 75 \t Batch: 70 \t Loss: 0.6896217465 \t\n",
      "Epoch: 76 \t Batch: 0 \t Loss: 0.6884008646 \t\n",
      "Epoch: 76 \t Batch: 10 \t Loss: 0.6874975562 \t\n",
      "Epoch: 76 \t Batch: 20 \t Loss: 0.6876401901 \t\n",
      "Epoch: 76 \t Batch: 30 \t Loss: 0.6875671148 \t\n",
      "Epoch: 76 \t Batch: 40 \t Loss: 0.6931182146 \t\n",
      "Epoch: 76 \t Batch: 50 \t Loss: 0.6891411543 \t\n",
      "Epoch: 76 \t Batch: 60 \t Loss: 0.6877942085 \t\n",
      "Epoch: 76 \t Batch: 70 \t Loss: 0.6898181438 \t\n",
      "Epoch: 77 \t Batch: 0 \t Loss: 0.6879669428 \t\n",
      "Epoch: 77 \t Batch: 10 \t Loss: 0.6869415045 \t\n",
      "Epoch: 77 \t Batch: 20 \t Loss: 0.6872314215 \t\n",
      "Epoch: 77 \t Batch: 30 \t Loss: 0.6870664358 \t\n",
      "Epoch: 77 \t Batch: 40 \t Loss: 0.6937988997 \t\n",
      "Epoch: 77 \t Batch: 50 \t Loss: 0.6890512705 \t\n",
      "Epoch: 77 \t Batch: 60 \t Loss: 0.6872861981 \t\n",
      "Epoch: 77 \t Batch: 70 \t Loss: 0.6900767088 \t\n",
      "Epoch: 78 \t Batch: 0 \t Loss: 0.6876217127 \t\n",
      "Epoch: 78 \t Batch: 10 \t Loss: 0.6864749789 \t\n",
      "Epoch: 78 \t Batch: 20 \t Loss: 0.6869068742 \t\n",
      "Epoch: 78 \t Batch: 30 \t Loss: 0.6866255403 \t\n",
      "Epoch: 78 \t Batch: 40 \t Loss: 0.6944505572 \t\n",
      "Epoch: 78 \t Batch: 50 \t Loss: 0.6889854670 \t\n",
      "Epoch: 78 \t Batch: 60 \t Loss: 0.6868566871 \t\n",
      "Epoch: 78 \t Batch: 70 \t Loss: 0.6902925968 \t\n",
      "Epoch: 79 \t Batch: 0 \t Loss: 0.6873193979 \t\n",
      "Epoch: 79 \t Batch: 10 \t Loss: 0.6860852242 \t\n",
      "Epoch: 79 \t Batch: 20 \t Loss: 0.6866254210 \t\n",
      "Epoch: 79 \t Batch: 30 \t Loss: 0.6862515807 \t\n",
      "Epoch: 79 \t Batch: 40 \t Loss: 0.6949785948 \t\n",
      "Epoch: 79 \t Batch: 50 \t Loss: 0.6888982058 \t\n",
      "Epoch: 79 \t Batch: 60 \t Loss: 0.6865018010 \t\n",
      "Epoch: 79 \t Batch: 70 \t Loss: 0.6903902292 \t\n",
      "Epoch: 80 \t Batch: 0 \t Loss: 0.6870525479 \t\n",
      "Epoch: 80 \t Batch: 10 \t Loss: 0.6857429147 \t\n",
      "Epoch: 80 \t Batch: 20 \t Loss: 0.6863433719 \t\n",
      "Epoch: 80 \t Batch: 30 \t Loss: 0.6859185696 \t\n",
      "Epoch: 80 \t Batch: 40 \t Loss: 0.6952485442 \t\n",
      "Epoch: 80 \t Batch: 50 \t Loss: 0.6887485385 \t\n",
      "Epoch: 80 \t Batch: 60 \t Loss: 0.6862174273 \t\n",
      "Epoch: 80 \t Batch: 70 \t Loss: 0.6902934313 \t\n",
      "Epoch: 81 \t Batch: 0 \t Loss: 0.6867979765 \t\n",
      "Epoch: 81 \t Batch: 10 \t Loss: 0.6854315400 \t\n",
      "Epoch: 81 \t Batch: 20 \t Loss: 0.6860350370 \t\n",
      "Epoch: 81 \t Batch: 30 \t Loss: 0.6856156588 \t\n",
      "Epoch: 81 \t Batch: 40 \t Loss: 0.6951630712 \t\n",
      "Epoch: 81 \t Batch: 50 \t Loss: 0.6885032654 \t\n",
      "Epoch: 81 \t Batch: 60 \t Loss: 0.6859991550 \t\n",
      "Epoch: 81 \t Batch: 70 \t Loss: 0.6899498701 \t\n",
      "Epoch: 82 \t Batch: 0 \t Loss: 0.6865457296 \t\n",
      "Epoch: 82 \t Batch: 10 \t Loss: 0.6851506829 \t\n",
      "Epoch: 82 \t Batch: 20 \t Loss: 0.6856905818 \t\n",
      "Epoch: 82 \t Batch: 30 \t Loss: 0.6853447556 \t\n",
      "Epoch: 82 \t Batch: 40 \t Loss: 0.6947127581 \t\n",
      "Epoch: 82 \t Batch: 50 \t Loss: 0.6881558895 \t\n",
      "Epoch: 82 \t Batch: 60 \t Loss: 0.6858559847 \t\n",
      "Epoch: 82 \t Batch: 70 \t Loss: 0.6893478036 \t\n",
      "Epoch: 83 \t Batch: 0 \t Loss: 0.6862996817 \t\n",
      "Epoch: 83 \t Batch: 10 \t Loss: 0.6849278212 \t\n",
      "Epoch: 83 \t Batch: 20 \t Loss: 0.6853536367 \t\n",
      "Epoch: 83 \t Batch: 30 \t Loss: 0.6851379871 \t\n",
      "Epoch: 83 \t Batch: 40 \t Loss: 0.6939964890 \t\n",
      "Epoch: 83 \t Batch: 50 \t Loss: 0.6877546310 \t\n",
      "Epoch: 83 \t Batch: 60 \t Loss: 0.6858103871 \t\n",
      "Epoch: 83 \t Batch: 70 \t Loss: 0.6886008382 \t\n",
      "Epoch: 84 \t Batch: 0 \t Loss: 0.6861085296 \t\n",
      "Epoch: 84 \t Batch: 10 \t Loss: 0.6848284006 \t\n",
      "Epoch: 84 \t Batch: 20 \t Loss: 0.6851261854 \t\n",
      "Epoch: 84 \t Batch: 30 \t Loss: 0.6850407720 \t\n",
      "Epoch: 84 \t Batch: 40 \t Loss: 0.6930795908 \t\n",
      "Epoch: 84 \t Batch: 50 \t Loss: 0.6873970628 \t\n",
      "Epoch: 84 \t Batch: 60 \t Loss: 0.6858946085 \t\n",
      "Epoch: 84 \t Batch: 70 \t Loss: 0.6878715158 \t\n",
      "Epoch: 85 \t Batch: 0 \t Loss: 0.6860609055 \t\n",
      "Epoch: 85 \t Batch: 10 \t Loss: 0.6847905517 \t\n",
      "Epoch: 85 \t Batch: 20 \t Loss: 0.6851769686 \t\n",
      "Epoch: 85 \t Batch: 30 \t Loss: 0.6850603819 \t\n",
      "Epoch: 85 \t Batch: 40 \t Loss: 0.6923141479 \t\n",
      "Epoch: 85 \t Batch: 50 \t Loss: 0.6871470213 \t\n",
      "Epoch: 85 \t Batch: 60 \t Loss: 0.6860398054 \t\n",
      "Epoch: 85 \t Batch: 70 \t Loss: 0.6873437166 \t\n",
      "Epoch: 86 \t Batch: 0 \t Loss: 0.6861208081 \t\n",
      "Epoch: 86 \t Batch: 10 \t Loss: 0.6847635508 \t\n",
      "Epoch: 86 \t Batch: 20 \t Loss: 0.6853998899 \t\n",
      "Epoch: 86 \t Batch: 30 \t Loss: 0.6850911379 \t\n",
      "Epoch: 86 \t Batch: 40 \t Loss: 0.6918064356 \t\n",
      "Epoch: 86 \t Batch: 50 \t Loss: 0.6869668365 \t\n",
      "Epoch: 86 \t Batch: 60 \t Loss: 0.6861994863 \t\n",
      "Epoch: 86 \t Batch: 70 \t Loss: 0.6870873570 \t\n",
      "Epoch: 87 \t Batch: 0 \t Loss: 0.6862495542 \t\n",
      "Epoch: 87 \t Batch: 10 \t Loss: 0.6846890450 \t\n",
      "Epoch: 87 \t Batch: 20 \t Loss: 0.6855791807 \t\n",
      "Epoch: 87 \t Batch: 30 \t Loss: 0.6851043701 \t\n",
      "Epoch: 87 \t Batch: 40 \t Loss: 0.6915508509 \t\n",
      "Epoch: 87 \t Batch: 50 \t Loss: 0.6868390441 \t\n",
      "Epoch: 87 \t Batch: 60 \t Loss: 0.6861666441 \t\n",
      "Epoch: 87 \t Batch: 70 \t Loss: 0.6869131923 \t\n",
      "Epoch: 88 \t Batch: 0 \t Loss: 0.6861636639 \t\n",
      "Epoch: 88 \t Batch: 10 \t Loss: 0.6844465733 \t\n",
      "Epoch: 88 \t Batch: 20 \t Loss: 0.6853922606 \t\n",
      "Epoch: 88 \t Batch: 30 \t Loss: 0.6848310828 \t\n",
      "Epoch: 88 \t Batch: 40 \t Loss: 0.6915886998 \t\n",
      "Epoch: 88 \t Batch: 50 \t Loss: 0.6866055727 \t\n",
      "Epoch: 88 \t Batch: 60 \t Loss: 0.6857153177 \t\n",
      "Epoch: 88 \t Batch: 70 \t Loss: 0.6866527796 \t\n",
      "Epoch: 89 \t Batch: 0 \t Loss: 0.6856755018 \t\n",
      "Epoch: 89 \t Batch: 10 \t Loss: 0.6839758158 \t\n",
      "Epoch: 89 \t Batch: 20 \t Loss: 0.6847420335 \t\n",
      "Epoch: 89 \t Batch: 30 \t Loss: 0.6842283607 \t\n",
      "Epoch: 89 \t Batch: 40 \t Loss: 0.6919624209 \t\n",
      "Epoch: 89 \t Batch: 50 \t Loss: 0.6863306761 \t\n",
      "Epoch: 89 \t Batch: 60 \t Loss: 0.6849921346 \t\n",
      "Epoch: 89 \t Batch: 70 \t Loss: 0.6865357161 \t\n",
      "Epoch: 90 \t Batch: 0 \t Loss: 0.6849876642 \t\n",
      "Epoch: 90 \t Batch: 10 \t Loss: 0.6833506227 \t\n",
      "Epoch: 90 \t Batch: 20 \t Loss: 0.6839263439 \t\n",
      "Epoch: 90 \t Batch: 30 \t Loss: 0.6835951209 \t\n",
      "Epoch: 90 \t Batch: 40 \t Loss: 0.6926448345 \t\n",
      "Epoch: 90 \t Batch: 50 \t Loss: 0.6861809492 \t\n",
      "Epoch: 90 \t Batch: 60 \t Loss: 0.6842886209 \t\n",
      "Epoch: 90 \t Batch: 70 \t Loss: 0.6867277026 \t\n",
      "Epoch: 91 \t Batch: 0 \t Loss: 0.6844123602 \t\n",
      "Epoch: 91 \t Batch: 10 \t Loss: 0.6828104854 \t\n",
      "Epoch: 91 \t Batch: 20 \t Loss: 0.6832272410 \t\n",
      "Epoch: 91 \t Batch: 30 \t Loss: 0.6829968691 \t\n",
      "Epoch: 91 \t Batch: 40 \t Loss: 0.6935873032 \t\n",
      "Epoch: 91 \t Batch: 50 \t Loss: 0.6861201525 \t\n",
      "Epoch: 91 \t Batch: 60 \t Loss: 0.6836656928 \t\n",
      "Epoch: 91 \t Batch: 70 \t Loss: 0.6871322393 \t\n",
      "Epoch: 92 \t Batch: 0 \t Loss: 0.6839873195 \t\n",
      "Epoch: 92 \t Batch: 10 \t Loss: 0.6823219061 \t\n",
      "Epoch: 92 \t Batch: 20 \t Loss: 0.6827783585 \t\n",
      "Epoch: 92 \t Batch: 30 \t Loss: 0.6824967265 \t\n",
      "Epoch: 92 \t Batch: 40 \t Loss: 0.6945682764 \t\n",
      "Epoch: 92 \t Batch: 50 \t Loss: 0.6861395240 \t\n",
      "Epoch: 92 \t Batch: 60 \t Loss: 0.6832169294 \t\n",
      "Epoch: 92 \t Batch: 70 \t Loss: 0.6875090599 \t\n",
      "Epoch: 93 \t Batch: 0 \t Loss: 0.6836992502 \t\n",
      "Epoch: 93 \t Batch: 10 \t Loss: 0.6819201708 \t\n",
      "Epoch: 93 \t Batch: 20 \t Loss: 0.6824885011 \t\n",
      "Epoch: 93 \t Batch: 30 \t Loss: 0.6821323633 \t\n",
      "Epoch: 93 \t Batch: 40 \t Loss: 0.6951968074 \t\n",
      "Epoch: 93 \t Batch: 50 \t Loss: 0.6861003041 \t\n",
      "Epoch: 93 \t Batch: 60 \t Loss: 0.6829081178 \t\n",
      "Epoch: 93 \t Batch: 70 \t Loss: 0.6875689626 \t\n",
      "Epoch: 94 \t Batch: 0 \t Loss: 0.6834560633 \t\n",
      "Epoch: 94 \t Batch: 10 \t Loss: 0.6816115975 \t\n",
      "Epoch: 94 \t Batch: 20 \t Loss: 0.6822022200 \t\n",
      "Epoch: 94 \t Batch: 30 \t Loss: 0.6818223596 \t\n",
      "Epoch: 94 \t Batch: 40 \t Loss: 0.6951901913 \t\n",
      "Epoch: 94 \t Batch: 50 \t Loss: 0.6858623624 \t\n",
      "Epoch: 94 \t Batch: 60 \t Loss: 0.6826924682 \t\n",
      "Epoch: 94 \t Batch: 70 \t Loss: 0.6871789694 \t\n",
      "Epoch: 95 \t Batch: 0 \t Loss: 0.6831896901 \t\n",
      "Epoch: 95 \t Batch: 10 \t Loss: 0.6813725233 \t\n",
      "Epoch: 95 \t Batch: 20 \t Loss: 0.6818953753 \t\n",
      "Epoch: 95 \t Batch: 30 \t Loss: 0.6815587878 \t\n",
      "Epoch: 95 \t Batch: 40 \t Loss: 0.6945118904 \t\n",
      "Epoch: 95 \t Batch: 50 \t Loss: 0.6854621172 \t\n",
      "Epoch: 95 \t Batch: 60 \t Loss: 0.6825962663 \t\n",
      "Epoch: 95 \t Batch: 70 \t Loss: 0.6864191890 \t\n",
      "Epoch: 96 \t Batch: 0 \t Loss: 0.6829765439 \t\n",
      "Epoch: 96 \t Batch: 10 \t Loss: 0.6812090278 \t\n",
      "Epoch: 96 \t Batch: 20 \t Loss: 0.6817107201 \t\n",
      "Epoch: 96 \t Batch: 30 \t Loss: 0.6814541817 \t\n",
      "Epoch: 96 \t Batch: 40 \t Loss: 0.6934911013 \t\n",
      "Epoch: 96 \t Batch: 50 \t Loss: 0.6850515604 \t\n",
      "Epoch: 96 \t Batch: 60 \t Loss: 0.6826577187 \t\n",
      "Epoch: 96 \t Batch: 70 \t Loss: 0.6855949163 \t\n",
      "Epoch: 97 \t Batch: 0 \t Loss: 0.6829113960 \t\n",
      "Epoch: 97 \t Batch: 10 \t Loss: 0.6810570955 \t\n",
      "Epoch: 97 \t Batch: 20 \t Loss: 0.6818041205 \t\n",
      "Epoch: 97 \t Batch: 30 \t Loss: 0.6814303994 \t\n",
      "Epoch: 97 \t Batch: 40 \t Loss: 0.6925641298 \t\n",
      "Epoch: 97 \t Batch: 50 \t Loss: 0.6847168207 \t\n",
      "Epoch: 97 \t Batch: 60 \t Loss: 0.6828197241 \t\n",
      "Epoch: 97 \t Batch: 70 \t Loss: 0.6849474311 \t\n",
      "Epoch: 98 \t Batch: 0 \t Loss: 0.6829806566 \t\n",
      "Epoch: 98 \t Batch: 10 \t Loss: 0.6810191870 \t\n",
      "Epoch: 98 \t Batch: 20 \t Loss: 0.6820928454 \t\n",
      "Epoch: 98 \t Batch: 30 \t Loss: 0.6814872622 \t\n",
      "Epoch: 98 \t Batch: 40 \t Loss: 0.6918651462 \t\n",
      "Epoch: 98 \t Batch: 50 \t Loss: 0.6845374107 \t\n",
      "Epoch: 98 \t Batch: 60 \t Loss: 0.6829988956 \t\n",
      "Epoch: 98 \t Batch: 70 \t Loss: 0.6846376061 \t\n",
      "Epoch: 99 \t Batch: 0 \t Loss: 0.6831338406 \t\n",
      "Epoch: 99 \t Batch: 10 \t Loss: 0.6809694171 \t\n",
      "Epoch: 99 \t Batch: 20 \t Loss: 0.6823271513 \t\n",
      "Epoch: 99 \t Batch: 30 \t Loss: 0.6815479398 \t\n",
      "Epoch: 99 \t Batch: 40 \t Loss: 0.6915460825 \t\n",
      "Epoch: 99 \t Batch: 50 \t Loss: 0.6844223738 \t\n",
      "Epoch: 99 \t Batch: 60 \t Loss: 0.6829685569 \t\n",
      "Epoch: 99 \t Batch: 70 \t Loss: 0.6844418049 \t\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "LOSS = []\n",
    "for e in range(epochs):\n",
    "\n",
    "  for i, (x, y) in enumerate(zip(X_train, labels_train_gender)):\n",
    "    y_pred = gender(x)\n",
    "    loss = criterio(y_pred, y)\n",
    "    #Calculate gradient of the loss \n",
    "    loss.backward()\n",
    "    #Gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    LOSS.append(loss.item())\n",
    "    if i%10==0:\n",
    "      print('Epoch: %d \\t Batch: %d \\t Loss: %.10f \\t'%(e,i, torch.tensor(LOSS[-10:]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubxEepgbqkLf"
   },
   "outputs": [],
   "source": [
    "y_pred = gender(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QENDEl6rtFH8",
    "outputId": "28d2e764-c564-4f3d-f304-1ff6ea98912b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "dxY1WRcztPHC",
    "outputId": "4546bb4b-162c-4ef9-c109-78362a9c3908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7386, -0.6496],\n",
      "        [-0.7305, -0.6571],\n",
      "        [-0.7147, -0.6720],\n",
      "        ...,\n",
      "        [-0.7147, -0.6721],\n",
      "        [-0.7386, -0.6496],\n",
      "        [-0.7012, -0.6851]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wD2a8-Wjtdtt"
   },
   "outputs": [],
   "source": [
    "#Choose the class of maximum probability\n",
    "y_pred = y_pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANbe8yBCt0VZ"
   },
   "outputs": [],
   "source": [
    "acc=metrics.accuracy_score(labels_test[0], y_pred)\n",
    "f1=metrics.f1_score(labels_test[0], y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v6RlS64nt_W6",
    "outputId": "15bb452a-acf8-40cc-9475-6b70500befc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jk1HjnX3h_9g"
   },
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Po5nOQaiBeG"
   },
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_features=1000)\n",
    "\n",
    "#just send in all your docs here\n",
    "tfidf_vectorizer_train=tfidf_vectorizer.fit_transform(t_train)\n",
    "\n",
    "tfidf_vectorizer_test=tfidf_vectorizer.transform(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "57tsiGy8ij-p",
    "outputId": "0f159c2b-2d93-4063-b282-abc9ea437a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzFJY6_UirPn"
   },
   "outputs": [],
   "source": [
    "class Gender(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Gender, self).__init__()\n",
    "    self.den1=nn.Linear(1000, 500)\n",
    "    self.den2=nn.Linear(500, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.den1(x))\n",
    "    \n",
    "    x = self.den2(x)\n",
    "\n",
    "    #The sum of the output of 1 for the loss function\n",
    "    return F.log_softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kobqjzBjwMI"
   },
   "outputs": [],
   "source": [
    "#convert to numpy array then to tensor\n",
    "X_train = torch.tensor(tfidf_vectorizer_train.toarray()).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4GbnSOoljeMe",
    "outputId": "83a893ca-b640-4418-f4aa-1c0c56843c07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "gender = Gender().cuda()\n",
    "\n",
    "batch_size = 50\n",
    "X_train = X_train.view(-1, batch_size, 1000)\n",
    "labels_train_gender = torch.tensor(labels_train[0]).view(-1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z53bZ9l8wjG2"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.view(-1, batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50a4wc5Bk9jp"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(gender.parameters(), lr=1e-5)\n",
    "criterio = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sEVOFltBk9jv",
    "outputId": "65903edc-6e7c-4cb1-fbdb-74de501e3937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t Loss: 0.6919872761 \t\n",
      "Epoch: 0 \t Batch: 10 \t Loss: 0.6929870844 \t\n",
      "Epoch: 0 \t Batch: 20 \t Loss: 0.6932057142 \t\n",
      "Epoch: 0 \t Batch: 30 \t Loss: 0.6930354238 \t\n",
      "Epoch: 0 \t Batch: 40 \t Loss: 0.6946591139 \t\n",
      "Epoch: 0 \t Batch: 50 \t Loss: 0.6934431791 \t\n",
      "Epoch: 0 \t Batch: 60 \t Loss: 0.6928786635 \t\n",
      "Epoch: 0 \t Batch: 70 \t Loss: 0.6939880252 \t\n",
      "Epoch: 1 \t Batch: 0 \t Loss: 0.6930136681 \t\n",
      "Epoch: 1 \t Batch: 10 \t Loss: 0.6929744482 \t\n",
      "Epoch: 1 \t Batch: 20 \t Loss: 0.6931899190 \t\n",
      "Epoch: 1 \t Batch: 30 \t Loss: 0.6930242181 \t\n",
      "Epoch: 1 \t Batch: 40 \t Loss: 0.6946060061 \t\n",
      "Epoch: 1 \t Batch: 50 \t Loss: 0.6934132576 \t\n",
      "Epoch: 1 \t Batch: 60 \t Loss: 0.6928614974 \t\n",
      "Epoch: 1 \t Batch: 70 \t Loss: 0.6939247251 \t\n",
      "Epoch: 2 \t Batch: 0 \t Loss: 0.6929915547 \t\n",
      "Epoch: 2 \t Batch: 10 \t Loss: 0.6929502487 \t\n",
      "Epoch: 2 \t Batch: 20 \t Loss: 0.6931577921 \t\n",
      "Epoch: 2 \t Batch: 30 \t Loss: 0.6930103302 \t\n",
      "Epoch: 2 \t Batch: 40 \t Loss: 0.6944727302 \t\n",
      "Epoch: 2 \t Batch: 50 \t Loss: 0.6933540702 \t\n",
      "Epoch: 2 \t Batch: 60 \t Loss: 0.6928371191 \t\n",
      "Epoch: 2 \t Batch: 70 \t Loss: 0.6938046217 \t\n",
      "Epoch: 3 \t Batch: 0 \t Loss: 0.6929566264 \t\n",
      "Epoch: 3 \t Batch: 10 \t Loss: 0.6929148436 \t\n",
      "Epoch: 3 \t Batch: 20 \t Loss: 0.6931101084 \t\n",
      "Epoch: 3 \t Batch: 30 \t Loss: 0.6929941773 \t\n",
      "Epoch: 3 \t Batch: 40 \t Loss: 0.6942632794 \t\n",
      "Epoch: 3 \t Batch: 50 \t Loss: 0.6932673454 \t\n",
      "Epoch: 3 \t Batch: 60 \t Loss: 0.6928054690 \t\n",
      "Epoch: 3 \t Batch: 70 \t Loss: 0.6936317086 \t\n",
      "Epoch: 4 \t Batch: 0 \t Loss: 0.6929098368 \t\n",
      "Epoch: 4 \t Batch: 10 \t Loss: 0.6928685904 \t\n",
      "Epoch: 4 \t Batch: 20 \t Loss: 0.6930481195 \t\n",
      "Epoch: 4 \t Batch: 30 \t Loss: 0.6929758191 \t\n",
      "Epoch: 4 \t Batch: 40 \t Loss: 0.6939842105 \t\n",
      "Epoch: 4 \t Batch: 50 \t Loss: 0.6931559443 \t\n",
      "Epoch: 4 \t Batch: 60 \t Loss: 0.6927669048 \t\n",
      "Epoch: 4 \t Batch: 70 \t Loss: 0.6934112906 \t\n",
      "Epoch: 5 \t Batch: 0 \t Loss: 0.6928526163 \t\n",
      "Epoch: 5 \t Batch: 10 \t Loss: 0.6928115487 \t\n",
      "Epoch: 5 \t Batch: 20 \t Loss: 0.6929733157 \t\n",
      "Epoch: 5 \t Batch: 30 \t Loss: 0.6929556727 \t\n",
      "Epoch: 5 \t Batch: 40 \t Loss: 0.6936445236 \t\n",
      "Epoch: 5 \t Batch: 50 \t Loss: 0.6930226684 \t\n",
      "Epoch: 5 \t Batch: 60 \t Loss: 0.6927208900 \t\n",
      "Epoch: 5 \t Batch: 70 \t Loss: 0.6931493282 \t\n",
      "Epoch: 6 \t Batch: 0 \t Loss: 0.6927851439 \t\n",
      "Epoch: 6 \t Batch: 10 \t Loss: 0.6927448511 \t\n",
      "Epoch: 6 \t Batch: 20 \t Loss: 0.6928860545 \t\n",
      "Epoch: 6 \t Batch: 30 \t Loss: 0.6929339170 \t\n",
      "Epoch: 6 \t Batch: 40 \t Loss: 0.6932541728 \t\n",
      "Epoch: 6 \t Batch: 50 \t Loss: 0.6928700209 \t\n",
      "Epoch: 6 \t Batch: 60 \t Loss: 0.6926656961 \t\n",
      "Epoch: 6 \t Batch: 70 \t Loss: 0.6928535700 \t\n",
      "Epoch: 7 \t Batch: 0 \t Loss: 0.6927084327 \t\n",
      "Epoch: 7 \t Batch: 10 \t Loss: 0.6926687956 \t\n",
      "Epoch: 7 \t Batch: 20 \t Loss: 0.6927869916 \t\n",
      "Epoch: 7 \t Batch: 30 \t Loss: 0.6929102540 \t\n",
      "Epoch: 7 \t Batch: 40 \t Loss: 0.6928235292 \t\n",
      "Epoch: 7 \t Batch: 50 \t Loss: 0.6927016973 \t\n",
      "Epoch: 7 \t Batch: 60 \t Loss: 0.6925988793 \t\n",
      "Epoch: 7 \t Batch: 70 \t Loss: 0.6925301552 \t\n",
      "Epoch: 8 \t Batch: 0 \t Loss: 0.6926215887 \t\n",
      "Epoch: 8 \t Batch: 10 \t Loss: 0.6925819516 \t\n",
      "Epoch: 8 \t Batch: 20 \t Loss: 0.6926766634 \t\n",
      "Epoch: 8 \t Batch: 30 \t Loss: 0.6928820014 \t\n",
      "Epoch: 8 \t Batch: 40 \t Loss: 0.6923645735 \t\n",
      "Epoch: 8 \t Batch: 50 \t Loss: 0.6925197244 \t\n",
      "Epoch: 8 \t Batch: 60 \t Loss: 0.6925174594 \t\n",
      "Epoch: 8 \t Batch: 70 \t Loss: 0.6921878457 \t\n",
      "Epoch: 9 \t Batch: 0 \t Loss: 0.6925259829 \t\n",
      "Epoch: 9 \t Batch: 10 \t Loss: 0.6924836040 \t\n",
      "Epoch: 9 \t Batch: 20 \t Loss: 0.6925550699 \t\n",
      "Epoch: 9 \t Batch: 30 \t Loss: 0.6928466558 \t\n",
      "Epoch: 9 \t Batch: 40 \t Loss: 0.6918884516 \t\n",
      "Epoch: 9 \t Batch: 50 \t Loss: 0.6923268437 \t\n",
      "Epoch: 9 \t Batch: 60 \t Loss: 0.6924189329 \t\n",
      "Epoch: 9 \t Batch: 70 \t Loss: 0.6918310523 \t\n",
      "Epoch: 10 \t Batch: 0 \t Loss: 0.6924182177 \t\n",
      "Epoch: 10 \t Batch: 10 \t Loss: 0.6923696399 \t\n",
      "Epoch: 10 \t Batch: 20 \t Loss: 0.6924209595 \t\n",
      "Epoch: 10 \t Batch: 30 \t Loss: 0.6928025484 \t\n",
      "Epoch: 10 \t Batch: 40 \t Loss: 0.6914030313 \t\n",
      "Epoch: 10 \t Batch: 50 \t Loss: 0.6921263933 \t\n",
      "Epoch: 10 \t Batch: 60 \t Loss: 0.6922985315 \t\n",
      "Epoch: 10 \t Batch: 70 \t Loss: 0.6914675236 \t\n",
      "Epoch: 11 \t Batch: 0 \t Loss: 0.6922966242 \t\n",
      "Epoch: 11 \t Batch: 10 \t Loss: 0.6922358274 \t\n",
      "Epoch: 11 \t Batch: 20 \t Loss: 0.6922714114 \t\n",
      "Epoch: 11 \t Batch: 30 \t Loss: 0.6927458644 \t\n",
      "Epoch: 11 \t Batch: 40 \t Loss: 0.6909191608 \t\n",
      "Epoch: 11 \t Batch: 50 \t Loss: 0.6919167638 \t\n",
      "Epoch: 11 \t Batch: 60 \t Loss: 0.6921504736 \t\n",
      "Epoch: 11 \t Batch: 70 \t Loss: 0.6911017895 \t\n",
      "Epoch: 12 \t Batch: 0 \t Loss: 0.6921596527 \t\n",
      "Epoch: 12 \t Batch: 10 \t Loss: 0.6920789480 \t\n",
      "Epoch: 12 \t Batch: 20 \t Loss: 0.6921036839 \t\n",
      "Epoch: 12 \t Batch: 30 \t Loss: 0.6926732659 \t\n",
      "Epoch: 12 \t Batch: 40 \t Loss: 0.6904461384 \t\n",
      "Epoch: 12 \t Batch: 50 \t Loss: 0.6916982532 \t\n",
      "Epoch: 12 \t Batch: 60 \t Loss: 0.6919699907 \t\n",
      "Epoch: 12 \t Batch: 70 \t Loss: 0.6907364130 \t\n",
      "Epoch: 13 \t Batch: 0 \t Loss: 0.6920015216 \t\n",
      "Epoch: 13 \t Batch: 10 \t Loss: 0.6918968558 \t\n",
      "Epoch: 13 \t Batch: 20 \t Loss: 0.6919137836 \t\n",
      "Epoch: 13 \t Batch: 30 \t Loss: 0.6925764084 \t\n",
      "Epoch: 13 \t Batch: 40 \t Loss: 0.6899904013 \t\n",
      "Epoch: 13 \t Batch: 50 \t Loss: 0.6914692521 \t\n",
      "Epoch: 13 \t Batch: 60 \t Loss: 0.6917495131 \t\n",
      "Epoch: 13 \t Batch: 70 \t Loss: 0.6903758049 \t\n",
      "Epoch: 14 \t Batch: 0 \t Loss: 0.6918197870 \t\n",
      "Epoch: 14 \t Batch: 10 \t Loss: 0.6916852593 \t\n",
      "Epoch: 14 \t Batch: 20 \t Loss: 0.6916968226 \t\n",
      "Epoch: 14 \t Batch: 30 \t Loss: 0.6924510598 \t\n",
      "Epoch: 14 \t Batch: 40 \t Loss: 0.6895548105 \t\n",
      "Epoch: 14 \t Batch: 50 \t Loss: 0.6912268400 \t\n",
      "Epoch: 14 \t Batch: 60 \t Loss: 0.6914852262 \t\n",
      "Epoch: 14 \t Batch: 70 \t Loss: 0.6900199652 \t\n",
      "Epoch: 15 \t Batch: 0 \t Loss: 0.6916109324 \t\n",
      "Epoch: 15 \t Batch: 10 \t Loss: 0.6914387941 \t\n",
      "Epoch: 15 \t Batch: 20 \t Loss: 0.6914521456 \t\n",
      "Epoch: 15 \t Batch: 30 \t Loss: 0.6922942400 \t\n",
      "Epoch: 15 \t Batch: 40 \t Loss: 0.6891434193 \t\n",
      "Epoch: 15 \t Batch: 50 \t Loss: 0.6909698248 \t\n",
      "Epoch: 15 \t Batch: 60 \t Loss: 0.6911730766 \t\n",
      "Epoch: 15 \t Batch: 70 \t Loss: 0.6896711588 \t\n",
      "Epoch: 16 \t Batch: 0 \t Loss: 0.6913717985 \t\n",
      "Epoch: 16 \t Batch: 10 \t Loss: 0.6911524534 \t\n",
      "Epoch: 16 \t Batch: 20 \t Loss: 0.6911743879 \t\n",
      "Epoch: 16 \t Batch: 30 \t Loss: 0.6921041012 \t\n",
      "Epoch: 16 \t Batch: 40 \t Loss: 0.6887613535 \t\n",
      "Epoch: 16 \t Batch: 50 \t Loss: 0.6906977892 \t\n",
      "Epoch: 16 \t Batch: 60 \t Loss: 0.6908105016 \t\n",
      "Epoch: 16 \t Batch: 70 \t Loss: 0.6893288493 \t\n",
      "Epoch: 17 \t Batch: 0 \t Loss: 0.6910989881 \t\n",
      "Epoch: 17 \t Batch: 10 \t Loss: 0.6908231974 \t\n",
      "Epoch: 17 \t Batch: 20 \t Loss: 0.6908648610 \t\n",
      "Epoch: 17 \t Batch: 30 \t Loss: 0.6918753386 \t\n",
      "Epoch: 17 \t Batch: 40 \t Loss: 0.6884101033 \t\n",
      "Epoch: 17 \t Batch: 50 \t Loss: 0.6904110909 \t\n",
      "Epoch: 17 \t Batch: 60 \t Loss: 0.6903947592 \t\n",
      "Epoch: 17 \t Batch: 70 \t Loss: 0.6889944077 \t\n",
      "Epoch: 18 \t Batch: 0 \t Loss: 0.6907930374 \t\n",
      "Epoch: 18 \t Batch: 10 \t Loss: 0.6904553771 \t\n",
      "Epoch: 18 \t Batch: 20 \t Loss: 0.6905220151 \t\n",
      "Epoch: 18 \t Batch: 30 \t Loss: 0.6916071773 \t\n",
      "Epoch: 18 \t Batch: 40 \t Loss: 0.6880912185 \t\n",
      "Epoch: 18 \t Batch: 50 \t Loss: 0.6901074052 \t\n",
      "Epoch: 18 \t Batch: 60 \t Loss: 0.6899261475 \t\n",
      "Epoch: 18 \t Batch: 70 \t Loss: 0.6886666417 \t\n",
      "Epoch: 19 \t Batch: 0 \t Loss: 0.6904538870 \t\n",
      "Epoch: 19 \t Batch: 10 \t Loss: 0.6900458336 \t\n",
      "Epoch: 19 \t Batch: 20 \t Loss: 0.6901490092 \t\n",
      "Epoch: 19 \t Batch: 30 \t Loss: 0.6913005710 \t\n",
      "Epoch: 19 \t Batch: 40 \t Loss: 0.6878031492 \t\n",
      "Epoch: 19 \t Batch: 50 \t Loss: 0.6897876263 \t\n",
      "Epoch: 19 \t Batch: 60 \t Loss: 0.6894067526 \t\n",
      "Epoch: 19 \t Batch: 70 \t Loss: 0.6883462667 \t\n",
      "Epoch: 20 \t Batch: 0 \t Loss: 0.6900838017 \t\n",
      "Epoch: 20 \t Batch: 10 \t Loss: 0.6895979643 \t\n",
      "Epoch: 20 \t Batch: 20 \t Loss: 0.6897464395 \t\n",
      "Epoch: 20 \t Batch: 30 \t Loss: 0.6909576654 \t\n",
      "Epoch: 20 \t Batch: 40 \t Loss: 0.6875402927 \t\n",
      "Epoch: 20 \t Batch: 50 \t Loss: 0.6894526482 \t\n",
      "Epoch: 20 \t Batch: 60 \t Loss: 0.6888391376 \t\n",
      "Epoch: 20 \t Batch: 70 \t Loss: 0.6880332232 \t\n",
      "Epoch: 21 \t Batch: 0 \t Loss: 0.6896855831 \t\n",
      "Epoch: 21 \t Batch: 10 \t Loss: 0.6891132593 \t\n",
      "Epoch: 21 \t Batch: 20 \t Loss: 0.6893135309 \t\n",
      "Epoch: 21 \t Batch: 30 \t Loss: 0.6905791759 \t\n",
      "Epoch: 21 \t Batch: 40 \t Loss: 0.6873019338 \t\n",
      "Epoch: 21 \t Batch: 50 \t Loss: 0.6891026497 \t\n",
      "Epoch: 21 \t Batch: 60 \t Loss: 0.6882244349 \t\n",
      "Epoch: 21 \t Batch: 70 \t Loss: 0.6877225041 \t\n",
      "Epoch: 22 \t Batch: 0 \t Loss: 0.6892607808 \t\n",
      "Epoch: 22 \t Batch: 10 \t Loss: 0.6885951757 \t\n",
      "Epoch: 22 \t Batch: 20 \t Loss: 0.6888513565 \t\n",
      "Epoch: 22 \t Batch: 30 \t Loss: 0.6901736856 \t\n",
      "Epoch: 22 \t Batch: 40 \t Loss: 0.6870835423 \t\n",
      "Epoch: 22 \t Batch: 50 \t Loss: 0.6887382269 \t\n",
      "Epoch: 22 \t Batch: 60 \t Loss: 0.6875688434 \t\n",
      "Epoch: 22 \t Batch: 70 \t Loss: 0.6874120235 \t\n",
      "Epoch: 23 \t Batch: 0 \t Loss: 0.6888118386 \t\n",
      "Epoch: 23 \t Batch: 10 \t Loss: 0.6880459189 \t\n",
      "Epoch: 23 \t Batch: 20 \t Loss: 0.6883654594 \t\n",
      "Epoch: 23 \t Batch: 30 \t Loss: 0.6897433996 \t\n",
      "Epoch: 23 \t Batch: 40 \t Loss: 0.6868758202 \t\n",
      "Epoch: 23 \t Batch: 50 \t Loss: 0.6883594990 \t\n",
      "Epoch: 23 \t Batch: 60 \t Loss: 0.6868797541 \t\n",
      "Epoch: 23 \t Batch: 70 \t Loss: 0.6870971918 \t\n",
      "Epoch: 24 \t Batch: 0 \t Loss: 0.6883412600 \t\n",
      "Epoch: 24 \t Batch: 10 \t Loss: 0.6874733567 \t\n",
      "Epoch: 24 \t Batch: 20 \t Loss: 0.6878589988 \t\n",
      "Epoch: 24 \t Batch: 30 \t Loss: 0.6892949343 \t\n",
      "Epoch: 24 \t Batch: 40 \t Loss: 0.6866726279 \t\n",
      "Epoch: 24 \t Batch: 50 \t Loss: 0.6879681349 \t\n",
      "Epoch: 24 \t Batch: 60 \t Loss: 0.6861616373 \t\n",
      "Epoch: 24 \t Batch: 70 \t Loss: 0.6867743731 \t\n",
      "Epoch: 25 \t Batch: 0 \t Loss: 0.6878528595 \t\n",
      "Epoch: 25 \t Batch: 10 \t Loss: 0.6868806481 \t\n",
      "Epoch: 25 \t Batch: 20 \t Loss: 0.6873330474 \t\n",
      "Epoch: 25 \t Batch: 30 \t Loss: 0.6888306141 \t\n",
      "Epoch: 25 \t Batch: 40 \t Loss: 0.6864612699 \t\n",
      "Epoch: 25 \t Batch: 50 \t Loss: 0.6875633001 \t\n",
      "Epoch: 25 \t Batch: 60 \t Loss: 0.6854177117 \t\n",
      "Epoch: 25 \t Batch: 70 \t Loss: 0.6864392757 \t\n",
      "Epoch: 26 \t Batch: 0 \t Loss: 0.6873506308 \t\n",
      "Epoch: 26 \t Batch: 10 \t Loss: 0.6862679720 \t\n",
      "Epoch: 26 \t Batch: 20 \t Loss: 0.6867883801 \t\n",
      "Epoch: 26 \t Batch: 30 \t Loss: 0.6883553267 \t\n",
      "Epoch: 26 \t Batch: 40 \t Loss: 0.6862295866 \t\n",
      "Epoch: 26 \t Batch: 50 \t Loss: 0.6871427298 \t\n",
      "Epoch: 26 \t Batch: 60 \t Loss: 0.6846486330 \t\n",
      "Epoch: 26 \t Batch: 70 \t Loss: 0.6860833764 \t\n",
      "Epoch: 27 \t Batch: 0 \t Loss: 0.6868360043 \t\n",
      "Epoch: 27 \t Batch: 10 \t Loss: 0.6856365204 \t\n",
      "Epoch: 27 \t Batch: 20 \t Loss: 0.6862233877 \t\n",
      "Epoch: 27 \t Batch: 30 \t Loss: 0.6878732443 \t\n",
      "Epoch: 27 \t Batch: 40 \t Loss: 0.6859614253 \t\n",
      "Epoch: 27 \t Batch: 50 \t Loss: 0.6867023110 \t\n",
      "Epoch: 27 \t Batch: 60 \t Loss: 0.6838610768 \t\n",
      "Epoch: 27 \t Batch: 70 \t Loss: 0.6856958866 \t\n",
      "Epoch: 28 \t Batch: 0 \t Loss: 0.6863059998 \t\n",
      "Epoch: 28 \t Batch: 10 \t Loss: 0.6849876642 \t\n",
      "Epoch: 28 \t Batch: 20 \t Loss: 0.6856391430 \t\n",
      "Epoch: 28 \t Batch: 30 \t Loss: 0.6873835325 \t\n",
      "Epoch: 28 \t Batch: 40 \t Loss: 0.6856461167 \t\n",
      "Epoch: 28 \t Batch: 50 \t Loss: 0.6862417459 \t\n",
      "Epoch: 28 \t Batch: 60 \t Loss: 0.6830545664 \t\n",
      "Epoch: 28 \t Batch: 70 \t Loss: 0.6852684021 \t\n",
      "Epoch: 29 \t Batch: 0 \t Loss: 0.6857624650 \t\n",
      "Epoch: 29 \t Batch: 10 \t Loss: 0.6843223572 \t\n",
      "Epoch: 29 \t Batch: 20 \t Loss: 0.6850354075 \t\n",
      "Epoch: 29 \t Batch: 30 \t Loss: 0.6868883371 \t\n",
      "Epoch: 29 \t Batch: 40 \t Loss: 0.6852689981 \t\n",
      "Epoch: 29 \t Batch: 50 \t Loss: 0.6857541800 \t\n",
      "Epoch: 29 \t Batch: 60 \t Loss: 0.6822333336 \t\n",
      "Epoch: 29 \t Batch: 70 \t Loss: 0.6847870946 \t\n",
      "Epoch: 30 \t Batch: 0 \t Loss: 0.6852012873 \t\n",
      "Epoch: 30 \t Batch: 10 \t Loss: 0.6836382151 \t\n",
      "Epoch: 30 \t Batch: 20 \t Loss: 0.6844108105 \t\n",
      "Epoch: 30 \t Batch: 30 \t Loss: 0.6863887310 \t\n",
      "Epoch: 30 \t Batch: 40 \t Loss: 0.6848148108 \t\n",
      "Epoch: 30 \t Batch: 50 \t Loss: 0.6852368116 \t\n",
      "Epoch: 30 \t Batch: 60 \t Loss: 0.6813928485 \t\n",
      "Epoch: 30 \t Batch: 70 \t Loss: 0.6842424273 \t\n",
      "Epoch: 31 \t Batch: 0 \t Loss: 0.6846207976 \t\n",
      "Epoch: 31 \t Batch: 10 \t Loss: 0.6829329133 \t\n",
      "Epoch: 31 \t Batch: 20 \t Loss: 0.6837633252 \t\n",
      "Epoch: 31 \t Batch: 30 \t Loss: 0.6858834624 \t\n",
      "Epoch: 31 \t Batch: 40 \t Loss: 0.6842674017 \t\n",
      "Epoch: 31 \t Batch: 50 \t Loss: 0.6846828461 \t\n",
      "Epoch: 31 \t Batch: 60 \t Loss: 0.6805332899 \t\n",
      "Epoch: 31 \t Batch: 70 \t Loss: 0.6836279631 \t\n",
      "Epoch: 32 \t Batch: 0 \t Loss: 0.6840243340 \t\n",
      "Epoch: 32 \t Batch: 10 \t Loss: 0.6822025180 \t\n",
      "Epoch: 32 \t Batch: 20 \t Loss: 0.6830899715 \t\n",
      "Epoch: 32 \t Batch: 30 \t Loss: 0.6853694916 \t\n",
      "Epoch: 32 \t Batch: 40 \t Loss: 0.6836165190 \t\n",
      "Epoch: 32 \t Batch: 50 \t Loss: 0.6840898991 \t\n",
      "Epoch: 32 \t Batch: 60 \t Loss: 0.6796549559 \t\n",
      "Epoch: 32 \t Batch: 70 \t Loss: 0.6829329729 \t\n",
      "Epoch: 33 \t Batch: 0 \t Loss: 0.6834081411 \t\n",
      "Epoch: 33 \t Batch: 10 \t Loss: 0.6814463735 \t\n",
      "Epoch: 33 \t Batch: 20 \t Loss: 0.6823902130 \t\n",
      "Epoch: 33 \t Batch: 30 \t Loss: 0.6848475337 \t\n",
      "Epoch: 33 \t Batch: 40 \t Loss: 0.6828542948 \t\n",
      "Epoch: 33 \t Batch: 50 \t Loss: 0.6834549904 \t\n",
      "Epoch: 33 \t Batch: 60 \t Loss: 0.6787562370 \t\n",
      "Epoch: 33 \t Batch: 70 \t Loss: 0.6821510196 \t\n",
      "Epoch: 34 \t Batch: 0 \t Loss: 0.6827687025 \t\n",
      "Epoch: 34 \t Batch: 10 \t Loss: 0.6806627512 \t\n",
      "Epoch: 34 \t Batch: 20 \t Loss: 0.6816640496 \t\n",
      "Epoch: 34 \t Batch: 30 \t Loss: 0.6843206286 \t\n",
      "Epoch: 34 \t Batch: 40 \t Loss: 0.6819751263 \t\n",
      "Epoch: 34 \t Batch: 50 \t Loss: 0.6827789545 \t\n",
      "Epoch: 34 \t Batch: 60 \t Loss: 0.6778351665 \t\n",
      "Epoch: 34 \t Batch: 70 \t Loss: 0.6812806726 \t\n",
      "Epoch: 35 \t Batch: 0 \t Loss: 0.6821067929 \t\n",
      "Epoch: 35 \t Batch: 10 \t Loss: 0.6798514128 \t\n",
      "Epoch: 35 \t Batch: 20 \t Loss: 0.6809086800 \t\n",
      "Epoch: 35 \t Batch: 30 \t Loss: 0.6837896109 \t\n",
      "Epoch: 35 \t Batch: 40 \t Loss: 0.6809751391 \t\n",
      "Epoch: 35 \t Batch: 50 \t Loss: 0.6820605397 \t\n",
      "Epoch: 35 \t Batch: 60 \t Loss: 0.6768931150 \t\n",
      "Epoch: 35 \t Batch: 70 \t Loss: 0.6803225279 \t\n",
      "Epoch: 36 \t Batch: 0 \t Loss: 0.6814212203 \t\n",
      "Epoch: 36 \t Batch: 10 \t Loss: 0.6790114045 \t\n",
      "Epoch: 36 \t Batch: 20 \t Loss: 0.6801253557 \t\n",
      "Epoch: 36 \t Batch: 30 \t Loss: 0.6832531691 \t\n",
      "Epoch: 36 \t Batch: 40 \t Loss: 0.6798630357 \t\n",
      "Epoch: 36 \t Batch: 50 \t Loss: 0.6813032627 \t\n",
      "Epoch: 36 \t Batch: 60 \t Loss: 0.6759286523 \t\n",
      "Epoch: 36 \t Batch: 70 \t Loss: 0.6792817712 \t\n",
      "Epoch: 37 \t Batch: 0 \t Loss: 0.6807135940 \t\n",
      "Epoch: 37 \t Batch: 10 \t Loss: 0.6781432033 \t\n",
      "Epoch: 37 \t Batch: 20 \t Loss: 0.6793149114 \t\n",
      "Epoch: 37 \t Batch: 30 \t Loss: 0.6827160120 \t\n",
      "Epoch: 37 \t Batch: 40 \t Loss: 0.6786470413 \t\n",
      "Epoch: 37 \t Batch: 50 \t Loss: 0.6805089116 \t\n",
      "Epoch: 37 \t Batch: 60 \t Loss: 0.6749401689 \t\n",
      "Epoch: 37 \t Batch: 70 \t Loss: 0.6781671643 \t\n",
      "Epoch: 38 \t Batch: 0 \t Loss: 0.6799870729 \t\n",
      "Epoch: 38 \t Batch: 10 \t Loss: 0.6772471070 \t\n",
      "Epoch: 38 \t Batch: 20 \t Loss: 0.6784778833 \t\n",
      "Epoch: 38 \t Batch: 30 \t Loss: 0.6821784377 \t\n",
      "Epoch: 38 \t Batch: 40 \t Loss: 0.6773408651 \t\n",
      "Epoch: 38 \t Batch: 50 \t Loss: 0.6796854734 \t\n",
      "Epoch: 38 \t Batch: 60 \t Loss: 0.6739247441 \t\n",
      "Epoch: 38 \t Batch: 70 \t Loss: 0.6769874692 \t\n",
      "Epoch: 39 \t Batch: 0 \t Loss: 0.6792427301 \t\n",
      "Epoch: 39 \t Batch: 10 \t Loss: 0.6763201356 \t\n",
      "Epoch: 39 \t Batch: 20 \t Loss: 0.6776158214 \t\n",
      "Epoch: 39 \t Batch: 30 \t Loss: 0.6816423535 \t\n",
      "Epoch: 39 \t Batch: 40 \t Loss: 0.6759631634 \t\n",
      "Epoch: 39 \t Batch: 50 \t Loss: 0.6788393259 \t\n",
      "Epoch: 39 \t Batch: 60 \t Loss: 0.6728810072 \t\n",
      "Epoch: 39 \t Batch: 70 \t Loss: 0.6757526398 \t\n",
      "Epoch: 40 \t Batch: 0 \t Loss: 0.6784785390 \t\n",
      "Epoch: 40 \t Batch: 10 \t Loss: 0.6753603816 \t\n",
      "Epoch: 40 \t Batch: 20 \t Loss: 0.6767312884 \t\n",
      "Epoch: 40 \t Batch: 30 \t Loss: 0.6811031103 \t\n",
      "Epoch: 40 \t Batch: 40 \t Loss: 0.6745318174 \t\n",
      "Epoch: 40 \t Batch: 50 \t Loss: 0.6779731512 \t\n",
      "Epoch: 40 \t Batch: 60 \t Loss: 0.6718020439 \t\n",
      "Epoch: 40 \t Batch: 70 \t Loss: 0.6744777560 \t\n",
      "Epoch: 41 \t Batch: 0 \t Loss: 0.6776947379 \t\n",
      "Epoch: 41 \t Batch: 10 \t Loss: 0.6743679643 \t\n",
      "Epoch: 41 \t Batch: 20 \t Loss: 0.6758199334 \t\n",
      "Epoch: 41 \t Batch: 30 \t Loss: 0.6805540919 \t\n",
      "Epoch: 41 \t Batch: 40 \t Loss: 0.6730679274 \t\n",
      "Epoch: 41 \t Batch: 50 \t Loss: 0.6770901680 \t\n",
      "Epoch: 41 \t Batch: 60 \t Loss: 0.6706795692 \t\n",
      "Epoch: 41 \t Batch: 70 \t Loss: 0.6731735468 \t\n",
      "Epoch: 42 \t Batch: 0 \t Loss: 0.6768841743 \t\n",
      "Epoch: 42 \t Batch: 10 \t Loss: 0.6733349562 \t\n",
      "Epoch: 42 \t Batch: 20 \t Loss: 0.6748732924 \t\n",
      "Epoch: 42 \t Batch: 30 \t Loss: 0.6799877286 \t\n",
      "Epoch: 42 \t Batch: 40 \t Loss: 0.6715961695 \t\n",
      "Epoch: 42 \t Batch: 50 \t Loss: 0.6761909723 \t\n",
      "Epoch: 42 \t Batch: 60 \t Loss: 0.6695033312 \t\n",
      "Epoch: 42 \t Batch: 70 \t Loss: 0.6718494892 \t\n",
      "Epoch: 43 \t Batch: 0 \t Loss: 0.6760408878 \t\n",
      "Epoch: 43 \t Batch: 10 \t Loss: 0.6722532511 \t\n",
      "Epoch: 43 \t Batch: 20 \t Loss: 0.6738850474 \t\n",
      "Epoch: 43 \t Batch: 30 \t Loss: 0.6793934107 \t\n",
      "Epoch: 43 \t Batch: 40 \t Loss: 0.6701368093 \t\n",
      "Epoch: 43 \t Batch: 50 \t Loss: 0.6752732396 \t\n",
      "Epoch: 43 \t Batch: 60 \t Loss: 0.6682561040 \t\n",
      "Epoch: 43 \t Batch: 70 \t Loss: 0.6705147028 \t\n",
      "Epoch: 44 \t Batch: 0 \t Loss: 0.6751527786 \t\n",
      "Epoch: 44 \t Batch: 10 \t Loss: 0.6711099744 \t\n",
      "Epoch: 44 \t Batch: 20 \t Loss: 0.6728482246 \t\n",
      "Epoch: 44 \t Batch: 30 \t Loss: 0.6787595749 \t\n",
      "Epoch: 44 \t Batch: 40 \t Loss: 0.6687085032 \t\n",
      "Epoch: 44 \t Batch: 50 \t Loss: 0.6743341684 \t\n",
      "Epoch: 44 \t Batch: 60 \t Loss: 0.6669231653 \t\n",
      "Epoch: 44 \t Batch: 70 \t Loss: 0.6691773534 \t\n",
      "Epoch: 45 \t Batch: 0 \t Loss: 0.6742099524 \t\n",
      "Epoch: 45 \t Batch: 10 \t Loss: 0.6698983908 \t\n",
      "Epoch: 45 \t Batch: 20 \t Loss: 0.6717509031 \t\n",
      "Epoch: 45 \t Batch: 30 \t Loss: 0.6780717373 \t\n",
      "Epoch: 45 \t Batch: 40 \t Loss: 0.6673310995 \t\n",
      "Epoch: 45 \t Batch: 50 \t Loss: 0.6733719110 \t\n",
      "Epoch: 45 \t Batch: 60 \t Loss: 0.6654912829 \t\n",
      "Epoch: 45 \t Batch: 70 \t Loss: 0.6678417921 \t\n",
      "Epoch: 46 \t Batch: 0 \t Loss: 0.6731991172 \t\n",
      "Epoch: 46 \t Batch: 10 \t Loss: 0.6686068773 \t\n",
      "Epoch: 46 \t Batch: 20 \t Loss: 0.6705828905 \t\n",
      "Epoch: 46 \t Batch: 30 \t Loss: 0.6773141623 \t\n",
      "Epoch: 46 \t Batch: 40 \t Loss: 0.6660203934 \t\n",
      "Epoch: 46 \t Batch: 50 \t Loss: 0.6723814607 \t\n",
      "Epoch: 46 \t Batch: 60 \t Loss: 0.6639488339 \t\n",
      "Epoch: 46 \t Batch: 70 \t Loss: 0.6665143371 \t\n",
      "Epoch: 47 \t Batch: 0 \t Loss: 0.6721128225 \t\n",
      "Epoch: 47 \t Batch: 10 \t Loss: 0.6672325730 \t\n",
      "Epoch: 47 \t Batch: 20 \t Loss: 0.6693412662 \t\n",
      "Epoch: 47 \t Batch: 30 \t Loss: 0.6764775515 \t\n",
      "Epoch: 47 \t Batch: 40 \t Loss: 0.6647863984 \t\n",
      "Epoch: 47 \t Batch: 50 \t Loss: 0.6713618040 \t\n",
      "Epoch: 47 \t Batch: 60 \t Loss: 0.6622943878 \t\n",
      "Epoch: 47 \t Batch: 70 \t Loss: 0.6652027369 \t\n",
      "Epoch: 48 \t Batch: 0 \t Loss: 0.6709490418 \t\n",
      "Epoch: 48 \t Batch: 10 \t Loss: 0.6657759547 \t\n",
      "Epoch: 48 \t Batch: 20 \t Loss: 0.6680268049 \t\n",
      "Epoch: 48 \t Batch: 30 \t Loss: 0.6755634546 \t\n",
      "Epoch: 48 \t Batch: 40 \t Loss: 0.6636404395 \t\n",
      "Epoch: 48 \t Batch: 50 \t Loss: 0.6703180075 \t\n",
      "Epoch: 48 \t Batch: 60 \t Loss: 0.6605286002 \t\n",
      "Epoch: 48 \t Batch: 70 \t Loss: 0.6639119983 \t\n",
      "Epoch: 49 \t Batch: 0 \t Loss: 0.6697087884 \t\n",
      "Epoch: 49 \t Batch: 10 \t Loss: 0.6642442942 \t\n",
      "Epoch: 49 \t Batch: 20 \t Loss: 0.6666462421 \t\n",
      "Epoch: 49 \t Batch: 30 \t Loss: 0.6745752096 \t\n",
      "Epoch: 49 \t Batch: 40 \t Loss: 0.6625923514 \t\n",
      "Epoch: 49 \t Batch: 50 \t Loss: 0.6692606211 \t\n",
      "Epoch: 49 \t Batch: 60 \t Loss: 0.6586699486 \t\n",
      "Epoch: 49 \t Batch: 70 \t Loss: 0.6626479030 \t\n",
      "Epoch: 50 \t Batch: 0 \t Loss: 0.6684009433 \t\n",
      "Epoch: 50 \t Batch: 10 \t Loss: 0.6626530886 \t\n",
      "Epoch: 50 \t Batch: 20 \t Loss: 0.6652107239 \t\n",
      "Epoch: 50 \t Batch: 30 \t Loss: 0.6735265851 \t\n",
      "Epoch: 50 \t Batch: 40 \t Loss: 0.6616469026 \t\n",
      "Epoch: 50 \t Batch: 50 \t Loss: 0.6682013273 \t\n",
      "Epoch: 50 \t Batch: 60 \t Loss: 0.6567381620 \t\n",
      "Epoch: 50 \t Batch: 70 \t Loss: 0.6614150405 \t\n",
      "Epoch: 51 \t Batch: 0 \t Loss: 0.6670437455 \t\n",
      "Epoch: 51 \t Batch: 10 \t Loss: 0.6610234976 \t\n",
      "Epoch: 51 \t Batch: 20 \t Loss: 0.6637380719 \t\n",
      "Epoch: 51 \t Batch: 30 \t Loss: 0.6724384427 \t\n",
      "Epoch: 51 \t Batch: 40 \t Loss: 0.6607998610 \t\n",
      "Epoch: 51 \t Batch: 50 \t Loss: 0.6671509147 \t\n",
      "Epoch: 51 \t Batch: 60 \t Loss: 0.6547585726 \t\n",
      "Epoch: 51 \t Batch: 70 \t Loss: 0.6602178812 \t\n",
      "Epoch: 52 \t Batch: 0 \t Loss: 0.6656607389 \t\n",
      "Epoch: 52 \t Batch: 10 \t Loss: 0.6593790650 \t\n",
      "Epoch: 52 \t Batch: 20 \t Loss: 0.6622495055 \t\n",
      "Epoch: 52 \t Batch: 30 \t Loss: 0.6713348627 \t\n",
      "Epoch: 52 \t Batch: 40 \t Loss: 0.6600346565 \t\n",
      "Epoch: 52 \t Batch: 50 \t Loss: 0.6661251783 \t\n",
      "Epoch: 52 \t Batch: 60 \t Loss: 0.6527625322 \t\n",
      "Epoch: 52 \t Batch: 70 \t Loss: 0.6590508223 \t\n",
      "Epoch: 53 \t Batch: 0 \t Loss: 0.6642708778 \t\n",
      "Epoch: 53 \t Batch: 10 \t Loss: 0.6577381492 \t\n",
      "Epoch: 53 \t Batch: 20 \t Loss: 0.6607597470 \t\n",
      "Epoch: 53 \t Batch: 30 \t Loss: 0.6702381968 \t\n",
      "Epoch: 53 \t Batch: 40 \t Loss: 0.6593227983 \t\n",
      "Epoch: 53 \t Batch: 50 \t Loss: 0.6651291847 \t\n",
      "Epoch: 53 \t Batch: 60 \t Loss: 0.6507772207 \t\n",
      "Epoch: 53 \t Batch: 70 \t Loss: 0.6579006910 \t\n",
      "Epoch: 54 \t Batch: 0 \t Loss: 0.6628936529 \t\n",
      "Epoch: 54 \t Batch: 10 \t Loss: 0.6561138034 \t\n",
      "Epoch: 54 \t Batch: 20 \t Loss: 0.6592822671 \t\n",
      "Epoch: 54 \t Batch: 30 \t Loss: 0.6691724062 \t\n",
      "Epoch: 54 \t Batch: 40 \t Loss: 0.6586236954 \t\n",
      "Epoch: 54 \t Batch: 50 \t Loss: 0.6641601920 \t\n",
      "Epoch: 54 \t Batch: 60 \t Loss: 0.6488245726 \t\n",
      "Epoch: 54 \t Batch: 70 \t Loss: 0.6567415595 \t\n",
      "Epoch: 55 \t Batch: 0 \t Loss: 0.6615408659 \t\n",
      "Epoch: 55 \t Batch: 10 \t Loss: 0.6545099020 \t\n",
      "Epoch: 55 \t Batch: 20 \t Loss: 0.6578253508 \t\n",
      "Epoch: 55 \t Batch: 30 \t Loss: 0.6681491137 \t\n",
      "Epoch: 55 \t Batch: 40 \t Loss: 0.6578809023 \t\n",
      "Epoch: 55 \t Batch: 50 \t Loss: 0.6632115245 \t\n",
      "Epoch: 55 \t Batch: 60 \t Loss: 0.6469219327 \t\n",
      "Epoch: 55 \t Batch: 70 \t Loss: 0.6555387974 \t\n",
      "Epoch: 56 \t Batch: 0 \t Loss: 0.6602166295 \t\n",
      "Epoch: 56 \t Batch: 10 \t Loss: 0.6529214978 \t\n",
      "Epoch: 56 \t Batch: 20 \t Loss: 0.6563853025 \t\n",
      "Epoch: 56 \t Batch: 30 \t Loss: 0.6671729684 \t\n",
      "Epoch: 56 \t Batch: 40 \t Loss: 0.6570347548 \t\n",
      "Epoch: 56 \t Batch: 50 \t Loss: 0.6622641683 \t\n",
      "Epoch: 56 \t Batch: 60 \t Loss: 0.6450761557 \t\n",
      "Epoch: 56 \t Batch: 70 \t Loss: 0.6542527080 \t\n",
      "Epoch: 57 \t Batch: 0 \t Loss: 0.6589173079 \t\n",
      "Epoch: 57 \t Batch: 10 \t Loss: 0.6513364911 \t\n",
      "Epoch: 57 \t Batch: 20 \t Loss: 0.6549505591 \t\n",
      "Epoch: 57 \t Batch: 30 \t Loss: 0.6662427187 \t\n",
      "Epoch: 57 \t Batch: 40 \t Loss: 0.6560296416 \t\n",
      "Epoch: 57 \t Batch: 50 \t Loss: 0.6612977982 \t\n",
      "Epoch: 57 \t Batch: 60 \t Loss: 0.6432936788 \t\n",
      "Epoch: 57 \t Batch: 70 \t Loss: 0.6528508067 \t\n",
      "Epoch: 58 \t Batch: 0 \t Loss: 0.6576377153 \t\n",
      "Epoch: 58 \t Batch: 10 \t Loss: 0.6497439146 \t\n",
      "Epoch: 58 \t Batch: 20 \t Loss: 0.6535130143 \t\n",
      "Epoch: 58 \t Batch: 30 \t Loss: 0.6653576493 \t\n",
      "Epoch: 58 \t Batch: 40 \t Loss: 0.6548210979 \t\n",
      "Epoch: 58 \t Batch: 50 \t Loss: 0.6602982283 \t\n",
      "Epoch: 58 \t Batch: 60 \t Loss: 0.6415773630 \t\n",
      "Epoch: 58 \t Batch: 70 \t Loss: 0.6513119936 \t\n",
      "Epoch: 59 \t Batch: 0 \t Loss: 0.6563745141 \t\n",
      "Epoch: 59 \t Batch: 10 \t Loss: 0.6481324434 \t\n",
      "Epoch: 59 \t Batch: 20 \t Loss: 0.6520689726 \t\n",
      "Epoch: 59 \t Batch: 30 \t Loss: 0.6645130515 \t\n",
      "Epoch: 59 \t Batch: 40 \t Loss: 0.6533883214 \t\n",
      "Epoch: 59 \t Batch: 50 \t Loss: 0.6592589021 \t\n",
      "Epoch: 59 \t Batch: 60 \t Loss: 0.6399283409 \t\n",
      "Epoch: 59 \t Batch: 70 \t Loss: 0.6496310234 \t\n",
      "Epoch: 60 \t Batch: 0 \t Loss: 0.6551254988 \t\n",
      "Epoch: 60 \t Batch: 10 \t Loss: 0.6464957595 \t\n",
      "Epoch: 60 \t Batch: 20 \t Loss: 0.6506169438 \t\n",
      "Epoch: 60 \t Batch: 30 \t Loss: 0.6637056470 \t\n",
      "Epoch: 60 \t Batch: 40 \t Loss: 0.6517433524 \t\n",
      "Epoch: 60 \t Batch: 50 \t Loss: 0.6581812501 \t\n",
      "Epoch: 60 \t Batch: 60 \t Loss: 0.6383484006 \t\n",
      "Epoch: 60 \t Batch: 70 \t Loss: 0.6478227377 \t\n",
      "Epoch: 61 \t Batch: 0 \t Loss: 0.6538892388 \t\n",
      "Epoch: 61 \t Batch: 10 \t Loss: 0.6448305845 \t\n",
      "Epoch: 61 \t Batch: 20 \t Loss: 0.6491578817 \t\n",
      "Epoch: 61 \t Batch: 30 \t Loss: 0.6629285812 \t\n",
      "Epoch: 61 \t Batch: 40 \t Loss: 0.6499267817 \t\n",
      "Epoch: 61 \t Batch: 50 \t Loss: 0.6570714712 \t\n",
      "Epoch: 61 \t Batch: 60 \t Loss: 0.6368353367 \t\n",
      "Epoch: 61 \t Batch: 70 \t Loss: 0.6459178925 \t\n",
      "Epoch: 62 \t Batch: 0 \t Loss: 0.6526614428 \t\n",
      "Epoch: 62 \t Batch: 10 \t Loss: 0.6431348324 \t\n",
      "Epoch: 62 \t Batch: 20 \t Loss: 0.6476913691 \t\n",
      "Epoch: 62 \t Batch: 30 \t Loss: 0.6621649265 \t\n",
      "Epoch: 62 \t Batch: 40 \t Loss: 0.6480020285 \t\n",
      "Epoch: 62 \t Batch: 50 \t Loss: 0.6559308767 \t\n",
      "Epoch: 62 \t Batch: 60 \t Loss: 0.6353713870 \t\n",
      "Epoch: 62 \t Batch: 70 \t Loss: 0.6439539790 \t\n",
      "Epoch: 63 \t Batch: 0 \t Loss: 0.6514245272 \t\n",
      "Epoch: 63 \t Batch: 10 \t Loss: 0.6414004564 \t\n",
      "Epoch: 63 \t Batch: 20 \t Loss: 0.6462048292 \t\n",
      "Epoch: 63 \t Batch: 30 \t Loss: 0.6613732576 \t\n",
      "Epoch: 63 \t Batch: 40 \t Loss: 0.6460410357 \t\n",
      "Epoch: 63 \t Batch: 50 \t Loss: 0.6547527909 \t\n",
      "Epoch: 63 \t Batch: 60 \t Loss: 0.6339195371 \t\n",
      "Epoch: 63 \t Batch: 70 \t Loss: 0.6419633031 \t\n",
      "Epoch: 64 \t Batch: 0 \t Loss: 0.6501443386 \t\n",
      "Epoch: 64 \t Batch: 10 \t Loss: 0.6396027803 \t\n",
      "Epoch: 64 \t Batch: 20 \t Loss: 0.6446666718 \t\n",
      "Epoch: 64 \t Batch: 30 \t Loss: 0.6604923010 \t\n",
      "Epoch: 64 \t Batch: 40 \t Loss: 0.6441135406 \t\n",
      "Epoch: 64 \t Batch: 50 \t Loss: 0.6535097361 \t\n",
      "Epoch: 64 \t Batch: 60 \t Loss: 0.6324167848 \t\n",
      "Epoch: 64 \t Batch: 70 \t Loss: 0.6399633288 \t\n",
      "Epoch: 65 \t Batch: 0 \t Loss: 0.6487643719 \t\n",
      "Epoch: 65 \t Batch: 10 \t Loss: 0.6376989484 \t\n",
      "Epoch: 65 \t Batch: 20 \t Loss: 0.6430228949 \t\n",
      "Epoch: 65 \t Batch: 30 \t Loss: 0.6594330668 \t\n",
      "Epoch: 65 \t Batch: 40 \t Loss: 0.6422699690 \t\n",
      "Epoch: 65 \t Batch: 50 \t Loss: 0.6521558762 \t\n",
      "Epoch: 65 \t Batch: 60 \t Loss: 0.6307840347 \t\n",
      "Epoch: 65 \t Batch: 70 \t Loss: 0.6379572153 \t\n",
      "Epoch: 66 \t Batch: 0 \t Loss: 0.6472140551 \t\n",
      "Epoch: 66 \t Batch: 10 \t Loss: 0.6356331110 \t\n",
      "Epoch: 66 \t Batch: 20 \t Loss: 0.6412084699 \t\n",
      "Epoch: 66 \t Batch: 30 \t Loss: 0.6581015587 \t\n",
      "Epoch: 66 \t Batch: 40 \t Loss: 0.6405402422 \t\n",
      "Epoch: 66 \t Batch: 50 \t Loss: 0.6506350636 \t\n",
      "Epoch: 66 \t Batch: 60 \t Loss: 0.6289411783 \t\n",
      "Epoch: 66 \t Batch: 70 \t Loss: 0.6359334588 \t\n",
      "Epoch: 67 \t Batch: 0 \t Loss: 0.6454212070 \t\n",
      "Epoch: 67 \t Batch: 10 \t Loss: 0.6333495378 \t\n",
      "Epoch: 67 \t Batch: 20 \t Loss: 0.6391640306 \t\n",
      "Epoch: 67 \t Batch: 30 \t Loss: 0.6564093828 \t\n",
      "Epoch: 67 \t Batch: 40 \t Loss: 0.6389339566 \t\n",
      "Epoch: 67 \t Batch: 50 \t Loss: 0.6488975286 \t\n",
      "Epoch: 67 \t Batch: 60 \t Loss: 0.6268281937 \t\n",
      "Epoch: 67 \t Batch: 70 \t Loss: 0.6338805556 \t\n",
      "Epoch: 68 \t Batch: 0 \t Loss: 0.6433359385 \t\n",
      "Epoch: 68 \t Batch: 10 \t Loss: 0.6308122277 \t\n",
      "Epoch: 68 \t Batch: 20 \t Loss: 0.6368497610 \t\n",
      "Epoch: 68 \t Batch: 30 \t Loss: 0.6543061733 \t\n",
      "Epoch: 68 \t Batch: 40 \t Loss: 0.6374468207 \t\n",
      "Epoch: 68 \t Batch: 50 \t Loss: 0.6469116211 \t\n",
      "Epoch: 68 \t Batch: 60 \t Loss: 0.6244252324 \t\n",
      "Epoch: 68 \t Batch: 70 \t Loss: 0.6317895651 \t\n",
      "Epoch: 69 \t Batch: 0 \t Loss: 0.6409451365 \t\n",
      "Epoch: 69 \t Batch: 10 \t Loss: 0.6280155182 \t\n",
      "Epoch: 69 \t Batch: 20 \t Loss: 0.6342607737 \t\n",
      "Epoch: 69 \t Batch: 30 \t Loss: 0.6517890096 \t\n",
      "Epoch: 69 \t Batch: 40 \t Loss: 0.6360586882 \t\n",
      "Epoch: 69 \t Batch: 50 \t Loss: 0.6446744800 \t\n",
      "Epoch: 69 \t Batch: 60 \t Loss: 0.6217623949 \t\n",
      "Epoch: 69 \t Batch: 70 \t Loss: 0.6296564341 \t\n",
      "Epoch: 70 \t Batch: 0 \t Loss: 0.6382743120 \t\n",
      "Epoch: 70 \t Batch: 10 \t Loss: 0.6249797940 \t\n",
      "Epoch: 70 \t Batch: 20 \t Loss: 0.6314271688 \t\n",
      "Epoch: 70 \t Batch: 30 \t Loss: 0.6489051580 \t\n",
      "Epoch: 70 \t Batch: 40 \t Loss: 0.6347261667 \t\n",
      "Epoch: 70 \t Batch: 50 \t Loss: 0.6422058344 \t\n",
      "Epoch: 70 \t Batch: 60 \t Loss: 0.6189072132 \t\n",
      "Epoch: 70 \t Batch: 70 \t Loss: 0.6274663806 \t\n",
      "Epoch: 71 \t Batch: 0 \t Loss: 0.6353788972 \t\n",
      "Epoch: 71 \t Batch: 10 \t Loss: 0.6217430234 \t\n",
      "Epoch: 71 \t Batch: 20 \t Loss: 0.6283925176 \t\n",
      "Epoch: 71 \t Batch: 30 \t Loss: 0.6457196474 \t\n",
      "Epoch: 71 \t Batch: 40 \t Loss: 0.6333589554 \t\n",
      "Epoch: 71 \t Batch: 50 \t Loss: 0.6395223141 \t\n",
      "Epoch: 71 \t Batch: 60 \t Loss: 0.6159363985 \t\n",
      "Epoch: 71 \t Batch: 70 \t Loss: 0.6251753569 \t\n",
      "Epoch: 72 \t Batch: 0 \t Loss: 0.6323096156 \t\n",
      "Epoch: 72 \t Batch: 10 \t Loss: 0.6183315516 \t\n",
      "Epoch: 72 \t Batch: 20 \t Loss: 0.6252011657 \t\n",
      "Epoch: 72 \t Batch: 30 \t Loss: 0.6422955394 \t\n",
      "Epoch: 72 \t Batch: 40 \t Loss: 0.6318292618 \t\n",
      "Epoch: 72 \t Batch: 50 \t Loss: 0.6366173625 \t\n",
      "Epoch: 72 \t Batch: 60 \t Loss: 0.6129166484 \t\n",
      "Epoch: 72 \t Batch: 70 \t Loss: 0.6226941943 \t\n",
      "Epoch: 73 \t Batch: 0 \t Loss: 0.6290915608 \t\n",
      "Epoch: 73 \t Batch: 10 \t Loss: 0.6147406101 \t\n",
      "Epoch: 73 \t Batch: 20 \t Loss: 0.6218654513 \t\n",
      "Epoch: 73 \t Batch: 30 \t Loss: 0.6386867762 \t\n",
      "Epoch: 73 \t Batch: 40 \t Loss: 0.6299661994 \t\n",
      "Epoch: 73 \t Batch: 50 \t Loss: 0.6334545016 \t\n",
      "Epoch: 73 \t Batch: 60 \t Loss: 0.6098864079 \t\n",
      "Epoch: 73 \t Batch: 70 \t Loss: 0.6199104786 \t\n",
      "Epoch: 74 \t Batch: 0 \t Loss: 0.6257282495 \t\n",
      "Epoch: 74 \t Batch: 10 \t Loss: 0.6109422445 \t\n",
      "Epoch: 74 \t Batch: 20 \t Loss: 0.6183679104 \t\n",
      "Epoch: 74 \t Batch: 30 \t Loss: 0.6349124908 \t\n",
      "Epoch: 74 \t Batch: 40 \t Loss: 0.6276050806 \t\n",
      "Epoch: 74 \t Batch: 50 \t Loss: 0.6299722791 \t\n",
      "Epoch: 74 \t Batch: 60 \t Loss: 0.6068574190 \t\n",
      "Epoch: 74 \t Batch: 70 \t Loss: 0.6167107224 \t\n",
      "Epoch: 75 \t Batch: 0 \t Loss: 0.6221960783 \t\n",
      "Epoch: 75 \t Batch: 10 \t Loss: 0.6068972349 \t\n",
      "Epoch: 75 \t Batch: 20 \t Loss: 0.6146965623 \t\n",
      "Epoch: 75 \t Batch: 30 \t Loss: 0.6309736967 \t\n",
      "Epoch: 75 \t Batch: 40 \t Loss: 0.6246434450 \t\n",
      "Epoch: 75 \t Batch: 50 \t Loss: 0.6261399388 \t\n",
      "Epoch: 75 \t Batch: 60 \t Loss: 0.6038586497 \t\n",
      "Epoch: 75 \t Batch: 70 \t Loss: 0.6130325198 \t\n",
      "Epoch: 76 \t Batch: 0 \t Loss: 0.6184854507 \t\n",
      "Epoch: 76 \t Batch: 10 \t Loss: 0.6026042700 \t\n",
      "Epoch: 76 \t Batch: 20 \t Loss: 0.6108735204 \t\n",
      "Epoch: 76 \t Batch: 30 \t Loss: 0.6268900633 \t\n",
      "Epoch: 76 \t Batch: 40 \t Loss: 0.6210955381 \t\n",
      "Epoch: 76 \t Batch: 50 \t Loss: 0.6219716072 \t\n",
      "Epoch: 76 \t Batch: 60 \t Loss: 0.6009337902 \t\n",
      "Epoch: 76 \t Batch: 70 \t Loss: 0.6089022160 \t\n",
      "Epoch: 77 \t Batch: 0 \t Loss: 0.6146209240 \t\n",
      "Epoch: 77 \t Batch: 10 \t Loss: 0.5981156826 \t\n",
      "Epoch: 77 \t Batch: 20 \t Loss: 0.6069496870 \t\n",
      "Epoch: 77 \t Batch: 30 \t Loss: 0.6227127314 \t\n",
      "Epoch: 77 \t Batch: 40 \t Loss: 0.6170915365 \t\n",
      "Epoch: 77 \t Batch: 50 \t Loss: 0.6175452471 \t\n",
      "Epoch: 77 \t Batch: 60 \t Loss: 0.5981322527 \t\n",
      "Epoch: 77 \t Batch: 70 \t Loss: 0.6044440269 \t\n",
      "Epoch: 78 \t Batch: 0 \t Loss: 0.6106607318 \t\n",
      "Epoch: 78 \t Batch: 10 \t Loss: 0.5935310125 \t\n",
      "Epoch: 78 \t Batch: 20 \t Loss: 0.6030101180 \t\n",
      "Epoch: 78 \t Batch: 30 \t Loss: 0.6184929609 \t\n",
      "Epoch: 78 \t Batch: 40 \t Loss: 0.6128717065 \t\n",
      "Epoch: 78 \t Batch: 50 \t Loss: 0.6129603982 \t\n",
      "Epoch: 78 \t Batch: 60 \t Loss: 0.5954815745 \t\n",
      "Epoch: 78 \t Batch: 70 \t Loss: 0.5998330712 \t\n",
      "Epoch: 79 \t Batch: 0 \t Loss: 0.6066590548 \t\n",
      "Epoch: 79 \t Batch: 10 \t Loss: 0.5889322162 \t\n",
      "Epoch: 79 \t Batch: 20 \t Loss: 0.5991218686 \t\n",
      "Epoch: 79 \t Batch: 30 \t Loss: 0.6142445803 \t\n",
      "Epoch: 79 \t Batch: 40 \t Loss: 0.6087118387 \t\n",
      "Epoch: 79 \t Batch: 50 \t Loss: 0.6083107591 \t\n",
      "Epoch: 79 \t Batch: 60 \t Loss: 0.5929702520 \t\n",
      "Epoch: 79 \t Batch: 70 \t Loss: 0.5952333212 \t\n",
      "Epoch: 80 \t Batch: 0 \t Loss: 0.6026259661 \t\n",
      "Epoch: 80 \t Batch: 10 \t Loss: 0.5843670368 \t\n",
      "Epoch: 80 \t Batch: 20 \t Loss: 0.5952945948 \t\n",
      "Epoch: 80 \t Batch: 30 \t Loss: 0.6099065542 \t\n",
      "Epoch: 80 \t Batch: 40 \t Loss: 0.6048483849 \t\n",
      "Epoch: 80 \t Batch: 50 \t Loss: 0.6036436558 \t\n",
      "Epoch: 80 \t Batch: 60 \t Loss: 0.5904880762 \t\n",
      "Epoch: 80 \t Batch: 70 \t Loss: 0.5907692909 \t\n",
      "Epoch: 81 \t Batch: 0 \t Loss: 0.5985181928 \t\n",
      "Epoch: 81 \t Batch: 10 \t Loss: 0.5798267126 \t\n",
      "Epoch: 81 \t Batch: 20 \t Loss: 0.5914816856 \t\n",
      "Epoch: 81 \t Batch: 30 \t Loss: 0.6053655744 \t\n",
      "Epoch: 81 \t Batch: 40 \t Loss: 0.6014369726 \t\n",
      "Epoch: 81 \t Batch: 50 \t Loss: 0.5989598632 \t\n",
      "Epoch: 81 \t Batch: 60 \t Loss: 0.5879317522 \t\n",
      "Epoch: 81 \t Batch: 70 \t Loss: 0.5865081549 \t\n",
      "Epoch: 82 \t Batch: 0 \t Loss: 0.5942703485 \t\n",
      "Epoch: 82 \t Batch: 10 \t Loss: 0.5752707124 \t\n",
      "Epoch: 82 \t Batch: 20 \t Loss: 0.5876139998 \t\n",
      "Epoch: 82 \t Batch: 30 \t Loss: 0.6005226374 \t\n",
      "Epoch: 82 \t Batch: 40 \t Loss: 0.5985708237 \t\n",
      "Epoch: 82 \t Batch: 50 \t Loss: 0.5942633748 \t\n",
      "Epoch: 82 \t Batch: 60 \t Loss: 0.5852417946 \t\n",
      "Epoch: 82 \t Batch: 70 \t Loss: 0.5825297236 \t\n",
      "Epoch: 83 \t Batch: 0 \t Loss: 0.5898890495 \t\n",
      "Epoch: 83 \t Batch: 10 \t Loss: 0.5706918836 \t\n",
      "Epoch: 83 \t Batch: 20 \t Loss: 0.5837030411 \t\n",
      "Epoch: 83 \t Batch: 30 \t Loss: 0.5954087377 \t\n",
      "Epoch: 83 \t Batch: 40 \t Loss: 0.5963097811 \t\n",
      "Epoch: 83 \t Batch: 50 \t Loss: 0.5896527767 \t\n",
      "Epoch: 83 \t Batch: 60 \t Loss: 0.5824688673 \t\n",
      "Epoch: 83 \t Batch: 70 \t Loss: 0.5789157152 \t\n",
      "Epoch: 84 \t Batch: 0 \t Loss: 0.5854870081 \t\n",
      "Epoch: 84 \t Batch: 10 \t Loss: 0.5661829114 \t\n",
      "Epoch: 84 \t Batch: 20 \t Loss: 0.5798654556 \t\n",
      "Epoch: 84 \t Batch: 30 \t Loss: 0.5901792049 \t\n",
      "Epoch: 84 \t Batch: 40 \t Loss: 0.5946790576 \t\n",
      "Epoch: 84 \t Batch: 50 \t Loss: 0.5852615833 \t\n",
      "Epoch: 84 \t Batch: 60 \t Loss: 0.5798040628 \t\n",
      "Epoch: 84 \t Batch: 70 \t Loss: 0.5757361650 \t\n",
      "Epoch: 85 \t Batch: 0 \t Loss: 0.5812497139 \t\n",
      "Epoch: 85 \t Batch: 10 \t Loss: 0.5619067550 \t\n",
      "Epoch: 85 \t Batch: 20 \t Loss: 0.5762695074 \t\n",
      "Epoch: 85 \t Batch: 30 \t Loss: 0.5850791335 \t\n",
      "Epoch: 85 \t Batch: 40 \t Loss: 0.5936199427 \t\n",
      "Epoch: 85 \t Batch: 50 \t Loss: 0.5812218785 \t\n",
      "Epoch: 85 \t Batch: 60 \t Loss: 0.5774670839 \t\n",
      "Epoch: 85 \t Batch: 70 \t Loss: 0.5729626417 \t\n",
      "Epoch: 86 \t Batch: 0 \t Loss: 0.5773353577 \t\n",
      "Epoch: 86 \t Batch: 10 \t Loss: 0.5580034256 \t\n",
      "Epoch: 86 \t Batch: 20 \t Loss: 0.5730508566 \t\n",
      "Epoch: 86 \t Batch: 30 \t Loss: 0.5803344846 \t\n",
      "Epoch: 86 \t Batch: 40 \t Loss: 0.5929462910 \t\n",
      "Epoch: 86 \t Batch: 50 \t Loss: 0.5775817633 \t\n",
      "Epoch: 86 \t Batch: 60 \t Loss: 0.5756201148 \t\n",
      "Epoch: 86 \t Batch: 70 \t Loss: 0.5703899860 \t\n",
      "Epoch: 87 \t Batch: 0 \t Loss: 0.5737699270 \t\n",
      "Epoch: 87 \t Batch: 10 \t Loss: 0.5545055270 \t\n",
      "Epoch: 87 \t Batch: 20 \t Loss: 0.5702344179 \t\n",
      "Epoch: 87 \t Batch: 30 \t Loss: 0.5760497451 \t\n",
      "Epoch: 87 \t Batch: 40 \t Loss: 0.5923375487 \t\n",
      "Epoch: 87 \t Batch: 50 \t Loss: 0.5742325783 \t\n",
      "Epoch: 87 \t Batch: 60 \t Loss: 0.5742877126 \t\n",
      "Epoch: 87 \t Batch: 70 \t Loss: 0.5676965117 \t\n",
      "Epoch: 88 \t Batch: 0 \t Loss: 0.5704419017 \t\n",
      "Epoch: 88 \t Batch: 10 \t Loss: 0.5513380766 \t\n",
      "Epoch: 88 \t Batch: 20 \t Loss: 0.5677418709 \t\n",
      "Epoch: 88 \t Batch: 30 \t Loss: 0.5721983910 \t\n",
      "Epoch: 88 \t Batch: 40 \t Loss: 0.5914682150 \t\n",
      "Epoch: 88 \t Batch: 50 \t Loss: 0.5710077882 \t\n",
      "Epoch: 88 \t Batch: 60 \t Loss: 0.5734032393 \t\n",
      "Epoch: 88 \t Batch: 70 \t Loss: 0.5645912886 \t\n",
      "Epoch: 89 \t Batch: 0 \t Loss: 0.5672042370 \t\n",
      "Epoch: 89 \t Batch: 10 \t Loss: 0.5484459996 \t\n",
      "Epoch: 89 \t Batch: 20 \t Loss: 0.5654791594 \t\n",
      "Epoch: 89 \t Batch: 30 \t Loss: 0.5687252879 \t\n",
      "Epoch: 89 \t Batch: 40 \t Loss: 0.5901777148 \t\n",
      "Epoch: 89 \t Batch: 50 \t Loss: 0.5678266287 \t\n",
      "Epoch: 89 \t Batch: 60 \t Loss: 0.5729034543 \t\n",
      "Epoch: 89 \t Batch: 70 \t Loss: 0.5609664321 \t\n",
      "Epoch: 90 \t Batch: 0 \t Loss: 0.5639679432 \t\n",
      "Epoch: 90 \t Batch: 10 \t Loss: 0.5458184481 \t\n",
      "Epoch: 90 \t Batch: 20 \t Loss: 0.5634085536 \t\n",
      "Epoch: 90 \t Batch: 30 \t Loss: 0.5655938983 \t\n",
      "Epoch: 90 \t Batch: 40 \t Loss: 0.5885339975 \t\n",
      "Epoch: 90 \t Batch: 50 \t Loss: 0.5647401214 \t\n",
      "Epoch: 90 \t Batch: 60 \t Loss: 0.5727130771 \t\n",
      "Epoch: 90 \t Batch: 70 \t Loss: 0.5569371581 \t\n",
      "Epoch: 91 \t Batch: 0 \t Loss: 0.5607298613 \t\n",
      "Epoch: 91 \t Batch: 10 \t Loss: 0.5434725285 \t\n",
      "Epoch: 91 \t Batch: 20 \t Loss: 0.5615274906 \t\n",
      "Epoch: 91 \t Batch: 30 \t Loss: 0.5627557635 \t\n",
      "Epoch: 91 \t Batch: 40 \t Loss: 0.5867514014 \t\n",
      "Epoch: 91 \t Batch: 50 \t Loss: 0.5618366599 \t\n",
      "Epoch: 91 \t Batch: 60 \t Loss: 0.5726591349 \t\n",
      "Epoch: 91 \t Batch: 70 \t Loss: 0.5527315140 \t\n",
      "Epoch: 92 \t Batch: 0 \t Loss: 0.5574740171 \t\n",
      "Epoch: 92 \t Batch: 10 \t Loss: 0.5413168669 \t\n",
      "Epoch: 92 \t Batch: 20 \t Loss: 0.5597199202 \t\n",
      "Epoch: 92 \t Batch: 30 \t Loss: 0.5600122213 \t\n",
      "Epoch: 92 \t Batch: 40 \t Loss: 0.5850194097 \t\n",
      "Epoch: 92 \t Batch: 50 \t Loss: 0.5590984225 \t\n",
      "Epoch: 92 \t Batch: 60 \t Loss: 0.5723617077 \t\n",
      "Epoch: 92 \t Batch: 70 \t Loss: 0.5484925508 \t\n",
      "Epoch: 93 \t Batch: 0 \t Loss: 0.5540503263 \t\n",
      "Epoch: 93 \t Batch: 10 \t Loss: 0.5390629768 \t\n",
      "Epoch: 93 \t Batch: 20 \t Loss: 0.5576851964 \t\n",
      "Epoch: 93 \t Batch: 30 \t Loss: 0.5569740534 \t\n",
      "Epoch: 93 \t Batch: 40 \t Loss: 0.5833450556 \t\n",
      "Epoch: 93 \t Batch: 50 \t Loss: 0.5563362837 \t\n",
      "Epoch: 93 \t Batch: 60 \t Loss: 0.5712988973 \t\n",
      "Epoch: 93 \t Batch: 70 \t Loss: 0.5442261696 \t\n",
      "Epoch: 94 \t Batch: 0 \t Loss: 0.5502046347 \t\n",
      "Epoch: 94 \t Batch: 10 \t Loss: 0.5362902284 \t\n",
      "Epoch: 94 \t Batch: 20 \t Loss: 0.5550454855 \t\n",
      "Epoch: 94 \t Batch: 30 \t Loss: 0.5532312989 \t\n",
      "Epoch: 94 \t Batch: 40 \t Loss: 0.5816143751 \t\n",
      "Epoch: 94 \t Batch: 50 \t Loss: 0.5533285141 \t\n",
      "Epoch: 94 \t Batch: 60 \t Loss: 0.5690142512 \t\n",
      "Epoch: 94 \t Batch: 70 \t Loss: 0.5398751497 \t\n",
      "Epoch: 95 \t Batch: 0 \t Loss: 0.5457489491 \t\n",
      "Epoch: 95 \t Batch: 10 \t Loss: 0.5326820612 \t\n",
      "Epoch: 95 \t Batch: 20 \t Loss: 0.5515708923 \t\n",
      "Epoch: 95 \t Batch: 30 \t Loss: 0.5485826135 \t\n",
      "Epoch: 95 \t Batch: 40 \t Loss: 0.5797126293 \t\n",
      "Epoch: 95 \t Batch: 50 \t Loss: 0.5499879718 \t\n",
      "Epoch: 95 \t Batch: 60 \t Loss: 0.5653814077 \t\n",
      "Epoch: 95 \t Batch: 70 \t Loss: 0.5353855491 \t\n",
      "Epoch: 96 \t Batch: 0 \t Loss: 0.5406819582 \t\n",
      "Epoch: 96 \t Batch: 10 \t Loss: 0.5282340646 \t\n",
      "Epoch: 96 \t Batch: 20 \t Loss: 0.5473254919 \t\n",
      "Epoch: 96 \t Batch: 30 \t Loss: 0.5431703925 \t\n",
      "Epoch: 96 \t Batch: 40 \t Loss: 0.5775429010 \t\n",
      "Epoch: 96 \t Batch: 50 \t Loss: 0.5463838577 \t\n",
      "Epoch: 96 \t Batch: 60 \t Loss: 0.5606210828 \t\n",
      "Epoch: 96 \t Batch: 70 \t Loss: 0.5306901336 \t\n",
      "Epoch: 97 \t Batch: 0 \t Loss: 0.5351606607 \t\n",
      "Epoch: 97 \t Batch: 10 \t Loss: 0.5231601000 \t\n",
      "Epoch: 97 \t Batch: 20 \t Loss: 0.5425146818 \t\n",
      "Epoch: 97 \t Batch: 30 \t Loss: 0.5373159647 \t\n",
      "Epoch: 97 \t Batch: 40 \t Loss: 0.5748648643 \t\n",
      "Epoch: 97 \t Batch: 50 \t Loss: 0.5425719023 \t\n",
      "Epoch: 97 \t Batch: 60 \t Loss: 0.5550447702 \t\n",
      "Epoch: 97 \t Batch: 70 \t Loss: 0.5255070925 \t\n",
      "Epoch: 98 \t Batch: 0 \t Loss: 0.5292606950 \t\n",
      "Epoch: 98 \t Batch: 10 \t Loss: 0.5176419616 \t\n",
      "Epoch: 98 \t Batch: 20 \t Loss: 0.5372838378 \t\n",
      "Epoch: 98 \t Batch: 30 \t Loss: 0.5312187672 \t\n",
      "Epoch: 98 \t Batch: 40 \t Loss: 0.5712893605 \t\n",
      "Epoch: 98 \t Batch: 50 \t Loss: 0.5384333730 \t\n",
      "Epoch: 98 \t Batch: 60 \t Loss: 0.5488635898 \t\n",
      "Epoch: 98 \t Batch: 70 \t Loss: 0.5193339586 \t\n",
      "Epoch: 99 \t Batch: 0 \t Loss: 0.5228345990 \t\n",
      "Epoch: 99 \t Batch: 10 \t Loss: 0.5117463470 \t\n",
      "Epoch: 99 \t Batch: 20 \t Loss: 0.5315730572 \t\n",
      "Epoch: 99 \t Batch: 30 \t Loss: 0.5249353647 \t\n",
      "Epoch: 99 \t Batch: 40 \t Loss: 0.5663447976 \t\n",
      "Epoch: 99 \t Batch: 50 \t Loss: 0.5338380933 \t\n",
      "Epoch: 99 \t Batch: 60 \t Loss: 0.5422132611 \t\n",
      "Epoch: 99 \t Batch: 70 \t Loss: 0.5118491054 \t\n",
      "Epoch: 100 \t Batch: 0 \t Loss: 0.5158039331 \t\n",
      "Epoch: 100 \t Batch: 10 \t Loss: 0.5055147409 \t\n",
      "Epoch: 100 \t Batch: 20 \t Loss: 0.5254082680 \t\n",
      "Epoch: 100 \t Batch: 30 \t Loss: 0.5185863972 \t\n",
      "Epoch: 100 \t Batch: 40 \t Loss: 0.5599865317 \t\n",
      "Epoch: 100 \t Batch: 50 \t Loss: 0.5288986564 \t\n",
      "Epoch: 100 \t Batch: 60 \t Loss: 0.5352617502 \t\n",
      "Epoch: 100 \t Batch: 70 \t Loss: 0.5032175779 \t\n",
      "Epoch: 101 \t Batch: 0 \t Loss: 0.5083619356 \t\n",
      "Epoch: 101 \t Batch: 10 \t Loss: 0.4991602898 \t\n",
      "Epoch: 101 \t Batch: 20 \t Loss: 0.5191106796 \t\n",
      "Epoch: 101 \t Batch: 30 \t Loss: 0.5124406219 \t\n",
      "Epoch: 101 \t Batch: 40 \t Loss: 0.5527262688 \t\n",
      "Epoch: 101 \t Batch: 50 \t Loss: 0.5241140127 \t\n",
      "Epoch: 101 \t Batch: 60 \t Loss: 0.5282814503 \t\n",
      "Epoch: 101 \t Batch: 70 \t Loss: 0.4941926897 \t\n",
      "Epoch: 102 \t Batch: 0 \t Loss: 0.5010110140 \t\n",
      "Epoch: 102 \t Batch: 10 \t Loss: 0.4930570722 \t\n",
      "Epoch: 102 \t Batch: 20 \t Loss: 0.5130816102 \t\n",
      "Epoch: 102 \t Batch: 30 \t Loss: 0.5068244338 \t\n",
      "Epoch: 102 \t Batch: 40 \t Loss: 0.5454214811 \t\n",
      "Epoch: 102 \t Batch: 50 \t Loss: 0.5200472474 \t\n",
      "Epoch: 102 \t Batch: 60 \t Loss: 0.5213664770 \t\n",
      "Epoch: 102 \t Batch: 70 \t Loss: 0.4856308997 \t\n",
      "Epoch: 103 \t Batch: 0 \t Loss: 0.4941692948 \t\n",
      "Epoch: 103 \t Batch: 10 \t Loss: 0.4873574674 \t\n",
      "Epoch: 103 \t Batch: 20 \t Loss: 0.5075312853 \t\n",
      "Epoch: 103 \t Batch: 30 \t Loss: 0.5016695261 \t\n",
      "Epoch: 103 \t Batch: 40 \t Loss: 0.5388240218 \t\n",
      "Epoch: 103 \t Batch: 50 \t Loss: 0.5169471502 \t\n",
      "Epoch: 103 \t Batch: 60 \t Loss: 0.5143290758 \t\n",
      "Epoch: 103 \t Batch: 70 \t Loss: 0.4781185091 \t\n",
      "Epoch: 104 \t Batch: 0 \t Loss: 0.4879720807 \t\n",
      "Epoch: 104 \t Batch: 10 \t Loss: 0.4819026887 \t\n",
      "Epoch: 104 \t Batch: 20 \t Loss: 0.5024044514 \t\n",
      "Epoch: 104 \t Batch: 30 \t Loss: 0.4967299104 \t\n",
      "Epoch: 104 \t Batch: 40 \t Loss: 0.5333823562 \t\n",
      "Epoch: 104 \t Batch: 50 \t Loss: 0.5147750378 \t\n",
      "Epoch: 104 \t Batch: 60 \t Loss: 0.5069333315 \t\n",
      "Epoch: 104 \t Batch: 70 \t Loss: 0.4720064104 \t\n",
      "Epoch: 105 \t Batch: 0 \t Loss: 0.4824942052 \t\n",
      "Epoch: 105 \t Batch: 10 \t Loss: 0.4766714573 \t\n",
      "Epoch: 105 \t Batch: 20 \t Loss: 0.4977210164 \t\n",
      "Epoch: 105 \t Batch: 30 \t Loss: 0.4919823706 \t\n",
      "Epoch: 105 \t Batch: 40 \t Loss: 0.5294245481 \t\n",
      "Epoch: 105 \t Batch: 50 \t Loss: 0.5136167407 \t\n",
      "Epoch: 105 \t Batch: 60 \t Loss: 0.4994244576 \t\n",
      "Epoch: 105 \t Batch: 70 \t Loss: 0.4674681127 \t\n",
      "Epoch: 106 \t Batch: 0 \t Loss: 0.4779632092 \t\n",
      "Epoch: 106 \t Batch: 10 \t Loss: 0.4720358253 \t\n",
      "Epoch: 106 \t Batch: 20 \t Loss: 0.4938728213 \t\n",
      "Epoch: 106 \t Batch: 30 \t Loss: 0.4878293872 \t\n",
      "Epoch: 106 \t Batch: 40 \t Loss: 0.5271276236 \t\n",
      "Epoch: 106 \t Batch: 50 \t Loss: 0.5137380362 \t\n",
      "Epoch: 106 \t Batch: 60 \t Loss: 0.4925664961 \t\n",
      "Epoch: 106 \t Batch: 70 \t Loss: 0.4645423293 \t\n",
      "Epoch: 107 \t Batch: 0 \t Loss: 0.4747486711 \t\n",
      "Epoch: 107 \t Batch: 10 \t Loss: 0.4685252607 \t\n",
      "Epoch: 107 \t Batch: 20 \t Loss: 0.4913098216 \t\n",
      "Epoch: 107 \t Batch: 30 \t Loss: 0.4848004282 \t\n",
      "Epoch: 107 \t Batch: 40 \t Loss: 0.5261945128 \t\n",
      "Epoch: 107 \t Batch: 50 \t Loss: 0.5152829289 \t\n",
      "Epoch: 107 \t Batch: 60 \t Loss: 0.4870695174 \t\n",
      "Epoch: 107 \t Batch: 70 \t Loss: 0.4627389908 \t\n",
      "Epoch: 108 \t Batch: 0 \t Loss: 0.4728274345 \t\n",
      "Epoch: 108 \t Batch: 10 \t Loss: 0.4663403034 \t\n",
      "Epoch: 108 \t Batch: 20 \t Loss: 0.4900305271 \t\n",
      "Epoch: 108 \t Batch: 30 \t Loss: 0.4830537736 \t\n",
      "Epoch: 108 \t Batch: 40 \t Loss: 0.5258013010 \t\n",
      "Epoch: 108 \t Batch: 50 \t Loss: 0.5179354548 \t\n",
      "Epoch: 108 \t Batch: 60 \t Loss: 0.4831235409 \t\n",
      "Epoch: 108 \t Batch: 70 \t Loss: 0.4611899853 \t\n",
      "Epoch: 109 \t Batch: 0 \t Loss: 0.4717559218 \t\n",
      "Epoch: 109 \t Batch: 10 \t Loss: 0.4651891589 \t\n",
      "Epoch: 109 \t Batch: 20 \t Loss: 0.4896340966 \t\n",
      "Epoch: 109 \t Batch: 30 \t Loss: 0.4822996259 \t\n",
      "Epoch: 109 \t Batch: 40 \t Loss: 0.5249722600 \t\n",
      "Epoch: 109 \t Batch: 50 \t Loss: 0.5212033391 \t\n",
      "Epoch: 109 \t Batch: 60 \t Loss: 0.4805676341 \t\n",
      "Epoch: 109 \t Batch: 70 \t Loss: 0.4592804015 \t\n",
      "Epoch: 110 \t Batch: 0 \t Loss: 0.4710806012 \t\n",
      "Epoch: 110 \t Batch: 10 \t Loss: 0.4646919668 \t\n",
      "Epoch: 110 \t Batch: 20 \t Loss: 0.4897785187 \t\n",
      "Epoch: 110 \t Batch: 30 \t Loss: 0.4822999537 \t\n",
      "Epoch: 110 \t Batch: 40 \t Loss: 0.5234197378 \t\n",
      "Epoch: 110 \t Batch: 50 \t Loss: 0.5248816609 \t\n",
      "Epoch: 110 \t Batch: 60 \t Loss: 0.4792306423 \t\n",
      "Epoch: 110 \t Batch: 70 \t Loss: 0.4571250379 \t\n",
      "Epoch: 111 \t Batch: 0 \t Loss: 0.4707191885 \t\n",
      "Epoch: 111 \t Batch: 10 \t Loss: 0.4647001624 \t\n",
      "Epoch: 111 \t Batch: 20 \t Loss: 0.4903624654 \t\n",
      "Epoch: 111 \t Batch: 30 \t Loss: 0.4828323722 \t\n",
      "Epoch: 111 \t Batch: 40 \t Loss: 0.5215885043 \t\n",
      "Epoch: 111 \t Batch: 50 \t Loss: 0.5289206505 \t\n",
      "Epoch: 111 \t Batch: 60 \t Loss: 0.4787198007 \t\n",
      "Epoch: 111 \t Batch: 70 \t Loss: 0.4551807344 \t\n",
      "Epoch: 112 \t Batch: 0 \t Loss: 0.4706487060 \t\n",
      "Epoch: 112 \t Batch: 10 \t Loss: 0.4649709761 \t\n",
      "Epoch: 112 \t Batch: 20 \t Loss: 0.4911921918 \t\n",
      "Epoch: 112 \t Batch: 30 \t Loss: 0.4832657874 \t\n",
      "Epoch: 112 \t Batch: 40 \t Loss: 0.5199562311 \t\n",
      "Epoch: 112 \t Batch: 50 \t Loss: 0.5329211354 \t\n",
      "Epoch: 112 \t Batch: 60 \t Loss: 0.4782217443 \t\n",
      "Epoch: 112 \t Batch: 70 \t Loss: 0.4535770416 \t\n",
      "Epoch: 113 \t Batch: 0 \t Loss: 0.4704092443 \t\n",
      "Epoch: 113 \t Batch: 10 \t Loss: 0.4648618698 \t\n",
      "Epoch: 113 \t Batch: 20 \t Loss: 0.4916491508 \t\n",
      "Epoch: 113 \t Batch: 30 \t Loss: 0.4825495183 \t\n",
      "Epoch: 113 \t Batch: 40 \t Loss: 0.5185597539 \t\n",
      "Epoch: 113 \t Batch: 50 \t Loss: 0.5361007452 \t\n",
      "Epoch: 113 \t Batch: 60 \t Loss: 0.4767292440 \t\n",
      "Epoch: 113 \t Batch: 70 \t Loss: 0.4520531297 \t\n",
      "Epoch: 114 \t Batch: 0 \t Loss: 0.4693395197 \t\n",
      "Epoch: 114 \t Batch: 10 \t Loss: 0.4636650681 \t\n",
      "Epoch: 114 \t Batch: 20 \t Loss: 0.4910697937 \t\n",
      "Epoch: 114 \t Batch: 30 \t Loss: 0.4798021317 \t\n",
      "Epoch: 114 \t Batch: 40 \t Loss: 0.5171869993 \t\n",
      "Epoch: 114 \t Batch: 50 \t Loss: 0.5379408598 \t\n",
      "Epoch: 114 \t Batch: 60 \t Loss: 0.4738542140 \t\n",
      "Epoch: 114 \t Batch: 70 \t Loss: 0.4503757358 \t\n",
      "Epoch: 115 \t Batch: 0 \t Loss: 0.4671754837 \t\n",
      "Epoch: 115 \t Batch: 10 \t Loss: 0.4611925185 \t\n",
      "Epoch: 115 \t Batch: 20 \t Loss: 0.4893181920 \t\n",
      "Epoch: 115 \t Batch: 30 \t Loss: 0.4750250280 \t\n",
      "Epoch: 115 \t Batch: 40 \t Loss: 0.5155678988 \t\n",
      "Epoch: 115 \t Batch: 50 \t Loss: 0.5384542942 \t\n",
      "Epoch: 115 \t Batch: 60 \t Loss: 0.4700909555 \t\n",
      "Epoch: 115 \t Batch: 70 \t Loss: 0.4483733773 \t\n",
      "Epoch: 116 \t Batch: 0 \t Loss: 0.4640921950 \t\n",
      "Epoch: 116 \t Batch: 10 \t Loss: 0.4577507377 \t\n",
      "Epoch: 116 \t Batch: 20 \t Loss: 0.4866806865 \t\n",
      "Epoch: 116 \t Batch: 30 \t Loss: 0.4688151479 \t\n",
      "Epoch: 116 \t Batch: 40 \t Loss: 0.5132308602 \t\n",
      "Epoch: 116 \t Batch: 50 \t Loss: 0.5378102660 \t\n",
      "Epoch: 116 \t Batch: 60 \t Loss: 0.4661895633 \t\n",
      "Epoch: 116 \t Batch: 70 \t Loss: 0.4455790520 \t\n",
      "Epoch: 117 \t Batch: 0 \t Loss: 0.4601815343 \t\n",
      "Epoch: 117 \t Batch: 10 \t Loss: 0.4535590112 \t\n",
      "Epoch: 117 \t Batch: 20 \t Loss: 0.4832327962 \t\n",
      "Epoch: 117 \t Batch: 30 \t Loss: 0.4616776407 \t\n",
      "Epoch: 117 \t Batch: 40 \t Loss: 0.5092297792 \t\n",
      "Epoch: 117 \t Batch: 50 \t Loss: 0.5358945131 \t\n",
      "Epoch: 117 \t Batch: 60 \t Loss: 0.4625108838 \t\n",
      "Epoch: 117 \t Batch: 70 \t Loss: 0.4412082732 \t\n",
      "Epoch: 118 \t Batch: 0 \t Loss: 0.4551682472 \t\n",
      "Epoch: 118 \t Batch: 10 \t Loss: 0.4485341012 \t\n",
      "Epoch: 118 \t Batch: 20 \t Loss: 0.4788862169 \t\n",
      "Epoch: 118 \t Batch: 30 \t Loss: 0.4538829327 \t\n",
      "Epoch: 118 \t Batch: 40 \t Loss: 0.5028935671 \t\n",
      "Epoch: 118 \t Batch: 50 \t Loss: 0.5325396657 \t\n",
      "Epoch: 118 \t Batch: 60 \t Loss: 0.4592282772 \t\n",
      "Epoch: 118 \t Batch: 70 \t Loss: 0.4349820018 \t\n",
      "Epoch: 119 \t Batch: 0 \t Loss: 0.4490259290 \t\n",
      "Epoch: 119 \t Batch: 10 \t Loss: 0.4428436756 \t\n",
      "Epoch: 119 \t Batch: 20 \t Loss: 0.4739458561 \t\n",
      "Epoch: 119 \t Batch: 30 \t Loss: 0.4459144473 \t\n",
      "Epoch: 119 \t Batch: 40 \t Loss: 0.4948235154 \t\n",
      "Epoch: 119 \t Batch: 50 \t Loss: 0.5282305479 \t\n",
      "Epoch: 119 \t Batch: 60 \t Loss: 0.4567957819 \t\n",
      "Epoch: 119 \t Batch: 70 \t Loss: 0.4277713895 \t\n",
      "Epoch: 120 \t Batch: 0 \t Loss: 0.4424568117 \t\n",
      "Epoch: 120 \t Batch: 10 \t Loss: 0.4371608198 \t\n",
      "Epoch: 120 \t Batch: 20 \t Loss: 0.4693251550 \t\n",
      "Epoch: 120 \t Batch: 30 \t Loss: 0.4384359717 \t\n",
      "Epoch: 120 \t Batch: 40 \t Loss: 0.4866752625 \t\n",
      "Epoch: 120 \t Batch: 50 \t Loss: 0.5238305330 \t\n",
      "Epoch: 120 \t Batch: 60 \t Loss: 0.4555703998 \t\n",
      "Epoch: 120 \t Batch: 70 \t Loss: 0.4210525453 \t\n",
      "Epoch: 121 \t Batch: 0 \t Loss: 0.4363973141 \t\n",
      "Epoch: 121 \t Batch: 10 \t Loss: 0.4320988655 \t\n",
      "Epoch: 121 \t Batch: 20 \t Loss: 0.4657457769 \t\n",
      "Epoch: 121 \t Batch: 30 \t Loss: 0.4316596091 \t\n",
      "Epoch: 121 \t Batch: 40 \t Loss: 0.4800451696 \t\n",
      "Epoch: 121 \t Batch: 50 \t Loss: 0.5198983550 \t\n",
      "Epoch: 121 \t Batch: 60 \t Loss: 0.4552685618 \t\n",
      "Epoch: 121 \t Batch: 70 \t Loss: 0.4159255028 \t\n",
      "Epoch: 122 \t Batch: 0 \t Loss: 0.4313554168 \t\n",
      "Epoch: 122 \t Batch: 10 \t Loss: 0.4276543260 \t\n",
      "Epoch: 122 \t Batch: 20 \t Loss: 0.4632700086 \t\n",
      "Epoch: 122 \t Batch: 30 \t Loss: 0.4252521098 \t\n",
      "Epoch: 122 \t Batch: 40 \t Loss: 0.4757264555 \t\n",
      "Epoch: 122 \t Batch: 50 \t Loss: 0.5166254044 \t\n",
      "Epoch: 122 \t Batch: 60 \t Loss: 0.4554291368 \t\n",
      "Epoch: 122 \t Batch: 70 \t Loss: 0.4130143523 \t\n",
      "Epoch: 123 \t Batch: 0 \t Loss: 0.4276556969 \t\n",
      "Epoch: 123 \t Batch: 10 \t Loss: 0.4237426817 \t\n",
      "Epoch: 123 \t Batch: 20 \t Loss: 0.4619497657 \t\n",
      "Epoch: 123 \t Batch: 30 \t Loss: 0.4193200469 \t\n",
      "Epoch: 123 \t Batch: 40 \t Loss: 0.4740787446 \t\n",
      "Epoch: 123 \t Batch: 50 \t Loss: 0.5145827532 \t\n",
      "Epoch: 123 \t Batch: 60 \t Loss: 0.4564160407 \t\n",
      "Epoch: 123 \t Batch: 70 \t Loss: 0.4127563834 \t\n",
      "Epoch: 124 \t Batch: 0 \t Loss: 0.4259021878 \t\n",
      "Epoch: 124 \t Batch: 10 \t Loss: 0.4209665358 \t\n",
      "Epoch: 124 \t Batch: 20 \t Loss: 0.4624078274 \t\n",
      "Epoch: 124 \t Batch: 30 \t Loss: 0.4149369597 \t\n",
      "Epoch: 124 \t Batch: 40 \t Loss: 0.4751697481 \t\n",
      "Epoch: 124 \t Batch: 50 \t Loss: 0.5145431161 \t\n",
      "Epoch: 124 \t Batch: 60 \t Loss: 0.4592673182 \t\n",
      "Epoch: 124 \t Batch: 70 \t Loss: 0.4150417447 \t\n",
      "Epoch: 125 \t Batch: 0 \t Loss: 0.4265959859 \t\n",
      "Epoch: 125 \t Batch: 10 \t Loss: 0.4200516641 \t\n",
      "Epoch: 125 \t Batch: 20 \t Loss: 0.4650464058 \t\n",
      "Epoch: 125 \t Batch: 30 \t Loss: 0.4130558372 \t\n",
      "Epoch: 125 \t Batch: 40 \t Loss: 0.4781237543 \t\n",
      "Epoch: 125 \t Batch: 50 \t Loss: 0.5164542198 \t\n",
      "Epoch: 125 \t Batch: 60 \t Loss: 0.4643447995 \t\n",
      "Epoch: 125 \t Batch: 70 \t Loss: 0.4185780585 \t\n",
      "Epoch: 126 \t Batch: 0 \t Loss: 0.4291512072 \t\n",
      "Epoch: 126 \t Batch: 10 \t Loss: 0.4209280908 \t\n",
      "Epoch: 126 \t Batch: 20 \t Loss: 0.4693953395 \t\n",
      "Epoch: 126 \t Batch: 30 \t Loss: 0.4136298299 \t\n",
      "Epoch: 126 \t Batch: 40 \t Loss: 0.4813486934 \t\n",
      "Epoch: 126 \t Batch: 50 \t Loss: 0.5193934441 \t\n",
      "Epoch: 126 \t Batch: 60 \t Loss: 0.4711064398 \t\n",
      "Epoch: 126 \t Batch: 70 \t Loss: 0.4217839837 \t\n",
      "Epoch: 127 \t Batch: 0 \t Loss: 0.4324367046 \t\n",
      "Epoch: 127 \t Batch: 10 \t Loss: 0.4230896533 \t\n",
      "Epoch: 127 \t Batch: 20 \t Loss: 0.4746858478 \t\n",
      "Epoch: 127 \t Batch: 30 \t Loss: 0.4161396027 \t\n",
      "Epoch: 127 \t Batch: 40 \t Loss: 0.4840515256 \t\n",
      "Epoch: 127 \t Batch: 50 \t Loss: 0.5227788687 \t\n",
      "Epoch: 127 \t Batch: 60 \t Loss: 0.4788235128 \t\n",
      "Epoch: 127 \t Batch: 70 \t Loss: 0.4243858755 \t\n",
      "Epoch: 128 \t Batch: 0 \t Loss: 0.4360976219 \t\n",
      "Epoch: 128 \t Batch: 10 \t Loss: 0.4262501299 \t\n",
      "Epoch: 128 \t Batch: 20 \t Loss: 0.4806426466 \t\n",
      "Epoch: 128 \t Batch: 30 \t Loss: 0.4200680256 \t\n",
      "Epoch: 128 \t Batch: 40 \t Loss: 0.4866043031 \t\n",
      "Epoch: 128 \t Batch: 50 \t Loss: 0.5267480016 \t\n",
      "Epoch: 128 \t Batch: 60 \t Loss: 0.4866806865 \t\n",
      "Epoch: 128 \t Batch: 70 \t Loss: 0.4271228313 \t\n",
      "Epoch: 129 \t Batch: 0 \t Loss: 0.4402585626 \t\n",
      "Epoch: 129 \t Batch: 10 \t Loss: 0.4299084544 \t\n",
      "Epoch: 129 \t Batch: 20 \t Loss: 0.4867980480 \t\n",
      "Epoch: 129 \t Batch: 30 \t Loss: 0.4242558479 \t\n",
      "Epoch: 129 \t Batch: 40 \t Loss: 0.4892882407 \t\n",
      "Epoch: 129 \t Batch: 50 \t Loss: 0.5311594009 \t\n",
      "Epoch: 129 \t Batch: 60 \t Loss: 0.4930697381 \t\n",
      "Epoch: 129 \t Batch: 70 \t Loss: 0.4303041399 \t\n",
      "Epoch: 130 \t Batch: 0 \t Loss: 0.4444426596 \t\n",
      "Epoch: 130 \t Batch: 10 \t Loss: 0.4327204227 \t\n",
      "Epoch: 130 \t Batch: 20 \t Loss: 0.4918800294 \t\n",
      "Epoch: 130 \t Batch: 30 \t Loss: 0.4268278182 \t\n",
      "Epoch: 130 \t Batch: 40 \t Loss: 0.4915682673 \t\n",
      "Epoch: 130 \t Batch: 50 \t Loss: 0.5352803469 \t\n",
      "Epoch: 130 \t Batch: 60 \t Loss: 0.4962982237 \t\n",
      "Epoch: 130 \t Batch: 70 \t Loss: 0.4335436225 \t\n",
      "Epoch: 131 \t Batch: 0 \t Loss: 0.4478496015 \t\n",
      "Epoch: 131 \t Batch: 10 \t Loss: 0.4335435033 \t\n",
      "Epoch: 131 \t Batch: 20 \t Loss: 0.4948717058 \t\n",
      "Epoch: 131 \t Batch: 30 \t Loss: 0.4267292023 \t\n",
      "Epoch: 131 \t Batch: 40 \t Loss: 0.4929053187 \t\n",
      "Epoch: 131 \t Batch: 50 \t Loss: 0.5387608409 \t\n",
      "Epoch: 131 \t Batch: 60 \t Loss: 0.4961464405 \t\n",
      "Epoch: 131 \t Batch: 70 \t Loss: 0.4364184737 \t\n",
      "Epoch: 132 \t Batch: 0 \t Loss: 0.4502668977 \t\n",
      "Epoch: 132 \t Batch: 10 \t Loss: 0.4326612353 \t\n",
      "Epoch: 132 \t Batch: 20 \t Loss: 0.4959026873 \t\n",
      "Epoch: 132 \t Batch: 30 \t Loss: 0.4245268703 \t\n",
      "Epoch: 132 \t Batch: 40 \t Loss: 0.4930768013 \t\n",
      "Epoch: 132 \t Batch: 50 \t Loss: 0.5417625308 \t\n",
      "Epoch: 132 \t Batch: 60 \t Loss: 0.4937490523 \t\n",
      "Epoch: 132 \t Batch: 70 \t Loss: 0.4383340478 \t\n",
      "Epoch: 133 \t Batch: 0 \t Loss: 0.4517594278 \t\n",
      "Epoch: 133 \t Batch: 10 \t Loss: 0.4309924245 \t\n",
      "Epoch: 133 \t Batch: 20 \t Loss: 0.4955032766 \t\n",
      "Epoch: 133 \t Batch: 30 \t Loss: 0.4213253558 \t\n",
      "Epoch: 133 \t Batch: 40 \t Loss: 0.4914162755 \t\n",
      "Epoch: 133 \t Batch: 50 \t Loss: 0.5439791083 \t\n",
      "Epoch: 133 \t Batch: 60 \t Loss: 0.4899659753 \t\n",
      "Epoch: 133 \t Batch: 70 \t Loss: 0.4380187988 \t\n",
      "Epoch: 134 \t Batch: 0 \t Loss: 0.4517799318 \t\n",
      "Epoch: 134 \t Batch: 10 \t Loss: 0.4288795590 \t\n",
      "Epoch: 134 \t Batch: 20 \t Loss: 0.4937261641 \t\n",
      "Epoch: 134 \t Batch: 30 \t Loss: 0.4176988602 \t\n",
      "Epoch: 134 \t Batch: 40 \t Loss: 0.4869793355 \t\n",
      "Epoch: 134 \t Batch: 50 \t Loss: 0.5449067354 \t\n",
      "Epoch: 134 \t Batch: 60 \t Loss: 0.4851388037 \t\n",
      "Epoch: 134 \t Batch: 70 \t Loss: 0.4347441196 \t\n",
      "Epoch: 135 \t Batch: 0 \t Loss: 0.4500173926 \t\n",
      "Epoch: 135 \t Batch: 10 \t Loss: 0.4264774919 \t\n",
      "Epoch: 135 \t Batch: 20 \t Loss: 0.4908456206 \t\n",
      "Epoch: 135 \t Batch: 30 \t Loss: 0.4143068194 \t\n",
      "Epoch: 135 \t Batch: 40 \t Loss: 0.4801779389 \t\n",
      "Epoch: 135 \t Batch: 50 \t Loss: 0.5451810956 \t\n",
      "Epoch: 135 \t Batch: 60 \t Loss: 0.4799862504 \t\n",
      "Epoch: 135 \t Batch: 70 \t Loss: 0.4297124445 \t\n",
      "Epoch: 136 \t Batch: 0 \t Loss: 0.4474615157 \t\n",
      "Epoch: 136 \t Batch: 10 \t Loss: 0.4244945645 \t\n",
      "Epoch: 136 \t Batch: 20 \t Loss: 0.4878768921 \t\n",
      "Epoch: 136 \t Batch: 30 \t Loss: 0.4119077623 \t\n",
      "Epoch: 136 \t Batch: 40 \t Loss: 0.4728783071 \t\n",
      "Epoch: 136 \t Batch: 50 \t Loss: 0.5463632345 \t\n",
      "Epoch: 136 \t Batch: 60 \t Loss: 0.4751592278 \t\n",
      "Epoch: 136 \t Batch: 70 \t Loss: 0.4250690937 \t\n",
      "Epoch: 137 \t Batch: 0 \t Loss: 0.4455552697 \t\n",
      "Epoch: 137 \t Batch: 10 \t Loss: 0.4234081209 \t\n",
      "Epoch: 137 \t Batch: 20 \t Loss: 0.4855408072 \t\n",
      "Epoch: 137 \t Batch: 30 \t Loss: 0.4103237689 \t\n",
      "Epoch: 137 \t Batch: 40 \t Loss: 0.4667757154 \t\n",
      "Epoch: 137 \t Batch: 50 \t Loss: 0.5493196249 \t\n",
      "Epoch: 137 \t Batch: 60 \t Loss: 0.4704383016 \t\n",
      "Epoch: 137 \t Batch: 70 \t Loss: 0.4219473004 \t\n",
      "Epoch: 138 \t Batch: 0 \t Loss: 0.4446295202 \t\n",
      "Epoch: 138 \t Batch: 10 \t Loss: 0.4228572845 \t\n",
      "Epoch: 138 \t Batch: 20 \t Loss: 0.4836842418 \t\n",
      "Epoch: 138 \t Batch: 30 \t Loss: 0.4084901214 \t\n",
      "Epoch: 138 \t Batch: 40 \t Loss: 0.4626063704 \t\n",
      "Epoch: 138 \t Batch: 50 \t Loss: 0.5538543463 \t\n",
      "Epoch: 138 \t Batch: 60 \t Loss: 0.4654754698 \t\n",
      "Epoch: 138 \t Batch: 70 \t Loss: 0.4205431342 \t\n",
      "Epoch: 139 \t Batch: 0 \t Loss: 0.4444947839 \t\n",
      "Epoch: 139 \t Batch: 10 \t Loss: 0.4225935042 \t\n",
      "Epoch: 139 \t Batch: 20 \t Loss: 0.4822250903 \t\n",
      "Epoch: 139 \t Batch: 30 \t Loss: 0.4063127041 \t\n",
      "Epoch: 139 \t Batch: 40 \t Loss: 0.4607118964 \t\n",
      "Epoch: 139 \t Batch: 50 \t Loss: 0.5599221587 \t\n",
      "Epoch: 139 \t Batch: 60 \t Loss: 0.4609941840 \t\n",
      "Epoch: 139 \t Batch: 70 \t Loss: 0.4208927751 \t\n",
      "Epoch: 140 \t Batch: 0 \t Loss: 0.4454435408 \t\n",
      "Epoch: 140 \t Batch: 10 \t Loss: 0.4231876433 \t\n",
      "Epoch: 140 \t Batch: 20 \t Loss: 0.4819171429 \t\n",
      "Epoch: 140 \t Batch: 30 \t Loss: 0.4049740434 \t\n",
      "Epoch: 140 \t Batch: 40 \t Loss: 0.4609753191 \t\n",
      "Epoch: 140 \t Batch: 50 \t Loss: 0.5674264431 \t\n",
      "Epoch: 140 \t Batch: 60 \t Loss: 0.4581243396 \t\n",
      "Epoch: 140 \t Batch: 70 \t Loss: 0.4225064218 \t\n",
      "Epoch: 141 \t Batch: 0 \t Loss: 0.4476308823 \t\n",
      "Epoch: 141 \t Batch: 10 \t Loss: 0.4249026179 \t\n",
      "Epoch: 141 \t Batch: 20 \t Loss: 0.4829359055 \t\n",
      "Epoch: 141 \t Batch: 30 \t Loss: 0.4054277837 \t\n",
      "Epoch: 141 \t Batch: 40 \t Loss: 0.4619895816 \t\n",
      "Epoch: 141 \t Batch: 50 \t Loss: 0.5753843188 \t\n",
      "Epoch: 141 \t Batch: 60 \t Loss: 0.4568561018 \t\n",
      "Epoch: 141 \t Batch: 70 \t Loss: 0.4239733815 \t\n",
      "Epoch: 142 \t Batch: 0 \t Loss: 0.4502617419 \t\n",
      "Epoch: 142 \t Batch: 10 \t Loss: 0.4269249439 \t\n",
      "Epoch: 142 \t Batch: 20 \t Loss: 0.4845716059 \t\n",
      "Epoch: 142 \t Batch: 30 \t Loss: 0.4075574875 \t\n",
      "Epoch: 142 \t Batch: 40 \t Loss: 0.4618562758 \t\n",
      "Epoch: 142 \t Batch: 50 \t Loss: 0.5824903250 \t\n",
      "Epoch: 142 \t Batch: 60 \t Loss: 0.4565084577 \t\n",
      "Epoch: 142 \t Batch: 70 \t Loss: 0.4243360460 \t\n",
      "Epoch: 143 \t Batch: 0 \t Loss: 0.4526005387 \t\n",
      "Epoch: 143 \t Batch: 10 \t Loss: 0.4284886718 \t\n",
      "Epoch: 143 \t Batch: 20 \t Loss: 0.4862612188 \t\n",
      "Epoch: 143 \t Batch: 30 \t Loss: 0.4108789861 \t\n",
      "Epoch: 143 \t Batch: 40 \t Loss: 0.4602532983 \t\n",
      "Epoch: 143 \t Batch: 50 \t Loss: 0.5884546041 \t\n",
      "Epoch: 143 \t Batch: 60 \t Loss: 0.4568498135 \t\n",
      "Epoch: 143 \t Batch: 70 \t Loss: 0.4240881801 \t\n",
      "Epoch: 144 \t Batch: 0 \t Loss: 0.4548473358 \t\n",
      "Epoch: 144 \t Batch: 10 \t Loss: 0.4296037555 \t\n",
      "Epoch: 144 \t Batch: 20 \t Loss: 0.4881017804 \t\n",
      "Epoch: 144 \t Batch: 30 \t Loss: 0.4148107171 \t\n",
      "Epoch: 144 \t Batch: 40 \t Loss: 0.4582868516 \t\n",
      "Epoch: 144 \t Batch: 50 \t Loss: 0.5936194062 \t\n",
      "Epoch: 144 \t Batch: 60 \t Loss: 0.4575162530 \t\n",
      "Epoch: 144 \t Batch: 70 \t Loss: 0.4239057899 \t\n",
      "Epoch: 145 \t Batch: 0 \t Loss: 0.4569769800 \t\n",
      "Epoch: 145 \t Batch: 10 \t Loss: 0.4299316406 \t\n",
      "Epoch: 145 \t Batch: 20 \t Loss: 0.4897860587 \t\n",
      "Epoch: 145 \t Batch: 30 \t Loss: 0.4176460803 \t\n",
      "Epoch: 145 \t Batch: 40 \t Loss: 0.4567131996 \t\n",
      "Epoch: 145 \t Batch: 50 \t Loss: 0.5973914862 \t\n",
      "Epoch: 145 \t Batch: 60 \t Loss: 0.4574601054 \t\n",
      "Epoch: 145 \t Batch: 70 \t Loss: 0.4237020612 \t\n",
      "Epoch: 146 \t Batch: 0 \t Loss: 0.4581887722 \t\n",
      "Epoch: 146 \t Batch: 10 \t Loss: 0.4284599721 \t\n",
      "Epoch: 146 \t Batch: 20 \t Loss: 0.4904722571 \t\n",
      "Epoch: 146 \t Batch: 30 \t Loss: 0.4177134633 \t\n",
      "Epoch: 146 \t Batch: 40 \t Loss: 0.4556273818 \t\n",
      "Epoch: 146 \t Batch: 50 \t Loss: 0.5990282297 \t\n",
      "Epoch: 146 \t Batch: 60 \t Loss: 0.4560895860 \t\n",
      "Epoch: 146 \t Batch: 70 \t Loss: 0.4231943190 \t\n",
      "Epoch: 147 \t Batch: 0 \t Loss: 0.4580651224 \t\n",
      "Epoch: 147 \t Batch: 10 \t Loss: 0.4246011376 \t\n",
      "Epoch: 147 \t Batch: 20 \t Loss: 0.4898130298 \t\n",
      "Epoch: 147 \t Batch: 30 \t Loss: 0.4149942994 \t\n",
      "Epoch: 147 \t Batch: 40 \t Loss: 0.4550672472 \t\n",
      "Epoch: 147 \t Batch: 50 \t Loss: 0.5985153913 \t\n",
      "Epoch: 147 \t Batch: 60 \t Loss: 0.4540109634 \t\n",
      "Epoch: 147 \t Batch: 70 \t Loss: 0.4225505292 \t\n",
      "Epoch: 148 \t Batch: 0 \t Loss: 0.4571622014 \t\n",
      "Epoch: 148 \t Batch: 10 \t Loss: 0.4190787375 \t\n",
      "Epoch: 148 \t Batch: 20 \t Loss: 0.4885451794 \t\n",
      "Epoch: 148 \t Batch: 30 \t Loss: 0.4111123979 \t\n",
      "Epoch: 148 \t Batch: 40 \t Loss: 0.4547719061 \t\n",
      "Epoch: 148 \t Batch: 50 \t Loss: 0.5963858962 \t\n",
      "Epoch: 148 \t Batch: 60 \t Loss: 0.4523154199 \t\n",
      "Epoch: 148 \t Batch: 70 \t Loss: 0.4216063619 \t\n",
      "Epoch: 149 \t Batch: 0 \t Loss: 0.4559548795 \t\n",
      "Epoch: 149 \t Batch: 10 \t Loss: 0.4128355384 \t\n",
      "Epoch: 149 \t Batch: 20 \t Loss: 0.4873492718 \t\n",
      "Epoch: 149 \t Batch: 30 \t Loss: 0.4074223638 \t\n",
      "Epoch: 149 \t Batch: 40 \t Loss: 0.4534783363 \t\n",
      "Epoch: 149 \t Batch: 50 \t Loss: 0.5927603245 \t\n",
      "Epoch: 149 \t Batch: 60 \t Loss: 0.4513267577 \t\n",
      "Epoch: 149 \t Batch: 70 \t Loss: 0.4194199443 \t\n",
      "Epoch: 150 \t Batch: 0 \t Loss: 0.4540192187 \t\n",
      "Epoch: 150 \t Batch: 10 \t Loss: 0.4064620435 \t\n",
      "Epoch: 150 \t Batch: 20 \t Loss: 0.4863942266 \t\n",
      "Epoch: 150 \t Batch: 30 \t Loss: 0.4044376910 \t\n",
      "Epoch: 150 \t Batch: 40 \t Loss: 0.4503905177 \t\n",
      "Epoch: 150 \t Batch: 50 \t Loss: 0.5878208876 \t\n",
      "Epoch: 150 \t Batch: 60 \t Loss: 0.4512519836 \t\n",
      "Epoch: 150 \t Batch: 70 \t Loss: 0.4159445167 \t\n",
      "Epoch: 151 \t Batch: 0 \t Loss: 0.4513555169 \t\n",
      "Epoch: 151 \t Batch: 10 \t Loss: 0.4008606970 \t\n",
      "Epoch: 151 \t Batch: 20 \t Loss: 0.4863948822 \t\n",
      "Epoch: 151 \t Batch: 30 \t Loss: 0.4027818739 \t\n",
      "Epoch: 151 \t Batch: 40 \t Loss: 0.4468951821 \t\n",
      "Epoch: 151 \t Batch: 50 \t Loss: 0.5828999281 \t\n",
      "Epoch: 151 \t Batch: 60 \t Loss: 0.4529876709 \t\n",
      "Epoch: 151 \t Batch: 70 \t Loss: 0.4128026962 \t\n",
      "Epoch: 152 \t Batch: 0 \t Loss: 0.4491871297 \t\n",
      "Epoch: 152 \t Batch: 10 \t Loss: 0.3971244395 \t\n",
      "Epoch: 152 \t Batch: 20 \t Loss: 0.4883852601 \t\n",
      "Epoch: 152 \t Batch: 30 \t Loss: 0.4028329253 \t\n",
      "Epoch: 152 \t Batch: 40 \t Loss: 0.4454490542 \t\n",
      "Epoch: 152 \t Batch: 50 \t Loss: 0.5792549849 \t\n",
      "Epoch: 152 \t Batch: 60 \t Loss: 0.4568582475 \t\n",
      "Epoch: 152 \t Batch: 70 \t Loss: 0.4116339684 \t\n",
      "Epoch: 153 \t Batch: 0 \t Loss: 0.4485763609 \t\n",
      "Epoch: 153 \t Batch: 10 \t Loss: 0.3950412571 \t\n",
      "Epoch: 153 \t Batch: 20 \t Loss: 0.4921229780 \t\n",
      "Epoch: 153 \t Batch: 30 \t Loss: 0.4038252831 \t\n",
      "Epoch: 153 \t Batch: 40 \t Loss: 0.4474400580 \t\n",
      "Epoch: 153 \t Batch: 50 \t Loss: 0.5770575404 \t\n",
      "Epoch: 153 \t Batch: 60 \t Loss: 0.4619795680 \t\n",
      "Epoch: 153 \t Batch: 70 \t Loss: 0.4127772748 \t\n",
      "Epoch: 154 \t Batch: 0 \t Loss: 0.4495881200 \t\n",
      "Epoch: 154 \t Batch: 10 \t Loss: 0.3934627175 \t\n",
      "Epoch: 154 \t Batch: 20 \t Loss: 0.4967196882 \t\n",
      "Epoch: 154 \t Batch: 30 \t Loss: 0.4048862457 \t\n",
      "Epoch: 154 \t Batch: 40 \t Loss: 0.4528282583 \t\n",
      "Epoch: 154 \t Batch: 50 \t Loss: 0.5762143731 \t\n",
      "Epoch: 154 \t Batch: 60 \t Loss: 0.4678470492 \t\n",
      "Epoch: 154 \t Batch: 70 \t Loss: 0.4163560271 \t\n",
      "Epoch: 155 \t Batch: 0 \t Loss: 0.4524812102 \t\n",
      "Epoch: 155 \t Batch: 10 \t Loss: 0.3924061656 \t\n",
      "Epoch: 155 \t Batch: 20 \t Loss: 0.5020696521 \t\n",
      "Epoch: 155 \t Batch: 30 \t Loss: 0.4067033231 \t\n",
      "Epoch: 155 \t Batch: 40 \t Loss: 0.4610283375 \t\n",
      "Epoch: 155 \t Batch: 50 \t Loss: 0.5771428347 \t\n",
      "Epoch: 155 \t Batch: 60 \t Loss: 0.4748166203 \t\n",
      "Epoch: 155 \t Batch: 70 \t Loss: 0.4219342768 \t\n",
      "Epoch: 156 \t Batch: 0 \t Loss: 0.4572897851 \t\n",
      "Epoch: 156 \t Batch: 10 \t Loss: 0.3927311301 \t\n",
      "Epoch: 156 \t Batch: 20 \t Loss: 0.5082781315 \t\n",
      "Epoch: 156 \t Batch: 30 \t Loss: 0.4101102352 \t\n",
      "Epoch: 156 \t Batch: 40 \t Loss: 0.4699648917 \t\n",
      "Epoch: 156 \t Batch: 50 \t Loss: 0.5793921351 \t\n",
      "Epoch: 156 \t Batch: 60 \t Loss: 0.4822830260 \t\n",
      "Epoch: 156 \t Batch: 70 \t Loss: 0.4275524616 \t\n",
      "Epoch: 157 \t Batch: 0 \t Loss: 0.4625145495 \t\n",
      "Epoch: 157 \t Batch: 10 \t Loss: 0.3944964409 \t\n",
      "Epoch: 157 \t Batch: 20 \t Loss: 0.5143687725 \t\n",
      "Epoch: 157 \t Batch: 30 \t Loss: 0.4144695401 \t\n",
      "Epoch: 157 \t Batch: 40 \t Loss: 0.4766079783 \t\n",
      "Epoch: 157 \t Batch: 50 \t Loss: 0.5812994838 \t\n",
      "Epoch: 157 \t Batch: 60 \t Loss: 0.4887170196 \t\n",
      "Epoch: 157 \t Batch: 70 \t Loss: 0.4307587743 \t\n",
      "Epoch: 158 \t Batch: 0 \t Loss: 0.4659543931 \t\n",
      "Epoch: 158 \t Batch: 10 \t Loss: 0.3968965411 \t\n",
      "Epoch: 158 \t Batch: 20 \t Loss: 0.5191946626 \t\n",
      "Epoch: 158 \t Batch: 30 \t Loss: 0.4186220169 \t\n",
      "Epoch: 158 \t Batch: 40 \t Loss: 0.4795925021 \t\n",
      "Epoch: 158 \t Batch: 50 \t Loss: 0.5819987655 \t\n",
      "Epoch: 158 \t Batch: 60 \t Loss: 0.4932698309 \t\n",
      "Epoch: 158 \t Batch: 70 \t Loss: 0.4311293662 \t\n",
      "Epoch: 159 \t Batch: 0 \t Loss: 0.4671616554 \t\n",
      "Epoch: 159 \t Batch: 10 \t Loss: 0.3994731307 \t\n",
      "Epoch: 159 \t Batch: 20 \t Loss: 0.5225566030 \t\n",
      "Epoch: 159 \t Batch: 30 \t Loss: 0.4220139384 \t\n",
      "Epoch: 159 \t Batch: 40 \t Loss: 0.4798264503 \t\n",
      "Epoch: 159 \t Batch: 50 \t Loss: 0.5817745328 \t\n",
      "Epoch: 159 \t Batch: 60 \t Loss: 0.4954605699 \t\n",
      "Epoch: 159 \t Batch: 70 \t Loss: 0.4301287532 \t\n",
      "Epoch: 160 \t Batch: 0 \t Loss: 0.4673129618 \t\n",
      "Epoch: 160 \t Batch: 10 \t Loss: 0.4011599422 \t\n",
      "Epoch: 160 \t Batch: 20 \t Loss: 0.5237505436 \t\n",
      "Epoch: 160 \t Batch: 30 \t Loss: 0.4236577451 \t\n",
      "Epoch: 160 \t Batch: 40 \t Loss: 0.4783539176 \t\n",
      "Epoch: 160 \t Batch: 50 \t Loss: 0.5806189775 \t\n",
      "Epoch: 160 \t Batch: 60 \t Loss: 0.4941521585 \t\n",
      "Epoch: 160 \t Batch: 70 \t Loss: 0.4284783006 \t\n",
      "Epoch: 161 \t Batch: 0 \t Loss: 0.4667004645 \t\n",
      "Epoch: 161 \t Batch: 10 \t Loss: 0.4000233710 \t\n",
      "Epoch: 161 \t Batch: 20 \t Loss: 0.5216788650 \t\n",
      "Epoch: 161 \t Batch: 30 \t Loss: 0.4223281741 \t\n",
      "Epoch: 161 \t Batch: 40 \t Loss: 0.4754835069 \t\n",
      "Epoch: 161 \t Batch: 50 \t Loss: 0.5780273676 \t\n",
      "Epoch: 161 \t Batch: 60 \t Loss: 0.4887405932 \t\n",
      "Epoch: 161 \t Batch: 70 \t Loss: 0.4264435768 \t\n",
      "Epoch: 162 \t Batch: 0 \t Loss: 0.4654405117 \t\n",
      "Epoch: 162 \t Batch: 10 \t Loss: 0.3956822455 \t\n",
      "Epoch: 162 \t Batch: 20 \t Loss: 0.5162425041 \t\n",
      "Epoch: 162 \t Batch: 30 \t Loss: 0.4183033407 \t\n",
      "Epoch: 162 \t Batch: 40 \t Loss: 0.4716973901 \t\n",
      "Epoch: 162 \t Batch: 50 \t Loss: 0.5750206113 \t\n",
      "Epoch: 162 \t Batch: 60 \t Loss: 0.4806752205 \t\n",
      "Epoch: 162 \t Batch: 70 \t Loss: 0.4246788025 \t\n",
      "Epoch: 163 \t Batch: 0 \t Loss: 0.4644004703 \t\n",
      "Epoch: 163 \t Batch: 10 \t Loss: 0.3902560472 \t\n",
      "Epoch: 163 \t Batch: 20 \t Loss: 0.5095242858 \t\n",
      "Epoch: 163 \t Batch: 30 \t Loss: 0.4135212004 \t\n",
      "Epoch: 163 \t Batch: 40 \t Loss: 0.4674017429 \t\n",
      "Epoch: 163 \t Batch: 50 \t Loss: 0.5729405284 \t\n",
      "Epoch: 163 \t Batch: 60 \t Loss: 0.4721155763 \t\n",
      "Epoch: 163 \t Batch: 70 \t Loss: 0.4226754308 \t\n",
      "Epoch: 164 \t Batch: 0 \t Loss: 0.4634489119 \t\n",
      "Epoch: 164 \t Batch: 10 \t Loss: 0.3857476711 \t\n",
      "Epoch: 164 \t Batch: 20 \t Loss: 0.5027750731 \t\n",
      "Epoch: 164 \t Batch: 30 \t Loss: 0.4095068872 \t\n",
      "Epoch: 164 \t Batch: 40 \t Loss: 0.4616107941 \t\n",
      "Epoch: 164 \t Batch: 50 \t Loss: 0.5713822842 \t\n",
      "Epoch: 164 \t Batch: 60 \t Loss: 0.4639704823 \t\n",
      "Epoch: 164 \t Batch: 70 \t Loss: 0.4190925062 \t\n",
      "Epoch: 165 \t Batch: 0 \t Loss: 0.4615404606 \t\n",
      "Epoch: 165 \t Batch: 10 \t Loss: 0.3828552365 \t\n",
      "Epoch: 165 \t Batch: 20 \t Loss: 0.4963643551 \t\n",
      "Epoch: 165 \t Batch: 30 \t Loss: 0.4068600535 \t\n",
      "Epoch: 165 \t Batch: 40 \t Loss: 0.4535955489 \t\n",
      "Epoch: 165 \t Batch: 50 \t Loss: 0.5701820850 \t\n",
      "Epoch: 165 \t Batch: 60 \t Loss: 0.4567280710 \t\n",
      "Epoch: 165 \t Batch: 70 \t Loss: 0.4142405391 \t\n",
      "Epoch: 166 \t Batch: 0 \t Loss: 0.4589322209 \t\n",
      "Epoch: 166 \t Batch: 10 \t Loss: 0.3821284175 \t\n",
      "Epoch: 166 \t Batch: 20 \t Loss: 0.4910826087 \t\n",
      "Epoch: 166 \t Batch: 30 \t Loss: 0.4063022137 \t\n",
      "Epoch: 166 \t Batch: 40 \t Loss: 0.4450649321 \t\n",
      "Epoch: 166 \t Batch: 50 \t Loss: 0.5706456304 \t\n",
      "Epoch: 166 \t Batch: 60 \t Loss: 0.4514797330 \t\n",
      "Epoch: 166 \t Batch: 70 \t Loss: 0.4104395807 \t\n",
      "Epoch: 167 \t Batch: 0 \t Loss: 0.4576056600 \t\n",
      "Epoch: 167 \t Batch: 10 \t Loss: 0.3837706745 \t\n",
      "Epoch: 167 \t Batch: 20 \t Loss: 0.4877929688 \t\n",
      "Epoch: 167 \t Batch: 30 \t Loss: 0.4079672694 \t\n",
      "Epoch: 167 \t Batch: 40 \t Loss: 0.4385311007 \t\n",
      "Epoch: 167 \t Batch: 50 \t Loss: 0.5737535357 \t\n",
      "Epoch: 167 \t Batch: 60 \t Loss: 0.4483026862 \t\n",
      "Epoch: 167 \t Batch: 70 \t Loss: 0.4092231691 \t\n",
      "Epoch: 168 \t Batch: 0 \t Loss: 0.4585706294 \t\n",
      "Epoch: 168 \t Batch: 10 \t Loss: 0.3864558637 \t\n",
      "Epoch: 168 \t Batch: 20 \t Loss: 0.4859577715 \t\n",
      "Epoch: 168 \t Batch: 30 \t Loss: 0.4102779925 \t\n",
      "Epoch: 168 \t Batch: 40 \t Loss: 0.4349950850 \t\n",
      "Epoch: 168 \t Batch: 50 \t Loss: 0.5789076090 \t\n",
      "Epoch: 168 \t Batch: 60 \t Loss: 0.4460033476 \t\n",
      "Epoch: 168 \t Batch: 70 \t Loss: 0.4101125300 \t\n",
      "Epoch: 169 \t Batch: 0 \t Loss: 0.4609058499 \t\n",
      "Epoch: 169 \t Batch: 10 \t Loss: 0.3885596693 \t\n",
      "Epoch: 169 \t Batch: 20 \t Loss: 0.4845909476 \t\n",
      "Epoch: 169 \t Batch: 30 \t Loss: 0.4117980897 \t\n",
      "Epoch: 169 \t Batch: 40 \t Loss: 0.4343546033 \t\n",
      "Epoch: 169 \t Batch: 50 \t Loss: 0.5851785541 \t\n",
      "Epoch: 169 \t Batch: 60 \t Loss: 0.4441313148 \t\n",
      "Epoch: 169 \t Batch: 70 \t Loss: 0.4125100672 \t\n",
      "Epoch: 170 \t Batch: 0 \t Loss: 0.4639514387 \t\n",
      "Epoch: 170 \t Batch: 10 \t Loss: 0.3901503384 \t\n",
      "Epoch: 170 \t Batch: 20 \t Loss: 0.4839991629 \t\n",
      "Epoch: 170 \t Batch: 30 \t Loss: 0.4129496515 \t\n",
      "Epoch: 170 \t Batch: 40 \t Loss: 0.4361009598 \t\n",
      "Epoch: 170 \t Batch: 50 \t Loss: 0.5923081040 \t\n",
      "Epoch: 170 \t Batch: 60 \t Loss: 0.4434530735 \t\n",
      "Epoch: 170 \t Batch: 70 \t Loss: 0.4159463048 \t\n",
      "Epoch: 171 \t Batch: 0 \t Loss: 0.4675204754 \t\n",
      "Epoch: 171 \t Batch: 10 \t Loss: 0.3922425807 \t\n",
      "Epoch: 171 \t Batch: 20 \t Loss: 0.4845390916 \t\n",
      "Epoch: 171 \t Batch: 30 \t Loss: 0.4146402478 \t\n",
      "Epoch: 171 \t Batch: 40 \t Loss: 0.4385322034 \t\n",
      "Epoch: 171 \t Batch: 50 \t Loss: 0.5991800427 \t\n",
      "Epoch: 171 \t Batch: 60 \t Loss: 0.4440014958 \t\n",
      "Epoch: 171 \t Batch: 70 \t Loss: 0.4189527631 \t\n",
      "Epoch: 172 \t Batch: 0 \t Loss: 0.4705772996 \t\n",
      "Epoch: 172 \t Batch: 10 \t Loss: 0.3947524428 \t\n",
      "Epoch: 172 \t Batch: 20 \t Loss: 0.4853042662 \t\n",
      "Epoch: 172 \t Batch: 30 \t Loss: 0.4165036678 \t\n",
      "Epoch: 172 \t Batch: 40 \t Loss: 0.4390371442 \t\n",
      "Epoch: 172 \t Batch: 50 \t Loss: 0.6039232612 \t\n",
      "Epoch: 172 \t Batch: 60 \t Loss: 0.4447244704 \t\n",
      "Epoch: 172 \t Batch: 70 \t Loss: 0.4198296070 \t\n",
      "Epoch: 173 \t Batch: 0 \t Loss: 0.4718067050 \t\n",
      "Epoch: 173 \t Batch: 10 \t Loss: 0.3968721330 \t\n",
      "Epoch: 173 \t Batch: 20 \t Loss: 0.4852340221 \t\n",
      "Epoch: 173 \t Batch: 30 \t Loss: 0.4178686738 \t\n",
      "Epoch: 173 \t Batch: 40 \t Loss: 0.4364652038 \t\n",
      "Epoch: 173 \t Batch: 50 \t Loss: 0.6057277918 \t\n",
      "Epoch: 173 \t Batch: 60 \t Loss: 0.4452310205 \t\n",
      "Epoch: 173 \t Batch: 70 \t Loss: 0.4186569750 \t\n",
      "Epoch: 174 \t Batch: 0 \t Loss: 0.4713293910 \t\n",
      "Epoch: 174 \t Batch: 10 \t Loss: 0.3981567025 \t\n",
      "Epoch: 174 \t Batch: 20 \t Loss: 0.4843577445 \t\n",
      "Epoch: 174 \t Batch: 30 \t Loss: 0.4185470641 \t\n",
      "Epoch: 174 \t Batch: 40 \t Loss: 0.4321065843 \t\n",
      "Epoch: 174 \t Batch: 50 \t Loss: 0.6052080393 \t\n",
      "Epoch: 174 \t Batch: 60 \t Loss: 0.4457148015 \t\n",
      "Epoch: 174 \t Batch: 70 \t Loss: 0.4165686071 \t\n",
      "Epoch: 175 \t Batch: 0 \t Loss: 0.4698763490 \t\n",
      "Epoch: 175 \t Batch: 10 \t Loss: 0.3984011710 \t\n",
      "Epoch: 175 \t Batch: 20 \t Loss: 0.4828948081 \t\n",
      "Epoch: 175 \t Batch: 30 \t Loss: 0.4178309441 \t\n",
      "Epoch: 175 \t Batch: 40 \t Loss: 0.4276956916 \t\n",
      "Epoch: 175 \t Batch: 50 \t Loss: 0.6028250456 \t\n",
      "Epoch: 175 \t Batch: 60 \t Loss: 0.4455083311 \t\n",
      "Epoch: 175 \t Batch: 70 \t Loss: 0.4139215052 \t\n",
      "Epoch: 176 \t Batch: 0 \t Loss: 0.4672771990 \t\n",
      "Epoch: 176 \t Batch: 10 \t Loss: 0.3965158463 \t\n",
      "Epoch: 176 \t Batch: 20 \t Loss: 0.4802111089 \t\n",
      "Epoch: 176 \t Batch: 30 \t Loss: 0.4142927527 \t\n",
      "Epoch: 176 \t Batch: 40 \t Loss: 0.4237948954 \t\n",
      "Epoch: 176 \t Batch: 50 \t Loss: 0.5982204676 \t\n",
      "Epoch: 176 \t Batch: 60 \t Loss: 0.4433510303 \t\n",
      "Epoch: 176 \t Batch: 70 \t Loss: 0.4104028642 \t\n",
      "Epoch: 177 \t Batch: 0 \t Loss: 0.4629004896 \t\n",
      "Epoch: 177 \t Batch: 10 \t Loss: 0.3915925026 \t\n",
      "Epoch: 177 \t Batch: 20 \t Loss: 0.4758433700 \t\n",
      "Epoch: 177 \t Batch: 30 \t Loss: 0.4075414240 \t\n",
      "Epoch: 177 \t Batch: 40 \t Loss: 0.4205032885 \t\n",
      "Epoch: 177 \t Batch: 50 \t Loss: 0.5915127993 \t\n",
      "Epoch: 177 \t Batch: 60 \t Loss: 0.4394958615 \t\n",
      "Epoch: 177 \t Batch: 70 \t Loss: 0.4063136578 \t\n",
      "Epoch: 178 \t Batch: 0 \t Loss: 0.4572065771 \t\n",
      "Epoch: 178 \t Batch: 10 \t Loss: 0.3844650388 \t\n",
      "Epoch: 178 \t Batch: 20 \t Loss: 0.4705088139 \t\n",
      "Epoch: 178 \t Batch: 30 \t Loss: 0.3990673721 \t\n",
      "Epoch: 178 \t Batch: 40 \t Loss: 0.4179306924 \t\n",
      "Epoch: 178 \t Batch: 50 \t Loss: 0.5835869312 \t\n",
      "Epoch: 178 \t Batch: 60 \t Loss: 0.4354999959 \t\n",
      "Epoch: 178 \t Batch: 70 \t Loss: 0.4021750093 \t\n",
      "Epoch: 179 \t Batch: 0 \t Loss: 0.4511874318 \t\n",
      "Epoch: 179 \t Batch: 10 \t Loss: 0.3766664565 \t\n",
      "Epoch: 179 \t Batch: 20 \t Loss: 0.4651707709 \t\n",
      "Epoch: 179 \t Batch: 30 \t Loss: 0.3906053007 \t\n",
      "Epoch: 179 \t Batch: 40 \t Loss: 0.4151528478 \t\n",
      "Epoch: 179 \t Batch: 50 \t Loss: 0.5746808052 \t\n",
      "Epoch: 179 \t Batch: 60 \t Loss: 0.4322341084 \t\n",
      "Epoch: 179 \t Batch: 70 \t Loss: 0.3975486159 \t\n",
      "Epoch: 180 \t Batch: 0 \t Loss: 0.4448727667 \t\n",
      "Epoch: 180 \t Batch: 10 \t Loss: 0.3690581620 \t\n",
      "Epoch: 180 \t Batch: 20 \t Loss: 0.4598426819 \t\n",
      "Epoch: 180 \t Batch: 30 \t Loss: 0.3826441467 \t\n",
      "Epoch: 180 \t Batch: 40 \t Loss: 0.4107784331 \t\n",
      "Epoch: 180 \t Batch: 50 \t Loss: 0.5643420815 \t\n",
      "Epoch: 180 \t Batch: 60 \t Loss: 0.4298045039 \t\n",
      "Epoch: 180 \t Batch: 70 \t Loss: 0.3918443322 \t\n",
      "Epoch: 181 \t Batch: 0 \t Loss: 0.4379039705 \t\n",
      "Epoch: 181 \t Batch: 10 \t Loss: 0.3621442616 \t\n",
      "Epoch: 181 \t Batch: 20 \t Loss: 0.4547812343 \t\n",
      "Epoch: 181 \t Batch: 30 \t Loss: 0.3755638599 \t\n",
      "Epoch: 181 \t Batch: 40 \t Loss: 0.4050205648 \t\n",
      "Epoch: 181 \t Batch: 50 \t Loss: 0.5534456968 \t\n",
      "Epoch: 181 \t Batch: 60 \t Loss: 0.4287326932 \t\n",
      "Epoch: 181 \t Batch: 70 \t Loss: 0.3859018683 \t\n",
      "Epoch: 182 \t Batch: 0 \t Loss: 0.4309243560 \t\n",
      "Epoch: 182 \t Batch: 10 \t Loss: 0.3568135798 \t\n",
      "Epoch: 182 \t Batch: 20 \t Loss: 0.4510442615 \t\n",
      "Epoch: 182 \t Batch: 30 \t Loss: 0.3699846566 \t\n",
      "Epoch: 182 \t Batch: 40 \t Loss: 0.4000059962 \t\n",
      "Epoch: 182 \t Batch: 50 \t Loss: 0.5439622998 \t\n",
      "Epoch: 182 \t Batch: 60 \t Loss: 0.4294891953 \t\n",
      "Epoch: 182 \t Batch: 70 \t Loss: 0.3812170625 \t\n",
      "Epoch: 183 \t Batch: 0 \t Loss: 0.4251070619 \t\n",
      "Epoch: 183 \t Batch: 10 \t Loss: 0.3531081676 \t\n",
      "Epoch: 183 \t Batch: 20 \t Loss: 0.4490775466 \t\n",
      "Epoch: 183 \t Batch: 30 \t Loss: 0.3658158183 \t\n",
      "Epoch: 183 \t Batch: 40 \t Loss: 0.3974642754 \t\n",
      "Epoch: 183 \t Batch: 50 \t Loss: 0.5367502570 \t\n",
      "Epoch: 183 \t Batch: 60 \t Loss: 0.4311078489 \t\n",
      "Epoch: 183 \t Batch: 70 \t Loss: 0.3782726228 \t\n",
      "Epoch: 184 \t Batch: 0 \t Loss: 0.4205610752 \t\n",
      "Epoch: 184 \t Batch: 10 \t Loss: 0.3495821655 \t\n",
      "Epoch: 184 \t Batch: 20 \t Loss: 0.4479257464 \t\n",
      "Epoch: 184 \t Batch: 30 \t Loss: 0.3617441952 \t\n",
      "Epoch: 184 \t Batch: 40 \t Loss: 0.3974076807 \t\n",
      "Epoch: 184 \t Batch: 50 \t Loss: 0.5315439701 \t\n",
      "Epoch: 184 \t Batch: 60 \t Loss: 0.4321293235 \t\n",
      "Epoch: 184 \t Batch: 70 \t Loss: 0.3767805398 \t\n",
      "Epoch: 185 \t Batch: 0 \t Loss: 0.4169733524 \t\n",
      "Epoch: 185 \t Batch: 10 \t Loss: 0.3453208506 \t\n",
      "Epoch: 185 \t Batch: 20 \t Loss: 0.4468318522 \t\n",
      "Epoch: 185 \t Batch: 30 \t Loss: 0.3574166596 \t\n",
      "Epoch: 185 \t Batch: 40 \t Loss: 0.3995575011 \t\n",
      "Epoch: 185 \t Batch: 50 \t Loss: 0.5281075239 \t\n",
      "Epoch: 185 \t Batch: 60 \t Loss: 0.4328056872 \t\n",
      "Epoch: 185 \t Batch: 70 \t Loss: 0.3768715262 \t\n",
      "Epoch: 186 \t Batch: 0 \t Loss: 0.4148054123 \t\n",
      "Epoch: 186 \t Batch: 10 \t Loss: 0.3411838710 \t\n",
      "Epoch: 186 \t Batch: 20 \t Loss: 0.4462691247 \t\n",
      "Epoch: 186 \t Batch: 30 \t Loss: 0.3539572954 \t\n",
      "Epoch: 186 \t Batch: 40 \t Loss: 0.4032378197 \t\n",
      "Epoch: 186 \t Batch: 50 \t Loss: 0.5265036225 \t\n",
      "Epoch: 186 \t Batch: 60 \t Loss: 0.4342191815 \t\n",
      "Epoch: 186 \t Batch: 70 \t Loss: 0.3779572845 \t\n",
      "Epoch: 187 \t Batch: 0 \t Loss: 0.4138242602 \t\n",
      "Epoch: 187 \t Batch: 10 \t Loss: 0.3383906484 \t\n",
      "Epoch: 187 \t Batch: 20 \t Loss: 0.4465532899 \t\n",
      "Epoch: 187 \t Batch: 30 \t Loss: 0.3518026769 \t\n",
      "Epoch: 187 \t Batch: 40 \t Loss: 0.4063209593 \t\n",
      "Epoch: 187 \t Batch: 50 \t Loss: 0.5255814791 \t\n",
      "Epoch: 187 \t Batch: 60 \t Loss: 0.4361037612 \t\n",
      "Epoch: 187 \t Batch: 70 \t Loss: 0.3781241477 \t\n",
      "Epoch: 188 \t Batch: 0 \t Loss: 0.4124697149 \t\n",
      "Epoch: 188 \t Batch: 10 \t Loss: 0.3367057145 \t\n",
      "Epoch: 188 \t Batch: 20 \t Loss: 0.4464624822 \t\n",
      "Epoch: 188 \t Batch: 30 \t Loss: 0.3501664400 \t\n",
      "Epoch: 188 \t Batch: 40 \t Loss: 0.4063099027 \t\n",
      "Epoch: 188 \t Batch: 50 \t Loss: 0.5238918662 \t\n",
      "Epoch: 188 \t Batch: 60 \t Loss: 0.4377112985 \t\n",
      "Epoch: 188 \t Batch: 70 \t Loss: 0.3760258853 \t\n",
      "Epoch: 189 \t Batch: 0 \t Loss: 0.4096933305 \t\n",
      "Epoch: 189 \t Batch: 10 \t Loss: 0.3360242248 \t\n",
      "Epoch: 189 \t Batch: 20 \t Loss: 0.4455687404 \t\n",
      "Epoch: 189 \t Batch: 30 \t Loss: 0.3487922549 \t\n",
      "Epoch: 189 \t Batch: 40 \t Loss: 0.4033165872 \t\n",
      "Epoch: 189 \t Batch: 50 \t Loss: 0.5218390822 \t\n",
      "Epoch: 189 \t Batch: 60 \t Loss: 0.4387966096 \t\n",
      "Epoch: 189 \t Batch: 70 \t Loss: 0.3725820482 \t\n",
      "Epoch: 190 \t Batch: 0 \t Loss: 0.4062703252 \t\n",
      "Epoch: 190 \t Batch: 10 \t Loss: 0.3362073302 \t\n",
      "Epoch: 190 \t Batch: 20 \t Loss: 0.4442126751 \t\n",
      "Epoch: 190 \t Batch: 30 \t Loss: 0.3477322161 \t\n",
      "Epoch: 190 \t Batch: 40 \t Loss: 0.3992687762 \t\n",
      "Epoch: 190 \t Batch: 50 \t Loss: 0.5203746557 \t\n",
      "Epoch: 190 \t Batch: 60 \t Loss: 0.4389145374 \t\n",
      "Epoch: 190 \t Batch: 70 \t Loss: 0.3692331314 \t\n",
      "Epoch: 191 \t Batch: 0 \t Loss: 0.4031855464 \t\n",
      "Epoch: 191 \t Batch: 10 \t Loss: 0.3359391987 \t\n",
      "Epoch: 191 \t Batch: 20 \t Loss: 0.4420209825 \t\n",
      "Epoch: 191 \t Batch: 30 \t Loss: 0.3461157680 \t\n",
      "Epoch: 191 \t Batch: 40 \t Loss: 0.3953667879 \t\n",
      "Epoch: 191 \t Batch: 50 \t Loss: 0.5193110108 \t\n",
      "Epoch: 191 \t Batch: 60 \t Loss: 0.4366228580 \t\n",
      "Epoch: 191 \t Batch: 70 \t Loss: 0.3663033843 \t\n",
      "Epoch: 192 \t Batch: 0 \t Loss: 0.4004965425 \t\n",
      "Epoch: 192 \t Batch: 10 \t Loss: 0.3334458470 \t\n",
      "Epoch: 192 \t Batch: 20 \t Loss: 0.4382359385 \t\n",
      "Epoch: 192 \t Batch: 30 \t Loss: 0.3429439068 \t\n",
      "Epoch: 192 \t Batch: 40 \t Loss: 0.3919450343 \t\n",
      "Epoch: 192 \t Batch: 50 \t Loss: 0.5181632638 \t\n",
      "Epoch: 192 \t Batch: 60 \t Loss: 0.4314348698 \t\n",
      "Epoch: 192 \t Batch: 70 \t Loss: 0.3640174866 \t\n",
      "Epoch: 193 \t Batch: 0 \t Loss: 0.3983552754 \t\n",
      "Epoch: 193 \t Batch: 10 \t Loss: 0.3288969100 \t\n",
      "Epoch: 193 \t Batch: 20 \t Loss: 0.4334194064 \t\n",
      "Epoch: 193 \t Batch: 30 \t Loss: 0.3388765454 \t\n",
      "Epoch: 193 \t Batch: 40 \t Loss: 0.3895328641 \t\n",
      "Epoch: 193 \t Batch: 50 \t Loss: 0.5176318884 \t\n",
      "Epoch: 193 \t Batch: 60 \t Loss: 0.4252598286 \t\n",
      "Epoch: 193 \t Batch: 70 \t Loss: 0.3627524376 \t\n",
      "Epoch: 194 \t Batch: 0 \t Loss: 0.3973091245 \t\n",
      "Epoch: 194 \t Batch: 10 \t Loss: 0.3245905340 \t\n",
      "Epoch: 194 \t Batch: 20 \t Loss: 0.4290910661 \t\n",
      "Epoch: 194 \t Batch: 30 \t Loss: 0.3354550004 \t\n",
      "Epoch: 194 \t Batch: 40 \t Loss: 0.3877573907 \t\n",
      "Epoch: 194 \t Batch: 50 \t Loss: 0.5180262923 \t\n",
      "Epoch: 194 \t Batch: 60 \t Loss: 0.4198395312 \t\n",
      "Epoch: 194 \t Batch: 70 \t Loss: 0.3618208468 \t\n",
      "Epoch: 195 \t Batch: 0 \t Loss: 0.3970053196 \t\n",
      "Epoch: 195 \t Batch: 10 \t Loss: 0.3221680820 \t\n",
      "Epoch: 195 \t Batch: 20 \t Loss: 0.4255380034 \t\n",
      "Epoch: 195 \t Batch: 30 \t Loss: 0.3333236277 \t\n",
      "Epoch: 195 \t Batch: 40 \t Loss: 0.3848747611 \t\n",
      "Epoch: 195 \t Batch: 50 \t Loss: 0.5184970498 \t\n",
      "Epoch: 195 \t Batch: 60 \t Loss: 0.4156370163 \t\n",
      "Epoch: 195 \t Batch: 70 \t Loss: 0.3598989248 \t\n",
      "Epoch: 196 \t Batch: 0 \t Loss: 0.3963724971 \t\n",
      "Epoch: 196 \t Batch: 10 \t Loss: 0.3217678070 \t\n",
      "Epoch: 196 \t Batch: 20 \t Loss: 0.4223722517 \t\n",
      "Epoch: 196 \t Batch: 30 \t Loss: 0.3324317932 \t\n",
      "Epoch: 196 \t Batch: 40 \t Loss: 0.3801827431 \t\n",
      "Epoch: 196 \t Batch: 50 \t Loss: 0.5190747976 \t\n",
      "Epoch: 196 \t Batch: 60 \t Loss: 0.4127519727 \t\n",
      "Epoch: 196 \t Batch: 70 \t Loss: 0.3574086130 \t\n",
      "Epoch: 197 \t Batch: 0 \t Loss: 0.3958961964 \t\n",
      "Epoch: 197 \t Batch: 10 \t Loss: 0.3235009611 \t\n",
      "Epoch: 197 \t Batch: 20 \t Loss: 0.4201868474 \t\n",
      "Epoch: 197 \t Batch: 30 \t Loss: 0.3333384693 \t\n",
      "Epoch: 197 \t Batch: 40 \t Loss: 0.3754134476 \t\n",
      "Epoch: 197 \t Batch: 50 \t Loss: 0.5209591985 \t\n",
      "Epoch: 197 \t Batch: 60 \t Loss: 0.4117333293 \t\n",
      "Epoch: 197 \t Batch: 70 \t Loss: 0.3561957181 \t\n",
      "Epoch: 198 \t Batch: 0 \t Loss: 0.3970860839 \t\n",
      "Epoch: 198 \t Batch: 10 \t Loss: 0.3270763755 \t\n",
      "Epoch: 198 \t Batch: 20 \t Loss: 0.4197124839 \t\n",
      "Epoch: 198 \t Batch: 30 \t Loss: 0.3360257745 \t\n",
      "Epoch: 198 \t Batch: 40 \t Loss: 0.3725257516 \t\n",
      "Epoch: 198 \t Batch: 50 \t Loss: 0.5245172977 \t\n",
      "Epoch: 198 \t Batch: 60 \t Loss: 0.4117536545 \t\n",
      "Epoch: 198 \t Batch: 70 \t Loss: 0.3570201397 \t\n",
      "Epoch: 199 \t Batch: 0 \t Loss: 0.4000430107 \t\n",
      "Epoch: 199 \t Batch: 10 \t Loss: 0.3307660818 \t\n",
      "Epoch: 199 \t Batch: 20 \t Loss: 0.4202980399 \t\n",
      "Epoch: 199 \t Batch: 30 \t Loss: 0.3388854861 \t\n",
      "Epoch: 199 \t Batch: 40 \t Loss: 0.3719775975 \t\n",
      "Epoch: 199 \t Batch: 50 \t Loss: 0.5285781622 \t\n",
      "Epoch: 199 \t Batch: 60 \t Loss: 0.4113264084 \t\n",
      "Epoch: 199 \t Batch: 70 \t Loss: 0.3591356874 \t\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "LOSS = []\n",
    "for e in range(epochs):\n",
    "\n",
    "  for i, (x, y) in enumerate(zip(X_train, labels_train_gender)):\n",
    "    x = x.cuda()\n",
    "    y_pred = gender(x)\n",
    "    y = y.cuda()\n",
    "    loss = criterio(y_pred, y)\n",
    "    #Calculate gradient of the loss \n",
    "    loss.backward()\n",
    "    #Gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    LOSS.append(loss.item())\n",
    "    if i%10==0:\n",
    "      print('Epoch: %d \\t Batch: %d \\t Loss: %.10f \\t'%(e,i, torch.tensor(LOSS[-10:]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oli5r0W9mzOf"
   },
   "outputs": [],
   "source": [
    "X_test = torch.tensor(tfidf_vectorizer_test.toarray()).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M6_Qv07HlMZw",
    "outputId": "f8df06f5-ff85-40ca-dd2f-29e7229e48e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 2])\n"
     ]
    }
   ],
   "source": [
    "y_pred = gender(X_test.cuda())\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLhw5B4SnRy9"
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred.argmax(1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GrlMzzzxm7u_",
    "outputId": "f27478aa-320e-4fd2-e9f4-466c49301d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707193941943625\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(labels_test[0], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkUfoOHguvXj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Twitter Project-01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a366769da444fb4bec573f68331d248": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12cfca9ab73e4734848ef284cf093e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1adba141b5564c6ea69bad5b5ca6397e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ce20293a77d463bbec7fd22df885c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a6b799add46408682a0e6179180919d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e45a2fe644749839b4ee624482110fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6544f71606443f092031ada9ed9ed2b",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a1e2b980f97046498ff8123a95f82927",
      "value": 440473133
     }
    },
    "3dc243aa3e7b45578406666f14e53741": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a6b799add46408682a0e6179180919d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_be71f2fd7e574e6f832583159cb458f2",
      "value": " 440M/440M [00:44&lt;00:00, 9.91MB/s]"
     }
    },
    "3fab529cfa1b40d5acccd7ac79adb60b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52bcb9acc09c49ce8a1fd1c114d5e1ad",
       "IPY_MODEL_440aeaf5fede4eb78085ba019d547787"
      ],
      "layout": "IPY_MODEL_0a366769da444fb4bec573f68331d248"
     }
    },
    "440aeaf5fede4eb78085ba019d547787": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1adba141b5564c6ea69bad5b5ca6397e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1ce20293a77d463bbec7fd22df885c1e",
      "value": " 361/361 [00:46&lt;00:00, 7.79B/s]"
     }
    },
    "4c09464bf9544bf8ab6fe1c35f858060": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52bcb9acc09c49ce8a1fd1c114d5e1ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d82b3051bf8e4190b075983ef4331189",
      "max": 361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7dc898effe4646d1b090d0b1b1ec4b40",
      "value": 361
     }
    },
    "6a133aa3d96544cc8db36621d6aba780": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e74f99b5ba934ec9b4ef67e7be64d3aa",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7f2f514769e409b9d20470fda881160",
      "value": 231508
     }
    },
    "7dc898effe4646d1b090d0b1b1ec4b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "865122ced3b54394aef5e6c05ba7ab7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8eed7a5cf47343f1b3995e1e57c90287",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_12cfca9ab73e4734848ef284cf093e96",
      "value": " 232k/232k [01:09&lt;00:00, 3.35kB/s]"
     }
    },
    "8eed7a5cf47343f1b3995e1e57c90287": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1e2b980f97046498ff8123a95f82927": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "be71f2fd7e574e6f832583159cb458f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0556a3d98fa442981f556f5f924d1d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e45a2fe644749839b4ee624482110fa",
       "IPY_MODEL_3dc243aa3e7b45578406666f14e53741"
      ],
      "layout": "IPY_MODEL_4c09464bf9544bf8ab6fe1c35f858060"
     }
    },
    "c6544f71606443f092031ada9ed9ed2b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d82b3051bf8e4190b075983ef4331189": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e74f99b5ba934ec9b4ef67e7be64d3aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7f2f514769e409b9d20470fda881160": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f4760dce9b064f15b36b72c132e9b020": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f681e7324d5f42798eea30a608a4ea66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a133aa3d96544cc8db36621d6aba780",
       "IPY_MODEL_865122ced3b54394aef5e6c05ba7ab7a"
      ],
      "layout": "IPY_MODEL_f4760dce9b064f15b36b72c132e9b020"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
